{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-06 15:56:37.075789\n",
      "epoch range:  1  to  5\n",
      "CONV1 torch.Size([4, 16, 30, 30])\n",
      "CONV2 torch.Size([4, 32, 30, 30])\n",
      "POOL1 torch.Size([4, 32, 15, 15])\n",
      "CONV3 torch.Size([4, 32, 13, 13])\n",
      "CONV4 torch.Size([4, 16, 13, 13])\n",
      "POOL2 torch.Size([4, 16, 6, 6])\n",
      "FC1 torch.Size([4, 120])\n",
      "FC2 torch.Size([4, 90])\n",
      "FC3 torch.Size([4, 10])\n",
      "2021-05-06 15:57:26.613698  Epoch 1 : Average Loss [2.035506275370717, 1.3867661869600416, 1.140778309263289]\n",
      "2021-05-06 15:58:18.428706  Epoch 2 : Average Loss [0.931992944477126, 0.8864164151852019, 0.8612100837067701]\n",
      "2021-05-06 15:59:14.297082  Epoch 3 : Average Loss [0.7472475227408577, 0.7240599455942865, 0.7024892100684228]\n",
      "2021-05-06 16:00:12.513974  Epoch 4 : Average Loss [0.649736838734243, 0.6346548123200991, 0.620919586107324]\n",
      "2021-05-06 16:01:13.512446  Epoch 5 : Average Loss [0.5685922200744244, 0.5355017677007782, 0.5616377598288236]\n",
      "Finished Training\n",
      "Training accuracy: 84 %\n",
      "Testing accuracy: 82 %\n",
      "Testing accuracy (each class): \n",
      "0: 85.6%;   1: 84.8%;   2: 82.4%;   3: 70.4%;   4: 89.4%;   5: 84.2%;   6: 75.8%;   7: 87.4%;   8: 84.0%;   9: 84.6%;   \n",
      "2021-05-06 16:01:48.951740\n",
      "epoch range:  6  to  10\n",
      "2021-05-06 16:02:46.938673  Epoch 1 : Average Loss [0.5195830552194384, 0.4994862020991059, 0.5084230093152692]\n",
      "2021-05-06 16:03:42.052469  Epoch 2 : Average Loss [0.4652751918245485, 0.47153110040398316, 0.4762632739707642]\n",
      "2021-05-06 16:04:40.019185  Epoch 3 : Average Loss [0.4215013743262498, 0.44727400514134935, 0.4470908621354756]\n",
      "2021-05-06 16:05:34.607994  Epoch 4 : Average Loss [0.4080113629772095, 0.42816804242807305, 0.41031476174014825]\n",
      "2021-05-06 16:06:32.137912  Epoch 5 : Average Loss [0.3843194341606031, 0.38547071889994916, 0.37824415733369643]\n",
      "Finished Training\n",
      "Training accuracy: 89 %\n",
      "Testing accuracy: 85 %\n",
      "Testing accuracy (each class): \n",
      "0: 90.8%;   1: 84.0%;   2: 83.8%;   3: 80.6%;   4: 89.4%;   5: 82.8%;   6: 84.0%;   7: 91.6%;   8: 78.0%;   9: 86.0%;   \n",
      "2021-05-06 16:07:09.141795\n",
      "epoch range:  11  to  15\n",
      "2021-05-06 16:08:09.374777  Epoch 1 : Average Loss [0.3769844487647124, 0.3674450888229376, 0.3724725439776248]\n",
      "2021-05-06 16:09:09.904756  Epoch 2 : Average Loss [0.35134909123428226, 0.3442646286520885, 0.3469300278871565]\n",
      "2021-05-06 16:10:08.133756  Epoch 3 : Average Loss [0.3261165474700847, 0.3545417026245914, 0.3119352537619475]\n",
      "2021-05-06 16:11:14.251459  Epoch 4 : Average Loss [0.3073818591839813, 0.3086854685307028, 0.32087549552868405]\n",
      "2021-05-06 16:12:11.134545  Epoch 5 : Average Loss [0.29189537534085674, 0.31573294624199705, 0.3091905872326228]\n",
      "Finished Training\n",
      "Training accuracy: 90 %\n",
      "Testing accuracy: 85 %\n",
      "Testing accuracy (each class): \n",
      "0: 90.6%;   1: 89.2%;   2: 88.6%;   3: 74.4%;   4: 94.4%;   5: 89.8%;   6: 81.6%;   7: 90.4%;   8: 80.6%;   9: 80.2%;   \n",
      "2021-05-06 16:12:50.857998\n",
      "epoch range:  16  to  20\n",
      "2021-05-06 16:13:52.920475  Epoch 1 : Average Loss [0.2806424497879325, 0.2916326666176949, 0.2877939274228061]\n",
      "2021-05-06 16:14:50.910869  Epoch 2 : Average Loss [0.2804063791460868, 0.28236100976773826, 0.27626007633625066]\n",
      "2021-05-06 16:15:50.752383  Epoch 3 : Average Loss [0.25193734482770413, 0.2528336304867741, 0.27273320829671954]\n",
      "2021-05-06 16:16:53.984038  Epoch 4 : Average Loss [0.2512783747140553, 0.24909309374441727, 0.24164043047209943]\n",
      "2021-05-06 16:17:58.816434  Epoch 5 : Average Loss [0.215572089888255, 0.2409899877422261, 0.24772535053549735]\n",
      "Finished Training\n",
      "Training accuracy: 93 %\n",
      "Testing accuracy: 87 %\n",
      "Testing accuracy (each class): \n",
      "0: 91.2%;   1: 89.4%;   2: 85.2%;   3: 84.2%;   4: 91.6%;   5: 81.4%;   6: 85.2%;   7: 89.8%;   8: 86.4%;   9: 87.0%;   \n",
      "2021-05-06 16:18:39.775680\n",
      "epoch range:  21  to  25\n",
      "2021-05-06 16:19:49.595428  Epoch 1 : Average Loss [0.21548155351210233, 0.23640000296800115, 0.23391560402339512]\n",
      "2021-05-06 16:20:49.247878  Epoch 2 : Average Loss [0.19902397006269404, 0.22036639198278826, 0.21917321084869495]\n",
      "2021-05-06 16:21:47.602638  Epoch 3 : Average Loss [0.19263453432500136, 0.20902483001227157, 0.21529653963544246]\n",
      "2021-05-06 16:22:48.534565  Epoch 4 : Average Loss [0.16420099572674154, 0.19773353752147746, 0.21238983894986185]\n",
      "2021-05-06 16:23:50.909846  Epoch 5 : Average Loss [0.17920072372065016, 0.19048224230084493, 0.20005393792438309]\n",
      "Finished Training\n",
      "Training accuracy: 95 %\n",
      "Testing accuracy: 87 %\n",
      "Testing accuracy (each class): \n",
      "0: 89.2%;   1: 88.0%;   2: 86.2%;   3: 84.2%;   4: 92.6%;   5: 86.2%;   6: 82.4%;   7: 91.4%;   8: 86.8%;   9: 86.2%;   \n",
      "2021-05-06 16:24:31.076774\n",
      "epoch range:  26  to  30\n",
      "2021-05-06 16:25:32.503890  Epoch 1 : Average Loss [0.18247157976554546, 0.17944337716883021, 0.1793039380310859]\n",
      "2021-05-06 16:26:31.951469  Epoch 2 : Average Loss [0.1682842469179809, 0.17476674023983366, 0.1769048457397725]\n",
      "2021-05-06 16:27:32.313252  Epoch 3 : Average Loss [0.16094984493081763, 0.14901782166226352, 0.1747044116031238]\n",
      "2021-05-06 16:28:31.865006  Epoch 4 : Average Loss [0.14435371953817383, 0.157404615226296, 0.17076410786514426]\n",
      "2021-05-06 16:29:32.985243  Epoch 5 : Average Loss [0.1365719150970305, 0.14866148840855295, 0.1580365032521432]\n",
      "Finished Training\n",
      "Training accuracy: 96 %\n",
      "Testing accuracy: 86 %\n",
      "Testing accuracy (each class): \n",
      "0: 88.4%;   1: 87.8%;   2: 82.6%;   3: 81.6%;   4: 92.6%;   5: 84.8%;   6: 83.6%;   7: 90.0%;   8: 85.6%;   9: 88.6%;   \n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import datasets, transforms, utils\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import datetime\n",
    "\n",
    "# TODO: Implement a convolutional neural network (https://pytorch.org/tutorials/recipes/recipes/defining_a_neural_network.html)\n",
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Input - 1x32x32\n",
    "    Output - 10\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.params = {'conv':[(), \n",
    "                               (3, 16, 5, 1, 1), \n",
    "                               (16, 32, 3, 1, 1),\n",
    "                               (32, 32, 3, 1, 0),\n",
    "                               (32, 16, 3, 1, 1)], # in_channels, out_channels, kernel_size, stride, padding\n",
    "                       'pool':[(), \n",
    "                               (2, 2, 0),\n",
    "                               (2, 2, 0)], # kernel_size, stride, padding\n",
    "                       'fc':[(), \n",
    "                             (16*6*6, 120),\n",
    "                             (120, 90), \n",
    "                             (90, 10)], # in_channels, out_channels\n",
    "                       'drop':[0, \n",
    "                               0.25, \n",
    "                               0.25]\n",
    "                      }\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(*self.params['conv'][1])\n",
    "        self.conv2 = nn.Conv2d(*self.params['conv'][2])\n",
    "        self.conv3 = nn.Conv2d(*self.params['conv'][3])\n",
    "        self.conv4 = nn.Conv2d(*self.params['conv'][4])\n",
    "        \n",
    "        self.pool1 = nn.MaxPool2d(*self.params['pool'][1])\n",
    "        self.pool2 = nn.MaxPool2d(*self.params['pool'][2])\n",
    "        \n",
    "        self.fc1 = nn.Linear(*self.params['fc'][1])\n",
    "        self.fc2 = nn.Linear(*self.params['fc'][2])\n",
    "        self.fc3 = nn.Linear(*self.params['fc'][3])\n",
    "        \n",
    "        # self.drop1 = nn.Dropout2d(self.params['drop'][1])\n",
    "        # self.drop2 = nn.Dropout2d(self.params['drop'][2])\n",
    "        \n",
    "        self.printed = False\n",
    "\n",
    "        # TODO: Initialize layers\n",
    "\n",
    "    def forward(self, img):\n",
    "\n",
    "        # TODO: Implement forward pass\n",
    "        x = img\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        if not self.printed: \n",
    "            print(\"CONV1\", x.size())\n",
    "        x = F.relu(self.conv2(x))\n",
    "        if not self.printed: \n",
    "            print(\"CONV2\", x.size())\n",
    "        x = self.pool1(x)\n",
    "        if not self.printed: \n",
    "            print(\"POOL1\", x.size())\n",
    "        \n",
    "        x = F.relu(self.conv3(x))\n",
    "        if not self.printed: \n",
    "            print(\"CONV3\", x.size())\n",
    "        x = F.relu(self.conv4(x))\n",
    "        if not self.printed: \n",
    "            print(\"CONV4\", x.size())\n",
    "        x = self.pool2(x)\n",
    "        if not self.printed: \n",
    "            print(\"POOL2\", x.size())\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        if not self.printed: \n",
    "            print(\"FC1\", x.size())\n",
    "        '''x = self.drop1(x)\n",
    "        if not self.printed: \n",
    "            print(\"DROP1\", x.size())'''\n",
    "        x = F.relu(self.fc2(x))\n",
    "        if not self.printed: \n",
    "            print(\"FC2\", x.size())\n",
    "        x = self.fc3(x)\n",
    "        if not self.printed: \n",
    "            print(\"FC3\", x.size())\n",
    "            self.printed = True\n",
    "\n",
    "        return x\n",
    "\n",
    "# TODO: You can change these data augmentation and normalization strategies for\n",
    "#  better training and testing (https://pytorch.org/vision/stable/transforms.html)\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((32,32)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((32,32)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Dataset initialization\n",
    "data_dir = 'data' # Suppose the dataset is stored under this folder\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'test']} # Read train and test sets, respectively.\n",
    "\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=0) for x in ['train', 'test']}\n",
    "# trainloader = torch.utils.data.DataLoader(image_datasets['train'], batch_size=4, shuffle=True, num_workers=2)\n",
    "# teatloader = torch.utils.data.DataLoader(image_datasets['test'], batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'test']}\n",
    "\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # Set device to \"cpu\" if you have no gpu\n",
    "\n",
    "# TODO: Implement training and testing procedures (https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html)\n",
    "def train_test(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    \n",
    "    for epoch in range(num_epochs):  \n",
    "\n",
    "        running_loss = 0.0\n",
    "        loss_record=[]\n",
    "        for i, data in enumerate(dataloaders['train'], 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "                loss_record.append(running_loss / 2000)\n",
    "                # print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000))\n",
    "                running_loss = 0.0\n",
    "                \n",
    "        print(datetime.datetime.now(), ' Epoch', (epoch + 1), ': Average Loss', loss_record)\n",
    "\n",
    "    print('Finished Training')\n",
    "    \n",
    "    \n",
    "    # save training results\n",
    "    PATH = './cifar_net.pth'\n",
    "    torch.save(model.state_dict(), PATH)\n",
    "    \n",
    "    \n",
    "    # testing overall correct rate\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloaders['train'], 0):\n",
    "            images, labels = data\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = model(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Training accuracy: %d %%' % (100 * correct / total))\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloaders['test'], 0):\n",
    "            images, labels = data\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = model(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Testing accuracy: %d %%' % (100 * correct / total))\n",
    "    \n",
    "    # prepare to count predictions for each class\n",
    "    correct_pred = {classname: 0 for classname in class_names}\n",
    "    total_pred = {classname: 0 for classname in class_names}\n",
    "\n",
    "    # again no gradients needed\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloaders['test'], 0):\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            # collect the correct predictions for each class\n",
    "            for label, prediction in zip(labels, predictions):\n",
    "                if label == prediction:\n",
    "                    correct_pred[class_names[label]] += 1\n",
    "                total_pred[class_names[label]] += 1\n",
    "\n",
    "\n",
    "    # print accuracy for each class\n",
    "    print(\"Testing accuracy (each class): \")\n",
    "    for classname, correct_count in correct_pred.items():\n",
    "        accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "        print(\"{:1s}: {:.1f}%;  \".format(classname, accuracy), end=' ')\n",
    "    print()    \n",
    "    \n",
    "    return None\n",
    "\n",
    "model_ft = Net() # Model initialization\n",
    "\n",
    "model_ft = model_ft.to(device) # Move model to cpu\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # Loss function initialization\n",
    "\n",
    "# TODO: Adjust the following hyper-parameters: learning rate, decay strategy, number of training epochs.\n",
    "optimizer_ft = optim.Adam(model_ft.parameters(), lr=1e-4) # Optimizer initialization\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=20, gamma=0.1) # Learning rate decay strategy\n",
    "\n",
    "for n in range(6):\n",
    "    print(datetime.datetime.now())\n",
    "    epo = 5*n+5\n",
    "    print(\"epoch range: \", epo-4, \" to \", epo)\n",
    "    train_test(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-06 16:40:06.488670\n",
      "epoch range:  1  to  5\n",
      "CONV1 torch.Size([4, 16, 30, 30])\n",
      "CONV2 torch.Size([4, 32, 30, 30])\n",
      "POOL1 torch.Size([4, 32, 15, 15])\n",
      "CONV3 torch.Size([4, 32, 13, 13])\n",
      "CONV4 torch.Size([4, 16, 13, 13])\n",
      "POOL2 torch.Size([4, 16, 6, 6])\n",
      "FC1 torch.Size([4, 120])\n",
      "FC2 torch.Size([4, 90])\n",
      "FC3 torch.Size([4, 10])\n",
      "2021-05-06 16:41:05.581086  Epoch 1 : Average Loss [2.3043154529333116, 2.3045051860809327, 2.3042086980342864]\n",
      "2021-05-06 16:41:59.618088  Epoch 2 : Average Loss [2.3027573450803756, 2.304300184249878, 2.3047116711139677]\n",
      "2021-05-06 16:43:10.421231  Epoch 3 : Average Loss [2.3051510660648344, 2.303858541727066, 2.304147281527519]\n",
      "2021-05-06 16:44:17.686033  Epoch 4 : Average Loss [2.303972874045372, 2.3040791311264037, 2.3041595437526703]\n",
      "2021-05-06 16:45:16.387825  Epoch 5 : Average Loss [2.3041162202358247, 2.3037228739261626, 2.3041245189905166]\n",
      "Finished Training\n",
      "Training accuracy: 10 %\n",
      "Testing accuracy: 10 %\n",
      "Testing accuracy (each class): \n",
      "0: 0.0%;   1: 0.0%;   2: 0.0%;   3: 0.0%;   4: 0.0%;   5: 0.0%;   6: 100.0%;   7: 0.0%;   8: 0.0%;   9: 0.0%;   \n",
      "2021-05-06 16:45:51.302637\n",
      "epoch range:  6  to  10\n",
      "2021-05-06 16:46:53.065745  Epoch 1 : Average Loss [2.3044177260398864, 2.3046243772506716, 2.304252769112587]\n",
      "2021-05-06 16:47:55.016583  Epoch 2 : Average Loss [2.303966151833534, 2.3044219113588333, 2.3041930882930757]\n",
      "2021-05-06 16:48:53.420801  Epoch 3 : Average Loss [2.305077434659004, 2.304637652873993, 2.303223826289177]\n",
      "2021-05-06 16:49:54.141636  Epoch 4 : Average Loss [2.30338006067276, 2.3044829570055008, 2.30452843272686]\n",
      "2021-05-06 16:50:52.032513  Epoch 5 : Average Loss [2.3037023512125017, 2.3045400552749635, 2.3040272575616836]\n",
      "Finished Training\n",
      "Training accuracy: 10 %\n",
      "Testing accuracy: 10 %\n",
      "Testing accuracy (each class): \n",
      "0: 0.0%;   1: 0.0%;   2: 0.0%;   3: 0.0%;   4: 0.0%;   5: 0.0%;   6: 100.0%;   7: 0.0%;   8: 0.0%;   9: 0.0%;   \n",
      "2021-05-06 16:51:26.446762\n",
      "epoch range:  11  to  15\n",
      "2021-05-06 16:52:23.091108  Epoch 1 : Average Loss [2.303218061327934, 2.3042212270498275, 2.304395101070404]\n",
      "2021-05-06 16:53:21.408743  Epoch 2 : Average Loss [2.303494477391243, 2.3044422104358673, 2.3036237671375273]\n",
      "2021-05-06 16:54:18.826306  Epoch 3 : Average Loss [2.3039246524572374, 2.3035950490236283, 2.3035900802612304]\n",
      "2021-05-06 16:55:17.492244  Epoch 4 : Average Loss [2.3036023069620133, 2.304266403198242, 2.3035171353816986]\n",
      "2021-05-06 16:56:14.931700  Epoch 5 : Average Loss [2.303829097390175, 2.3040354113578796, 2.3034952077865603]\n",
      "Finished Training\n",
      "Training accuracy: 10 %\n",
      "Testing accuracy: 10 %\n",
      "Testing accuracy (each class): \n",
      "0: 0.0%;   1: 0.0%;   2: 0.0%;   3: 0.0%;   4: 0.0%;   5: 0.0%;   6: 100.0%;   7: 0.0%;   8: 0.0%;   9: 0.0%;   \n",
      "2021-05-06 16:56:49.675507\n",
      "epoch range:  16  to  20\n",
      "2021-05-06 16:57:46.077695  Epoch 1 : Average Loss [2.303361713171005, 2.30318518447876, 2.3041020197868347]\n",
      "2021-05-06 16:58:46.304397  Epoch 2 : Average Loss [2.3046165373325347, 2.30314210999012, 2.3044151314496992]\n",
      "2021-05-06 16:59:46.528313  Epoch 3 : Average Loss [2.3041057963371276, 2.3044942957162857, 2.302790130376816]\n",
      "2021-05-06 17:00:44.464475  Epoch 4 : Average Loss [2.3030620889663695, 2.3043007060289384, 2.3051239005327226]\n",
      "2021-05-06 17:01:41.858777  Epoch 5 : Average Loss [2.3036599777936937, 2.3042588460445406, 2.303579719185829]\n",
      "Finished Training\n",
      "Training accuracy: 10 %\n",
      "Testing accuracy: 10 %\n",
      "Testing accuracy (each class): \n",
      "0: 0.0%;   1: 0.0%;   2: 0.0%;   3: 0.0%;   4: 0.0%;   5: 0.0%;   6: 100.0%;   7: 0.0%;   8: 0.0%;   9: 0.0%;   \n",
      "2021-05-06 17:02:18.805655\n",
      "epoch range:  21  to  25\n",
      "2021-05-06 17:03:16.794139  Epoch 1 : Average Loss [2.3038649369478224, 2.303974863409996, 2.3036461873054503]\n",
      "2021-05-06 17:04:15.208943  Epoch 2 : Average Loss [2.303770859837532, 2.3039993188381196, 2.304053900718689]\n",
      "2021-05-06 17:05:13.148563  Epoch 3 : Average Loss [2.3032824845314024, 2.3038496092557907, 2.304706813573837]\n",
      "2021-05-06 17:06:11.770859  Epoch 4 : Average Loss [2.3029425753355026, 2.3048788138628007, 2.3040023159980776]\n",
      "2021-05-06 17:07:09.791501  Epoch 5 : Average Loss [2.303676019668579, 2.3045263357162478, 2.302822575211525]\n",
      "Finished Training\n",
      "Training accuracy: 10 %\n",
      "Testing accuracy: 10 %\n",
      "Testing accuracy (each class): \n",
      "0: 0.0%;   1: 0.0%;   2: 0.0%;   3: 0.0%;   4: 0.0%;   5: 0.0%;   6: 100.0%;   7: 0.0%;   8: 0.0%;   9: 0.0%;   \n",
      "2021-05-06 17:07:44.320517\n",
      "epoch range:  26  to  30\n",
      "2021-05-06 17:08:42.934054  Epoch 1 : Average Loss [2.3038332531452177, 2.3035614956617354, 2.3044558255672456]\n",
      "2021-05-06 17:09:40.828705  Epoch 2 : Average Loss [2.3039750871658327, 2.303462601661682, 2.3038741139173506]\n",
      "2021-05-06 17:10:38.290767  Epoch 3 : Average Loss [2.303543598175049, 2.303546508193016, 2.3033560190200806]\n",
      "2021-05-06 17:11:35.811704  Epoch 4 : Average Loss [2.3032486494779585, 2.304058141231537, 2.3034237711429597]\n",
      "2021-05-06 17:12:33.958261  Epoch 5 : Average Loss [2.3039625207185743, 2.303689599990845, 2.3034341799020766]\n",
      "Finished Training\n",
      "Training accuracy: 10 %\n",
      "Testing accuracy: 10 %\n",
      "Testing accuracy (each class): \n",
      "0: 0.0%;   1: 0.0%;   2: 0.0%;   3: 0.0%;   4: 0.0%;   5: 0.0%;   6: 100.0%;   7: 0.0%;   8: 0.0%;   9: 0.0%;   \n"
     ]
    }
   ],
   "source": [
    "# optim.Adadelta\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import datasets, transforms, utils\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import datetime\n",
    "\n",
    "# TODO: Implement a convolutional neural network (https://pytorch.org/tutorials/recipes/recipes/defining_a_neural_network.html)\n",
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Input - 1x32x32\n",
    "    Output - 10\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.params = {'conv':[(), \n",
    "                               (3, 16, 5, 1, 1), \n",
    "                               (16, 32, 3, 1, 1),\n",
    "                               (32, 32, 3, 1, 0),\n",
    "                               (32, 16, 3, 1, 1)], # in_channels, out_channels, kernel_size, stride, padding\n",
    "                       'pool':[(), \n",
    "                               (2, 2, 0),\n",
    "                               (2, 2, 0)], # kernel_size, stride, padding\n",
    "                       'fc':[(), \n",
    "                             (16*6*6, 120),\n",
    "                             (120, 90), \n",
    "                             (90, 10)], # in_channels, out_channels\n",
    "                       'drop':[0, \n",
    "                               0.25, \n",
    "                               0.25]\n",
    "                      }\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(*self.params['conv'][1])\n",
    "        self.conv2 = nn.Conv2d(*self.params['conv'][2])\n",
    "        self.conv3 = nn.Conv2d(*self.params['conv'][3])\n",
    "        self.conv4 = nn.Conv2d(*self.params['conv'][4])\n",
    "        \n",
    "        self.pool1 = nn.MaxPool2d(*self.params['pool'][1])\n",
    "        self.pool2 = nn.MaxPool2d(*self.params['pool'][2])\n",
    "        \n",
    "        self.fc1 = nn.Linear(*self.params['fc'][1])\n",
    "        self.fc2 = nn.Linear(*self.params['fc'][2])\n",
    "        self.fc3 = nn.Linear(*self.params['fc'][3])\n",
    "        \n",
    "        # self.drop1 = nn.Dropout2d(self.params['drop'][1])\n",
    "        # self.drop2 = nn.Dropout2d(self.params['drop'][2])\n",
    "        \n",
    "        self.printed = False\n",
    "\n",
    "        # TODO: Initialize layers\n",
    "\n",
    "    def forward(self, img):\n",
    "\n",
    "        # TODO: Implement forward pass\n",
    "        x = img\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        if not self.printed: \n",
    "            print(\"CONV1\", x.size())\n",
    "        x = F.relu(self.conv2(x))\n",
    "        if not self.printed: \n",
    "            print(\"CONV2\", x.size())\n",
    "        x = self.pool1(x)\n",
    "        if not self.printed: \n",
    "            print(\"POOL1\", x.size())\n",
    "        \n",
    "        x = F.relu(self.conv3(x))\n",
    "        if not self.printed: \n",
    "            print(\"CONV3\", x.size())\n",
    "        x = F.relu(self.conv4(x))\n",
    "        if not self.printed: \n",
    "            print(\"CONV4\", x.size())\n",
    "        x = self.pool2(x)\n",
    "        if not self.printed: \n",
    "            print(\"POOL2\", x.size())\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        if not self.printed: \n",
    "            print(\"FC1\", x.size())\n",
    "        '''x = self.drop1(x)\n",
    "        if not self.printed: \n",
    "            print(\"DROP1\", x.size())'''\n",
    "        x = F.relu(self.fc2(x))\n",
    "        if not self.printed: \n",
    "            print(\"FC2\", x.size())\n",
    "        x = self.fc3(x)\n",
    "        if not self.printed: \n",
    "            print(\"FC3\", x.size())\n",
    "            self.printed = True\n",
    "\n",
    "        return x\n",
    "\n",
    "# TODO: You can change these data augmentation and normalization strategies for\n",
    "#  better training and testing (https://pytorch.org/vision/stable/transforms.html)\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((32,32)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((32,32)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Dataset initialization\n",
    "data_dir = 'data' # Suppose the dataset is stored under this folder\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'test']} # Read train and test sets, respectively.\n",
    "\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=0) for x in ['train', 'test']}\n",
    "# trainloader = torch.utils.data.DataLoader(image_datasets['train'], batch_size=4, shuffle=True, num_workers=2)\n",
    "# teatloader = torch.utils.data.DataLoader(image_datasets['test'], batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'test']}\n",
    "\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # Set device to \"cpu\" if you have no gpu\n",
    "\n",
    "# TODO: Implement training and testing procedures (https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html)\n",
    "def train_test(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    \n",
    "    for epoch in range(num_epochs):  \n",
    "\n",
    "        running_loss = 0.0\n",
    "        loss_record=[]\n",
    "        for i, data in enumerate(dataloaders['train'], 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "                loss_record.append(running_loss / 2000)\n",
    "                # print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000))\n",
    "                running_loss = 0.0\n",
    "                \n",
    "        print(datetime.datetime.now(), ' Epoch', (epoch + 1), ': Average Loss', loss_record)\n",
    "\n",
    "    print('Finished Training')\n",
    "    \n",
    "    \n",
    "    # save training results\n",
    "    PATH = './cifar_net.pth'\n",
    "    torch.save(model.state_dict(), PATH)\n",
    "    \n",
    "    \n",
    "    # testing overall correct rate\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloaders['train'], 0):\n",
    "            images, labels = data\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = model(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Training accuracy: %d %%' % (100 * correct / total))\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloaders['test'], 0):\n",
    "            images, labels = data\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = model(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Testing accuracy: %d %%' % (100 * correct / total))\n",
    "    \n",
    "    # prepare to count predictions for each class\n",
    "    correct_pred = {classname: 0 for classname in class_names}\n",
    "    total_pred = {classname: 0 for classname in class_names}\n",
    "\n",
    "    # again no gradients needed\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloaders['test'], 0):\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            # collect the correct predictions for each class\n",
    "            for label, prediction in zip(labels, predictions):\n",
    "                if label == prediction:\n",
    "                    correct_pred[class_names[label]] += 1\n",
    "                total_pred[class_names[label]] += 1\n",
    "\n",
    "\n",
    "    # print accuracy for each class\n",
    "    print(\"Testing accuracy (each class): \")\n",
    "    for classname, correct_count in correct_pred.items():\n",
    "        accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "        print(\"{:1s}: {:.1f}%;  \".format(classname, accuracy), end=' ')\n",
    "    print()    \n",
    "    \n",
    "    return None\n",
    "\n",
    "model_ft = Net() # Model initialization\n",
    "\n",
    "model_ft = model_ft.to(device) # Move model to cpu\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # Loss function initialization\n",
    "\n",
    "# TODO: Adjust the following hyper-parameters: learning rate, decay strategy, number of training epochs.\n",
    "optimizer_ft = optim.Adadelta(model_ft.parameters(), lr=1e-4) # Optimizer initialization\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=20, gamma=0.1) # Learning rate decay strategy\n",
    "\n",
    "for n in range(6):\n",
    "    print(datetime.datetime.now())\n",
    "    epo = 5*n+5\n",
    "    print(\"epoch range: \", epo-4, \" to \", epo)\n",
    "    train_test(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-06 18:13:19.463533\n",
      "epoch range:  1  to  5\n",
      "CONV1 torch.Size([4, 16, 30, 30])\n",
      "CONV2 torch.Size([4, 32, 30, 30])\n",
      "POOL1 torch.Size([4, 32, 15, 15])\n",
      "CONV3 torch.Size([4, 32, 13, 13])\n",
      "CONV4 torch.Size([4, 16, 13, 13])\n",
      "POOL2 torch.Size([4, 16, 6, 6])\n",
      "FC1 torch.Size([4, 120])\n",
      "FC2 torch.Size([4, 90])\n",
      "FC3 torch.Size([4, 10])\n",
      "2021-05-06 18:14:12.039795  Epoch 1 : Average Loss [2.3046225954294206, 2.3030373970270155, 2.3034356145858763]\n",
      "2021-05-06 18:15:10.813733  Epoch 2 : Average Loss [2.3033910413980485, 2.302909638404846, 2.303143483519554]\n",
      "2021-05-06 18:16:11.405797  Epoch 3 : Average Loss [2.303170191168785, 2.3029901208877566, 2.3031493718624114]\n",
      "2021-05-06 18:17:11.464257  Epoch 4 : Average Loss [2.3031673308610916, 2.302951886296272, 2.3031302314996718]\n",
      "2021-05-06 18:18:09.657266  Epoch 5 : Average Loss [2.303080519795418, 2.303159546971321, 2.302984843850136]\n",
      "Finished Training\n",
      "Training accuracy: 10 %\n",
      "Testing accuracy: 10 %\n",
      "Testing accuracy (each class): \n",
      "0: 0.0%;   1: 0.0%;   2: 0.0%;   3: 0.0%;   4: 0.0%;   5: 100.0%;   6: 0.0%;   7: 0.0%;   8: 0.0%;   9: 0.0%;   \n",
      "2021-05-06 18:18:44.298406\n",
      "epoch range:  6  to  10\n",
      "2021-05-06 18:19:45.863893  Epoch 1 : Average Loss [2.302988816022873, 2.3031172016859056, 2.303084499359131]\n",
      "2021-05-06 18:20:52.223174  Epoch 2 : Average Loss [2.302930813193321, 2.3029691984653473, 2.3033941898345947]\n",
      "2021-05-06 18:21:56.444537  Epoch 3 : Average Loss [2.302762973666191, 2.3033040401935576, 2.3027752075195314]\n",
      "2021-05-06 18:23:01.248564  Epoch 4 : Average Loss [2.3031181223392485, 2.3027935869693756, 2.3032601348161696]\n",
      "2021-05-06 18:24:15.802292  Epoch 5 : Average Loss [2.3029578433036804, 2.3029731509685516, 2.3032932599782945]\n",
      "Finished Training\n",
      "Training accuracy: 10 %\n",
      "Testing accuracy: 10 %\n",
      "Testing accuracy (each class): \n",
      "0: 0.0%;   1: 0.0%;   2: 0.0%;   3: 0.0%;   4: 0.0%;   5: 0.0%;   6: 0.0%;   7: 100.0%;   8: 0.0%;   9: 0.0%;   \n",
      "2021-05-06 18:24:50.900682\n",
      "epoch range:  11  to  15\n",
      "2021-05-06 18:26:05.641274  Epoch 1 : Average Loss [2.303137485742569, 2.303155721902847, 2.3030978761911394]\n",
      "2021-05-06 18:27:22.889715  Epoch 2 : Average Loss [2.302996984243393, 2.303167954325676, 2.3032531512975694]\n",
      "2021-05-06 18:28:37.416700  Epoch 3 : Average Loss [2.302945070385933, 2.3031600028276444, 2.3030513832569124]\n",
      "2021-05-06 18:29:54.241441  Epoch 4 : Average Loss [2.3031206600666048, 2.303090579509735, 2.3031738806962965]\n",
      "2021-05-06 18:31:06.753925  Epoch 5 : Average Loss [2.303327810049057, 2.303128144979477, 2.303218946814537]\n",
      "Finished Training\n",
      "Training accuracy: 10 %\n",
      "Testing accuracy: 10 %\n",
      "Testing accuracy (each class): \n",
      "0: 0.0%;   1: 100.0%;   2: 0.0%;   3: 0.0%;   4: 0.0%;   5: 0.0%;   6: 0.0%;   7: 0.0%;   8: 0.0%;   9: 0.0%;   \n",
      "2021-05-06 18:31:40.763425\n",
      "epoch range:  16  to  20\n",
      "2021-05-06 18:32:59.550378  Epoch 1 : Average Loss [2.3025819537639616, 2.30319626390934, 2.3030011345148087]\n",
      "2021-05-06 18:34:14.112705  Epoch 2 : Average Loss [2.303317208647728, 2.3030441851615904, 2.303216817140579]\n",
      "2021-05-06 18:35:31.953076  Epoch 3 : Average Loss [2.3027499269247054, 2.3031123461723326, 2.303163942337036]\n",
      "2021-05-06 18:36:47.458074  Epoch 4 : Average Loss [2.3031005942821503, 2.302932648897171, 2.3031473327875136]\n",
      "2021-05-06 18:37:59.682699  Epoch 5 : Average Loss [2.3032949241399767, 2.3028807998895644, 2.3033412606716155]\n",
      "Finished Training\n",
      "Training accuracy: 10 %\n",
      "Testing accuracy: 10 %\n",
      "Testing accuracy (each class): \n",
      "0: 0.0%;   1: 0.0%;   2: 0.0%;   3: 0.0%;   4: 0.0%;   5: 0.0%;   6: 0.0%;   7: 0.0%;   8: 0.0%;   9: 100.0%;   \n",
      "2021-05-06 18:38:39.584982\n",
      "epoch range:  21  to  25\n",
      "2021-05-06 18:41:57.486410  Epoch 1 : Average Loss [2.3029049280881884, 2.303036827683449, 2.303074522614479]\n",
      "2021-05-06 18:44:36.112199  Epoch 2 : Average Loss [2.3031079552173614, 2.3029874603748324, 2.30324910235405]\n",
      "2021-05-06 18:48:26.084169  Epoch 3 : Average Loss [2.303048715829849, 2.3030890485048294, 2.3032003887891768]\n",
      "2021-05-06 18:52:49.663453  Epoch 4 : Average Loss [2.303172029495239, 2.3028912554979324, 2.303302928328514]\n",
      "2021-05-06 18:57:02.128015  Epoch 5 : Average Loss [2.302769396662712, 2.3032881774902343, 2.303163277387619]\n",
      "Finished Training\n",
      "Training accuracy: 10 %\n",
      "Testing accuracy: 10 %\n",
      "Testing accuracy (each class): \n",
      "0: 0.0%;   1: 0.0%;   2: 0.0%;   3: 0.0%;   4: 0.0%;   5: 0.0%;   6: 0.0%;   7: 100.0%;   8: 0.0%;   9: 0.0%;   \n",
      "2021-05-06 18:59:39.540488\n",
      "epoch range:  26  to  30\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-7ec2e85a1b95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0mepo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"epoch range: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepo\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" to \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m     \u001b[0mtrain_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_lr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-7ec2e85a1b95>\u001b[0m in \u001b[0;36mtrain_test\u001b[0;34m(model, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0;31m# zero the parameter gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jiarui/snap/jupyter/common/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-7ec2e85a1b95>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprinted\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CONV1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jiarui/snap/jupyter/common/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jiarui/snap/jupyter/common/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jiarui/snap/jupyter/common/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    394\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    395\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 396\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 1e-5\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import datasets, transforms, utils\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import datetime\n",
    "\n",
    "# TODO: Implement a convolutional neural network (https://pytorch.org/tutorials/recipes/recipes/defining_a_neural_network.html)\n",
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Input - 1x32x32\n",
    "    Output - 10\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.params = {'conv':[(), \n",
    "                               (3, 16, 5, 1, 1), \n",
    "                               (16, 32, 3, 1, 1),\n",
    "                               (32, 32, 3, 1, 0),\n",
    "                               (32, 16, 3, 1, 1)], # in_channels, out_channels, kernel_size, stride, padding\n",
    "                       'pool':[(), \n",
    "                               (2, 2, 0),\n",
    "                               (2, 2, 0)], # kernel_size, stride, padding\n",
    "                       'fc':[(), \n",
    "                             (16*6*6, 120),\n",
    "                             (120, 90), \n",
    "                             (90, 10)], # in_channels, out_channels\n",
    "                       'drop':[0, \n",
    "                               0.25, \n",
    "                               0.25]\n",
    "                      }\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(*self.params['conv'][1])\n",
    "        self.conv2 = nn.Conv2d(*self.params['conv'][2])\n",
    "        self.conv3 = nn.Conv2d(*self.params['conv'][3])\n",
    "        self.conv4 = nn.Conv2d(*self.params['conv'][4])\n",
    "        \n",
    "        self.pool1 = nn.MaxPool2d(*self.params['pool'][1])\n",
    "        self.pool2 = nn.MaxPool2d(*self.params['pool'][2])\n",
    "        \n",
    "        self.fc1 = nn.Linear(*self.params['fc'][1])\n",
    "        self.fc2 = nn.Linear(*self.params['fc'][2])\n",
    "        self.fc3 = nn.Linear(*self.params['fc'][3])\n",
    "        \n",
    "        # self.drop1 = nn.Dropout2d(self.params['drop'][1])\n",
    "        # self.drop2 = nn.Dropout2d(self.params['drop'][2])\n",
    "        \n",
    "        self.printed = False\n",
    "\n",
    "        # TODO: Initialize layers\n",
    "\n",
    "    def forward(self, img):\n",
    "\n",
    "        # TODO: Implement forward pass\n",
    "        x = img\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        if not self.printed: \n",
    "            print(\"CONV1\", x.size())\n",
    "        x = F.relu(self.conv2(x))\n",
    "        if not self.printed: \n",
    "            print(\"CONV2\", x.size())\n",
    "        x = self.pool1(x)\n",
    "        if not self.printed: \n",
    "            print(\"POOL1\", x.size())\n",
    "        \n",
    "        x = F.relu(self.conv3(x))\n",
    "        if not self.printed: \n",
    "            print(\"CONV3\", x.size())\n",
    "        x = F.relu(self.conv4(x))\n",
    "        if not self.printed: \n",
    "            print(\"CONV4\", x.size())\n",
    "        x = self.pool2(x)\n",
    "        if not self.printed: \n",
    "            print(\"POOL2\", x.size())\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        if not self.printed: \n",
    "            print(\"FC1\", x.size())\n",
    "        '''x = self.drop1(x)\n",
    "        if not self.printed: \n",
    "            print(\"DROP1\", x.size())'''\n",
    "        x = F.relu(self.fc2(x))\n",
    "        if not self.printed: \n",
    "            print(\"FC2\", x.size())\n",
    "        x = self.fc3(x)\n",
    "        if not self.printed: \n",
    "            print(\"FC3\", x.size())\n",
    "            self.printed = True\n",
    "\n",
    "        return x\n",
    "\n",
    "# TODO: You can change these data augmentation and normalization strategies for\n",
    "#  better training and testing (https://pytorch.org/vision/stable/transforms.html)\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((32,32)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((32,32)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Dataset initialization\n",
    "data_dir = 'data' # Suppose the dataset is stored under this folder\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'test']} # Read train and test sets, respectively.\n",
    "\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=0) for x in ['train', 'test']}\n",
    "# trainloader = torch.utils.data.DataLoader(image_datasets['train'], batch_size=4, shuffle=True, num_workers=2)\n",
    "# teatloader = torch.utils.data.DataLoader(image_datasets['test'], batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'test']}\n",
    "\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # Set device to \"cpu\" if you have no gpu\n",
    "\n",
    "# TODO: Implement training and testing procedures (https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html)\n",
    "def train_test(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    \n",
    "    for epoch in range(num_epochs):  \n",
    "\n",
    "        running_loss = 0.0\n",
    "        loss_record=[]\n",
    "        for i, data in enumerate(dataloaders['train'], 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "                loss_record.append(running_loss / 2000)\n",
    "                # print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000))\n",
    "                running_loss = 0.0\n",
    "                \n",
    "        print(datetime.datetime.now(), ' Epoch', (epoch + 1), ': Average Loss', loss_record)\n",
    "\n",
    "    print('Finished Training')\n",
    "    \n",
    "    \n",
    "    # save training results\n",
    "    PATH = './cifar_net.pth'\n",
    "    torch.save(model.state_dict(), PATH)\n",
    "    \n",
    "    \n",
    "    # testing overall correct rate\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloaders['train'], 0):\n",
    "            images, labels = data\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = model(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Training accuracy: %d %%' % (100 * correct / total))\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloaders['test'], 0):\n",
    "            images, labels = data\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = model(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Testing accuracy: %d %%' % (100 * correct / total))\n",
    "    \n",
    "    # prepare to count predictions for each class\n",
    "    correct_pred = {classname: 0 for classname in class_names}\n",
    "    total_pred = {classname: 0 for classname in class_names}\n",
    "\n",
    "    # again no gradients needed\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloaders['test'], 0):\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            # collect the correct predictions for each class\n",
    "            for label, prediction in zip(labels, predictions):\n",
    "                if label == prediction:\n",
    "                    correct_pred[class_names[label]] += 1\n",
    "                total_pred[class_names[label]] += 1\n",
    "\n",
    "\n",
    "    # print accuracy for each class\n",
    "    print(\"Testing accuracy (each class): \")\n",
    "    for classname, correct_count in correct_pred.items():\n",
    "        accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "        print(\"{:1s}: {:.1f}%;  \".format(classname, accuracy), end=' ')\n",
    "    print()    \n",
    "    \n",
    "    return None\n",
    "\n",
    "model_ft = Net() # Model initialization\n",
    "\n",
    "model_ft = model_ft.to(device) # Move model to cpu\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # Loss function initialization\n",
    "\n",
    "# TODO: Adjust the following hyper-parameters: learning rate, decay strategy, number of training epochs.\n",
    "optimizer_ft = optim.Adam(model_ft.parameters(), lr=1e-3) # Optimizer initialization\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=20, gamma=0.1) # Learning rate decay strategy\n",
    "\n",
    "for n in range(6):\n",
    "    print(datetime.datetime.now())\n",
    "    epo = 5*n+5\n",
    "    print(\"epoch range: \", epo-4, \" to \", epo)\n",
    "    train_test(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exponential\n",
      "2021-05-06 19:08:26.184762\n",
      "epoch range:  1  to  5\n",
      "CONV1 torch.Size([4, 16, 30, 30])\n",
      "CONV2 torch.Size([4, 32, 30, 30])\n",
      "POOL1 torch.Size([4, 32, 15, 15])\n",
      "CONV3 torch.Size([4, 32, 13, 13])\n",
      "CONV4 torch.Size([4, 16, 13, 13])\n",
      "POOL2 torch.Size([4, 16, 6, 6])\n",
      "FC1 torch.Size([4, 120])\n",
      "FC2 torch.Size([4, 90])\n",
      "FC3 torch.Size([4, 10])\n",
      "2021-05-06 19:11:11.604472  Epoch 1 : Average Loss [2.184932445913553, 1.5080651405006646, 1.2355563238412142]\n",
      "2021-05-06 19:13:48.296891  Epoch 2 : Average Loss [0.9596751684837509, 0.8605482460223138, 0.8012347627994605]\n",
      "2021-05-06 19:16:31.462133  Epoch 3 : Average Loss [0.7033124383199029, 0.6965837975306204, 0.6829580159215256]\n",
      "2021-05-06 19:19:03.385788  Epoch 4 : Average Loss [0.622213598640752, 0.6101786483501782, 0.6050627870413591]\n",
      "2021-05-06 19:21:31.522503  Epoch 5 : Average Loss [0.5486901869789872, 0.5490558464560891, 0.5538750369380286]\n",
      "Finished Training\n",
      "Training accuracy: 85 %\n",
      "Testing accuracy: 83 %\n",
      "Testing accuracy (each class): \n",
      "0: 88.0%;   1: 83.4%;   2: 84.2%;   3: 74.6%;   4: 87.6%;   5: 80.4%;   6: 78.4%;   7: 88.6%;   8: 83.6%;   9: 84.0%;   \n",
      "2021-05-06 19:22:55.504771\n",
      "epoch range:  6  to  10\n",
      "2021-05-06 19:25:15.178792  Epoch 1 : Average Loss [0.4854667235595407, 0.5042891558517076, 0.5172443166161538]\n",
      "2021-05-06 19:27:32.721900  Epoch 2 : Average Loss [0.4718255075171837, 0.4484659046757661, 0.46885624213388655]\n",
      "2021-05-06 19:30:07.811667  Epoch 3 : Average Loss [0.4133255958694717, 0.41857018134094687, 0.43423920797553184]\n",
      "2021-05-06 19:32:36.159960  Epoch 4 : Average Loss [0.40419292340168433, 0.4019273038323281, 0.40660480271571575]\n",
      "2021-05-06 19:34:56.184664  Epoch 5 : Average Loss [0.37298059686568014, 0.3758076565365918, 0.385661634932716]\n",
      "Finished Training\n",
      "Training accuracy: 88 %\n",
      "Testing accuracy: 85 %\n",
      "Testing accuracy (each class): \n",
      "0: 84.6%;   1: 80.6%;   2: 87.2%;   3: 84.4%;   4: 88.4%;   5: 84.0%;   6: 84.0%;   7: 89.8%;   8: 82.0%;   9: 86.6%;   \n",
      "2021-05-06 19:36:12.874728\n",
      "epoch range:  11  to  15\n",
      "2021-05-06 19:38:40.427649  Epoch 1 : Average Loss [0.3575139123042827, 0.38232325250592114, 0.3496908033501777]\n",
      "2021-05-06 19:41:29.090825  Epoch 2 : Average Loss [0.33552202750092547, 0.34587511705531004, 0.3340016242578713]\n",
      "2021-05-06 19:44:06.287971  Epoch 3 : Average Loss [0.3192571779869363, 0.3126139727232228, 0.32764417930227135]\n",
      "2021-05-06 19:46:44.356114  Epoch 4 : Average Loss [0.314816887935056, 0.3173189481910167, 0.30282429032281083]\n",
      "2021-05-06 19:49:15.663011  Epoch 5 : Average Loss [0.27331604478977534, 0.3091901196698184, 0.29472750467246905]\n",
      "Finished Training\n",
      "Training accuracy: 91 %\n",
      "Testing accuracy: 86 %\n",
      "Testing accuracy (each class): \n",
      "0: 87.4%;   1: 83.4%;   2: 87.6%;   3: 75.2%;   4: 87.6%;   5: 85.2%;   6: 85.8%;   7: 92.6%;   8: 89.4%;   9: 87.2%;   \n",
      "2021-05-06 19:50:40.755073\n",
      "epoch range:  16  to  20\n",
      "2021-05-06 19:53:09.061749  Epoch 1 : Average Loss [0.2765874411686832, 0.26795522739253874, 0.29927797812639667]\n",
      "2021-05-06 19:55:46.173336  Epoch 2 : Average Loss [0.2564634196820163, 0.2689674595967194, 0.2818202448671818]\n",
      "2021-05-06 19:58:35.033099  Epoch 3 : Average Loss [0.24444672015229013, 0.2717644347816497, 0.2564272261707317]\n",
      "2021-05-06 20:01:07.002101  Epoch 4 : Average Loss [0.2301389152073433, 0.24060171354581114, 0.2519983177475526]\n",
      "2021-05-06 20:03:44.015648  Epoch 5 : Average Loss [0.2218436712652818, 0.23861512922497047, 0.2246249104032667]\n",
      "Finished Training\n",
      "Training accuracy: 93 %\n",
      "Testing accuracy: 86 %\n",
      "Testing accuracy (each class): \n",
      "0: 88.4%;   1: 88.4%;   2: 85.2%;   3: 76.0%;   4: 90.2%;   5: 87.2%;   6: 85.0%;   7: 88.6%;   8: 88.8%;   9: 87.8%;   \n",
      "2021-05-06 20:05:26.185591\n",
      "epoch range:  21  to  25\n",
      "2021-05-06 20:07:37.869211  Epoch 1 : Average Loss [0.21346753736966498, 0.21712714124417806, 0.23688207844651732]\n",
      "2021-05-06 20:10:04.180717  Epoch 2 : Average Loss [0.19937644035725735, 0.22335125306623146, 0.21148438932736682]\n",
      "2021-05-06 20:12:07.750944  Epoch 3 : Average Loss [0.1852951542547412, 0.202455862484218, 0.21454051962772303]\n",
      "2021-05-06 20:14:35.041722  Epoch 4 : Average Loss [0.1718723876042583, 0.1916001990786794, 0.2003133167678319]\n",
      "2021-05-06 20:16:55.749353  Epoch 5 : Average Loss [0.19053144576893039, 0.1705542912246872, 0.19320842037231012]\n",
      "Finished Training\n",
      "Training accuracy: 94 %\n",
      "Testing accuracy: 86 %\n",
      "Testing accuracy (each class): \n",
      "0: 85.4%;   1: 87.4%;   2: 89.8%;   3: 87.0%;   4: 86.4%;   5: 81.6%;   6: 86.4%;   7: 89.6%;   8: 86.4%;   9: 85.0%;   \n",
      "2021-05-06 20:18:24.987665\n",
      "epoch range:  26  to  30\n",
      "2021-05-06 20:20:33.406967  Epoch 1 : Average Loss [0.15966143333659585, 0.17399201236335898, 0.18125001476777663]\n",
      "2021-05-06 20:22:56.401710  Epoch 2 : Average Loss [0.16792812507352517, 0.16484163416992295, 0.17448065605269206]\n",
      "2021-05-06 20:26:04.683671  Epoch 3 : Average Loss [0.14974954366788396, 0.15770042096572495, 0.1696664982111122]\n",
      "2021-05-06 20:29:30.905526  Epoch 4 : Average Loss [0.15137668450006392, 0.1474671183403458, 0.16490471658466135]\n",
      "2021-05-06 20:32:14.420605  Epoch 5 : Average Loss [0.1406445288018023, 0.14421301661022481, 0.15035289977141722]\n",
      "Finished Training\n",
      "Training accuracy: 96 %\n",
      "Testing accuracy: 87 %\n",
      "Testing accuracy (each class): \n",
      "0: 89.4%;   1: 87.8%;   2: 90.2%;   3: 78.6%;   4: 90.8%;   5: 86.0%;   6: 85.4%;   7: 92.0%;   8: 85.4%;   9: 89.2%;   \n"
     ]
    }
   ],
   "source": [
    "# ExponentialLR\n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import datasets, transforms, utils\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import datetime\n",
    "\n",
    "# TODO: Implement a convolutional neural network (https://pytorch.org/tutorials/recipes/recipes/defining_a_neural_network.html)\n",
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Input - 1x32x32\n",
    "    Output - 10\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.params = {'conv':[(), \n",
    "                               (3, 16, 5, 1, 1), \n",
    "                               (16, 32, 3, 1, 1),\n",
    "                               (32, 32, 3, 1, 0),\n",
    "                               (32, 16, 3, 1, 1)], # in_channels, out_channels, kernel_size, stride, padding\n",
    "                       'pool':[(), \n",
    "                               (2, 2, 0),\n",
    "                               (2, 2, 0)], # kernel_size, stride, padding\n",
    "                       'fc':[(), \n",
    "                             (16*6*6, 120),\n",
    "                             (120, 90), \n",
    "                             (90, 10)], # in_channels, out_channels\n",
    "                       'drop':[0, \n",
    "                               0.25, \n",
    "                               0.25]\n",
    "                      }\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(*self.params['conv'][1])\n",
    "        self.conv2 = nn.Conv2d(*self.params['conv'][2])\n",
    "        self.conv3 = nn.Conv2d(*self.params['conv'][3])\n",
    "        self.conv4 = nn.Conv2d(*self.params['conv'][4])\n",
    "        \n",
    "        self.pool1 = nn.MaxPool2d(*self.params['pool'][1])\n",
    "        self.pool2 = nn.MaxPool2d(*self.params['pool'][2])\n",
    "        \n",
    "        self.fc1 = nn.Linear(*self.params['fc'][1])\n",
    "        self.fc2 = nn.Linear(*self.params['fc'][2])\n",
    "        self.fc3 = nn.Linear(*self.params['fc'][3])\n",
    "        \n",
    "        # self.drop1 = nn.Dropout2d(self.params['drop'][1])\n",
    "        # self.drop2 = nn.Dropout2d(self.params['drop'][2])\n",
    "        \n",
    "        self.printed = False\n",
    "\n",
    "        # TODO: Initialize layers\n",
    "\n",
    "    def forward(self, img):\n",
    "\n",
    "        # TODO: Implement forward pass\n",
    "        x = img\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        if not self.printed: \n",
    "            print(\"CONV1\", x.size())\n",
    "        x = F.relu(self.conv2(x))\n",
    "        if not self.printed: \n",
    "            print(\"CONV2\", x.size())\n",
    "        x = self.pool1(x)\n",
    "        if not self.printed: \n",
    "            print(\"POOL1\", x.size())\n",
    "        \n",
    "        x = F.relu(self.conv3(x))\n",
    "        if not self.printed: \n",
    "            print(\"CONV3\", x.size())\n",
    "        x = F.relu(self.conv4(x))\n",
    "        if not self.printed: \n",
    "            print(\"CONV4\", x.size())\n",
    "        x = self.pool2(x)\n",
    "        if not self.printed: \n",
    "            print(\"POOL2\", x.size())\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        if not self.printed: \n",
    "            print(\"FC1\", x.size())\n",
    "        '''x = self.drop1(x)\n",
    "        if not self.printed: \n",
    "            print(\"DROP1\", x.size())'''\n",
    "        x = F.relu(self.fc2(x))\n",
    "        if not self.printed: \n",
    "            print(\"FC2\", x.size())\n",
    "        x = self.fc3(x)\n",
    "        if not self.printed: \n",
    "            print(\"FC3\", x.size())\n",
    "            self.printed = True\n",
    "\n",
    "        return x\n",
    "\n",
    "# TODO: You can change these data augmentation and normalization strategies for\n",
    "#  better training and testing (https://pytorch.org/vision/stable/transforms.html)\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((32,32)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((32,32)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Dataset initialization\n",
    "data_dir = 'data' # Suppose the dataset is stored under this folder\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'test']} # Read train and test sets, respectively.\n",
    "\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=0) for x in ['train', 'test']}\n",
    "# trainloader = torch.utils.data.DataLoader(image_datasets['train'], batch_size=4, shuffle=True, num_workers=2)\n",
    "# teatloader = torch.utils.data.DataLoader(image_datasets['test'], batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'test']}\n",
    "\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # Set device to \"cpu\" if you have no gpu\n",
    "\n",
    "# TODO: Implement training and testing procedures (https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html)\n",
    "def train_test(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    \n",
    "    for epoch in range(num_epochs):  \n",
    "\n",
    "        running_loss = 0.0\n",
    "        loss_record=[]\n",
    "        for i, data in enumerate(dataloaders['train'], 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "                loss_record.append(running_loss / 2000)\n",
    "                # print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000))\n",
    "                running_loss = 0.0\n",
    "                \n",
    "        print(datetime.datetime.now(), ' Epoch', (epoch + 1), ': Average Loss', loss_record)\n",
    "\n",
    "    print('Finished Training')\n",
    "    \n",
    "    \n",
    "    # save training results\n",
    "    PATH = './cifar_net.pth'\n",
    "    torch.save(model.state_dict(), PATH)\n",
    "    \n",
    "    \n",
    "    # testing overall correct rate\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloaders['train'], 0):\n",
    "            images, labels = data\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = model(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Training accuracy: %d %%' % (100 * correct / total))\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloaders['test'], 0):\n",
    "            images, labels = data\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = model(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Testing accuracy: %d %%' % (100 * correct / total))\n",
    "    \n",
    "    # prepare to count predictions for each class\n",
    "    correct_pred = {classname: 0 for classname in class_names}\n",
    "    total_pred = {classname: 0 for classname in class_names}\n",
    "\n",
    "    # again no gradients needed\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloaders['test'], 0):\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            # collect the correct predictions for each class\n",
    "            for label, prediction in zip(labels, predictions):\n",
    "                if label == prediction:\n",
    "                    correct_pred[class_names[label]] += 1\n",
    "                total_pred[class_names[label]] += 1\n",
    "\n",
    "\n",
    "    # print accuracy for each class\n",
    "    print(\"Testing accuracy (each class): \")\n",
    "    for classname, correct_count in correct_pred.items():\n",
    "        accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "        print(\"{:1s}: {:.1f}%;  \".format(classname, accuracy), end=' ')\n",
    "    print()    \n",
    "    \n",
    "    return None\n",
    "\n",
    "model_ft = Net() # Model initialization\n",
    "\n",
    "model_ft = model_ft.to(device) # Move model to cpu\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # Loss function initialization\n",
    "\n",
    "# TODO: Adjust the following hyper-parameters: learning rate, decay strategy, number of training epochs.\n",
    "optimizer_ft = optim.Adam(model_ft.parameters(), lr=1e-4) # Optimizer initialization\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=20, gamma=0.1) # Learning rate decay strategy\n",
    "#cyc_lr_scheduler = lr_scheduler.CyclicLR(optimizer_ft, base_lr=1e-4, max_lr=5e-4, step_size_up=21, step_size_down=19, mode='triangular', gamma=0.1, scale_fn=None, scale_mode='cycle', cycle_momentum=True, base_momentum=0.8, max_momentum=0.9, last_epoch=-1)\n",
    "expon_lr_scheduler = lr_scheduler.ExponentialLR(optimizer_ft, gamma=0.1)\n",
    "\n",
    "'''print(\"cyclic\")\n",
    "for n in range(6):\n",
    "    print(datetime.datetime.now())\n",
    "    epo = 5*n+5\n",
    "    print(\"epoch range: \", epo-4, \" to \", epo)\n",
    "    train_test(model_ft, criterion, optimizer_ft, cyc_lr_scheduler, num_epochs=5)'''\n",
    "print(\"exponential\")\n",
    "for n in range(6):\n",
    "    print(datetime.datetime.now())\n",
    "    epo = 5*n+5\n",
    "    print(\"epoch range: \", epo-4, \" to \", epo)\n",
    "    train_test(model_ft, criterion, optimizer_ft, expon_lr_scheduler, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
