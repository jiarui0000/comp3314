{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-06 20:49:14.772196\n",
      "epoch range:  1  to  5\n",
      "CONV1 torch.Size([4, 16, 30, 30])\n",
      "CONV2 torch.Size([4, 32, 30, 30])\n",
      "POOL1 torch.Size([4, 32, 15, 15])\n",
      "CONV3 torch.Size([4, 32, 13, 13])\n",
      "CONV4 torch.Size([4, 16, 13, 13])\n",
      "POOL2 torch.Size([4, 16, 6, 6])\n",
      "FC1 torch.Size([4, 120])\n",
      "FC2 torch.Size([4, 90])\n",
      "FC3 torch.Size([4, 10])\n",
      "2021-05-06 20:50:34.252678  Epoch 1 : Average Loss [2.256450728416443, 1.4969576964974403, 1.2289507491793483]\n",
      "2021-05-06 20:51:24.518523  Epoch 2 : Average Loss [1.0001430722074582, 0.9434580509588122, 0.8630769251538223]\n",
      "2021-05-06 20:52:17.554406  Epoch 3 : Average Loss [0.7628402986791916, 0.7584733539889567, 0.7047955251077656]\n",
      "2021-05-06 20:53:09.944474  Epoch 4 : Average Loss [0.6560948984310963, 0.6334348885513609, 0.6289269694458927]\n",
      "2021-05-06 20:54:02.320431  Epoch 5 : Average Loss [0.5779247439355532, 0.562194696746068, 0.550704367017257]\n",
      "Finished Training\n",
      "Training accuracy: 83 %\n",
      "Testing accuracy: 82 %\n",
      "Testing accuracy (each class): \n",
      "0: 89.8%;   1: 86.6%;   2: 86.6%;   3: 73.8%;   4: 84.8%;   5: 78.4%;   6: 74.0%;   7: 85.2%;   8: 81.8%;   9: 79.6%;   \n",
      "2021-05-06 20:54:35.474871\n",
      "epoch range:  6  to  10\n",
      "2021-05-06 20:55:27.772737  Epoch 1 : Average Loss [0.5184117190673023, 0.49544954473289543, 0.5189514948612632]\n",
      "2021-05-06 20:56:19.156127  Epoch 2 : Average Loss [0.4700050302936361, 0.46491971829348766, 0.4915798759192185]\n",
      "2021-05-06 20:57:11.546824  Epoch 3 : Average Loss [0.4350982928688318, 0.4439276163229488, 0.4400245511771318]\n",
      "2021-05-06 20:58:04.157958  Epoch 4 : Average Loss [0.4077270945334312, 0.407392973390135, 0.41237439948040994]\n",
      "2021-05-06 20:59:01.911677  Epoch 5 : Average Loss [0.37242342821536295, 0.39390079340807277, 0.3904075068112652]\n",
      "Finished Training\n",
      "Training accuracy: 89 %\n",
      "Testing accuracy: 84 %\n",
      "Testing accuracy (each class): \n",
      "0: 88.8%;   1: 84.8%;   2: 83.2%;   3: 75.6%;   4: 88.0%;   5: 85.0%;   6: 83.2%;   7: 89.4%;   8: 83.0%;   9: 87.2%;   \n",
      "2021-05-06 20:59:34.488673\n",
      "epoch range:  11  to  15\n",
      "2021-05-06 21:00:29.043594  Epoch 1 : Average Loss [0.36497564402024385, 0.3743492659178864, 0.36675593837674025]\n",
      "2021-05-06 21:01:38.006447  Epoch 2 : Average Loss [0.33699032285139036, 0.34593069588968595, 0.3550512872496665]\n",
      "2021-05-06 21:02:39.321883  Epoch 3 : Average Loss [0.3038898858646271, 0.3596155992870845, 0.32815267805786086]\n",
      "2021-05-06 21:03:42.851439  Epoch 4 : Average Loss [0.3151140286630568, 0.31348110763123543, 0.29799740892188403]\n",
      "2021-05-06 21:04:43.984304  Epoch 5 : Average Loss [0.2805460947671718, 0.31118075856671523, 0.3090578205542888]\n",
      "Finished Training\n",
      "Training accuracy: 91 %\n",
      "Testing accuracy: 85 %\n",
      "Testing accuracy (each class): \n",
      "0: 87.0%;   1: 86.2%;   2: 88.8%;   3: 80.6%;   4: 92.8%;   5: 87.8%;   6: 81.0%;   7: 87.8%;   8: 83.2%;   9: 83.2%;   \n",
      "2021-05-06 21:05:20.296482\n",
      "epoch range:  16  to  20\n",
      "2021-05-06 21:06:23.531932  Epoch 1 : Average Loss [0.27988107256830064, 0.30329751253505377, 0.2763950683080559]\n",
      "2021-05-06 21:07:24.219295  Epoch 2 : Average Loss [0.2604399686550792, 0.2729650627914466, 0.2873573397251642]\n",
      "2021-05-06 21:08:22.809038  Epoch 3 : Average Loss [0.2509980979497183, 0.254774724059848, 0.26480585489145253]\n",
      "2021-05-06 21:09:23.301300  Epoch 4 : Average Loss [0.22676293900281264, 0.2596673908660262, 0.24322822304591363]\n",
      "2021-05-06 21:10:23.429180  Epoch 5 : Average Loss [0.22748217605732243, 0.22615532286685766, 0.25914241577668484]\n",
      "Finished Training\n",
      "Training accuracy: 93 %\n",
      "Testing accuracy: 85 %\n",
      "Testing accuracy (each class): \n",
      "0: 85.2%;   1: 87.4%;   2: 87.0%;   3: 81.6%;   4: 89.4%;   5: 87.6%;   6: 77.4%;   7: 89.6%;   8: 87.0%;   9: 86.6%;   \n",
      "2021-05-06 21:10:57.161605\n",
      "epoch range:  21  to  25\n",
      "2021-05-06 21:11:55.835510  Epoch 1 : Average Loss [0.21528366011399155, 0.23915300892263822, 0.2282022030052169]\n",
      "2021-05-06 21:12:57.029556  Epoch 2 : Average Loss [0.2089074918749312, 0.21556307520184104, 0.22383136529198994]\n",
      "2021-05-06 21:13:59.818142  Epoch 3 : Average Loss [0.20844814602567385, 0.19749044601272533, 0.19703964922202533]\n",
      "2021-05-06 21:14:53.501137  Epoch 4 : Average Loss [0.1835687386354667, 0.19633879326655085, 0.19187729163708522]\n",
      "2021-05-06 21:15:51.629613  Epoch 5 : Average Loss [0.17462410403741707, 0.18508217885618433, 0.1939206948646313]\n",
      "Finished Training\n",
      "Training accuracy: 94 %\n",
      "Testing accuracy: 86 %\n",
      "Testing accuracy (each class): \n",
      "0: 86.6%;   1: 87.4%;   2: 85.8%;   3: 85.2%;   4: 88.6%;   5: 85.4%;   6: 82.6%;   7: 90.8%;   8: 83.0%;   9: 88.0%;   \n",
      "2021-05-06 21:16:26.728752\n",
      "epoch range:  26  to  30\n",
      "2021-05-06 21:17:22.682826  Epoch 1 : Average Loss [0.17434256798938763, 0.1805227069562081, 0.18655350519142713]\n",
      "2021-05-06 21:18:19.500037  Epoch 2 : Average Loss [0.16608974367189921, 0.16517146274656716, 0.17598187640713575]\n",
      "2021-05-06 21:19:12.008756  Epoch 3 : Average Loss [0.15504469408434285, 0.15781123934354738, 0.16353992451729127]\n",
      "2021-05-06 21:20:10.478160  Epoch 4 : Average Loss [0.1507237178296013, 0.15070489367914758, 0.15466855863654552]\n",
      "2021-05-06 21:21:08.444318  Epoch 5 : Average Loss [0.1460834342288998, 0.1563405455567787, 0.14189765275195598]\n",
      "Finished Training\n",
      "Training accuracy: 96 %\n",
      "Testing accuracy: 86 %\n",
      "Testing accuracy (each class): \n",
      "0: 90.0%;   1: 86.8%;   2: 85.8%;   3: 82.6%;   4: 88.6%;   5: 84.6%;   6: 82.2%;   7: 91.2%;   8: 83.6%;   9: 85.0%;   \n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import datasets, transforms, utils\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import datetime\n",
    "\n",
    "# TODO: Implement a convolutional neural network (https://pytorch.org/tutorials/recipes/recipes/defining_a_neural_network.html)\n",
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Input - 1x32x32\n",
    "    Output - 10\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.params = {'conv':[(), \n",
    "                               (3, 16, 5, 1, 1), \n",
    "                               (16, 32, 3, 1, 1),\n",
    "                               (32, 32, 3, 1, 0),\n",
    "                               (32, 16, 3, 1, 1)], # in_channels, out_channels, kernel_size, stride, padding\n",
    "                       'pool':[(), \n",
    "                               (2, 2, 0),\n",
    "                               (2, 2, 0)], # kernel_size, stride, padding\n",
    "                       'fc':[(), \n",
    "                             (16*6*6, 120),\n",
    "                             (120, 90), \n",
    "                             (90, 10)], # in_channels, out_channels\n",
    "                       'drop':[0, \n",
    "                               0.25, \n",
    "                               0.25]\n",
    "                      }\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(*self.params['conv'][1])\n",
    "        self.conv2 = nn.Conv2d(*self.params['conv'][2])\n",
    "        self.conv3 = nn.Conv2d(*self.params['conv'][3])\n",
    "        self.conv4 = nn.Conv2d(*self.params['conv'][4])\n",
    "        \n",
    "        self.pool1 = nn.MaxPool2d(*self.params['pool'][1])\n",
    "        self.pool2 = nn.MaxPool2d(*self.params['pool'][2])\n",
    "        \n",
    "        self.fc1 = nn.Linear(*self.params['fc'][1])\n",
    "        self.fc2 = nn.Linear(*self.params['fc'][2])\n",
    "        self.fc3 = nn.Linear(*self.params['fc'][3])\n",
    "        \n",
    "        # self.drop1 = nn.Dropout2d(self.params['drop'][1])\n",
    "        # self.drop2 = nn.Dropout2d(self.params['drop'][2])\n",
    "        \n",
    "        self.printed = False\n",
    "\n",
    "        # TODO: Initialize layers\n",
    "\n",
    "    def forward(self, img):\n",
    "\n",
    "        # TODO: Implement forward pass\n",
    "        x = img\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        if not self.printed: \n",
    "            print(\"CONV1\", x.size())\n",
    "        x = F.relu(self.conv2(x))\n",
    "        if not self.printed: \n",
    "            print(\"CONV2\", x.size())\n",
    "        x = self.pool1(x)\n",
    "        if not self.printed: \n",
    "            print(\"POOL1\", x.size())\n",
    "        \n",
    "        x = F.relu(self.conv3(x))\n",
    "        if not self.printed: \n",
    "            print(\"CONV3\", x.size())\n",
    "        x = F.relu(self.conv4(x))\n",
    "        if not self.printed: \n",
    "            print(\"CONV4\", x.size())\n",
    "        x = self.pool2(x)\n",
    "        if not self.printed: \n",
    "            print(\"POOL2\", x.size())\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        if not self.printed: \n",
    "            print(\"FC1\", x.size())\n",
    "        '''x = self.drop1(x)\n",
    "        if not self.printed: \n",
    "            print(\"DROP1\", x.size())'''\n",
    "        x = F.relu(self.fc2(x))\n",
    "        if not self.printed: \n",
    "            print(\"FC2\", x.size())\n",
    "        x = self.fc3(x)\n",
    "        if not self.printed: \n",
    "            print(\"FC3\", x.size())\n",
    "            self.printed = True\n",
    "\n",
    "        return x\n",
    "\n",
    "# TODO: You can change these data augmentation and normalization strategies for\n",
    "#  better training and testing (https://pytorch.org/vision/stable/transforms.html)\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((32,32)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((32,32)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Dataset initialization\n",
    "data_dir = 'data' # Suppose the dataset is stored under this folder\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'test']} # Read train and test sets, respectively.\n",
    "\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=0) for x in ['train', 'test']}\n",
    "# trainloader = torch.utils.data.DataLoader(image_datasets['train'], batch_size=4, shuffle=True, num_workers=2)\n",
    "# teatloader = torch.utils.data.DataLoader(image_datasets['test'], batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'test']}\n",
    "\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # Set device to \"cpu\" if you have no gpu\n",
    "\n",
    "# TODO: Implement training and testing procedures (https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html)\n",
    "def train_test(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    \n",
    "    for epoch in range(num_epochs):  \n",
    "\n",
    "        running_loss = 0.0\n",
    "        loss_record=[]\n",
    "        for i, data in enumerate(dataloaders['train'], 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "                loss_record.append(running_loss / 2000)\n",
    "                # print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000))\n",
    "                running_loss = 0.0\n",
    "                \n",
    "        print(datetime.datetime.now(), ' Epoch', (epoch + 1), ': Average Loss', loss_record)\n",
    "\n",
    "    print('Finished Training')\n",
    "    \n",
    "    \n",
    "    # save training results\n",
    "    PATH = './cifar_net.pth'\n",
    "    torch.save(model.state_dict(), PATH)\n",
    "    \n",
    "    \n",
    "    # testing overall correct rate\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloaders['train'], 0):\n",
    "            images, labels = data\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = model(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Training accuracy: %d %%' % (100 * correct / total))\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloaders['test'], 0):\n",
    "            images, labels = data\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = model(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Testing accuracy: %d %%' % (100 * correct / total))\n",
    "    \n",
    "    # prepare to count predictions for each class\n",
    "    correct_pred = {classname: 0 for classname in class_names}\n",
    "    total_pred = {classname: 0 for classname in class_names}\n",
    "\n",
    "    # again no gradients needed\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloaders['test'], 0):\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            # collect the correct predictions for each class\n",
    "            for label, prediction in zip(labels, predictions):\n",
    "                if label == prediction:\n",
    "                    correct_pred[class_names[label]] += 1\n",
    "                total_pred[class_names[label]] += 1\n",
    "\n",
    "\n",
    "    # print accuracy for each class\n",
    "    print(\"Testing accuracy (each class): \")\n",
    "    for classname, correct_count in correct_pred.items():\n",
    "        accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "        print(\"{:1s}: {:.1f}%;  \".format(classname, accuracy), end=' ')\n",
    "    print()    \n",
    "    \n",
    "    return None\n",
    "\n",
    "model_ft = Net() # Model initialization\n",
    "\n",
    "model_ft = model_ft.to(device) # Move model to cpu\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # Loss function initialization\n",
    "\n",
    "# TODO: Adjust the following hyper-parameters: learning rate, decay strategy, number of training epochs.\n",
    "optimizer_ft = optim.Adam(model_ft.parameters(), lr=1e-4) # Optimizer initialization\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=20, gamma=0.1) # Learning rate decay strategy\n",
    "\n",
    "for n in range(6):\n",
    "    print(datetime.datetime.now())\n",
    "    epo = 5*n+5\n",
    "    print(\"epoch range: \", epo-4, \" to \", epo)\n",
    "    train_test(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-06 16:40:06.488670\n",
      "epoch range:  1  to  5\n",
      "CONV1 torch.Size([4, 16, 30, 30])\n",
      "CONV2 torch.Size([4, 32, 30, 30])\n",
      "POOL1 torch.Size([4, 32, 15, 15])\n",
      "CONV3 torch.Size([4, 32, 13, 13])\n",
      "CONV4 torch.Size([4, 16, 13, 13])\n",
      "POOL2 torch.Size([4, 16, 6, 6])\n",
      "FC1 torch.Size([4, 120])\n",
      "FC2 torch.Size([4, 90])\n",
      "FC3 torch.Size([4, 10])\n",
      "2021-05-06 16:41:05.581086  Epoch 1 : Average Loss [2.3043154529333116, 2.3045051860809327, 2.3042086980342864]\n",
      "2021-05-06 16:41:59.618088  Epoch 2 : Average Loss [2.3027573450803756, 2.304300184249878, 2.3047116711139677]\n",
      "2021-05-06 16:43:10.421231  Epoch 3 : Average Loss [2.3051510660648344, 2.303858541727066, 2.304147281527519]\n",
      "2021-05-06 16:44:17.686033  Epoch 4 : Average Loss [2.303972874045372, 2.3040791311264037, 2.3041595437526703]\n",
      "2021-05-06 16:45:16.387825  Epoch 5 : Average Loss [2.3041162202358247, 2.3037228739261626, 2.3041245189905166]\n",
      "Finished Training\n",
      "Training accuracy: 10 %\n",
      "Testing accuracy: 10 %\n",
      "Testing accuracy (each class): \n",
      "0: 0.0%;   1: 0.0%;   2: 0.0%;   3: 0.0%;   4: 0.0%;   5: 0.0%;   6: 100.0%;   7: 0.0%;   8: 0.0%;   9: 0.0%;   \n",
      "2021-05-06 16:45:51.302637\n",
      "epoch range:  6  to  10\n",
      "2021-05-06 16:46:53.065745  Epoch 1 : Average Loss [2.3044177260398864, 2.3046243772506716, 2.304252769112587]\n",
      "2021-05-06 16:47:55.016583  Epoch 2 : Average Loss [2.303966151833534, 2.3044219113588333, 2.3041930882930757]\n",
      "2021-05-06 16:48:53.420801  Epoch 3 : Average Loss [2.305077434659004, 2.304637652873993, 2.303223826289177]\n",
      "2021-05-06 16:49:54.141636  Epoch 4 : Average Loss [2.30338006067276, 2.3044829570055008, 2.30452843272686]\n",
      "2021-05-06 16:50:52.032513  Epoch 5 : Average Loss [2.3037023512125017, 2.3045400552749635, 2.3040272575616836]\n",
      "Finished Training\n",
      "Training accuracy: 10 %\n",
      "Testing accuracy: 10 %\n",
      "Testing accuracy (each class): \n",
      "0: 0.0%;   1: 0.0%;   2: 0.0%;   3: 0.0%;   4: 0.0%;   5: 0.0%;   6: 100.0%;   7: 0.0%;   8: 0.0%;   9: 0.0%;   \n",
      "2021-05-06 16:51:26.446762\n",
      "epoch range:  11  to  15\n",
      "2021-05-06 16:52:23.091108  Epoch 1 : Average Loss [2.303218061327934, 2.3042212270498275, 2.304395101070404]\n",
      "2021-05-06 16:53:21.408743  Epoch 2 : Average Loss [2.303494477391243, 2.3044422104358673, 2.3036237671375273]\n",
      "2021-05-06 16:54:18.826306  Epoch 3 : Average Loss [2.3039246524572374, 2.3035950490236283, 2.3035900802612304]\n",
      "2021-05-06 16:55:17.492244  Epoch 4 : Average Loss [2.3036023069620133, 2.304266403198242, 2.3035171353816986]\n",
      "2021-05-06 16:56:14.931700  Epoch 5 : Average Loss [2.303829097390175, 2.3040354113578796, 2.3034952077865603]\n",
      "Finished Training\n",
      "Training accuracy: 10 %\n",
      "Testing accuracy: 10 %\n",
      "Testing accuracy (each class): \n",
      "0: 0.0%;   1: 0.0%;   2: 0.0%;   3: 0.0%;   4: 0.0%;   5: 0.0%;   6: 100.0%;   7: 0.0%;   8: 0.0%;   9: 0.0%;   \n",
      "2021-05-06 16:56:49.675507\n",
      "epoch range:  16  to  20\n",
      "2021-05-06 16:57:46.077695  Epoch 1 : Average Loss [2.303361713171005, 2.30318518447876, 2.3041020197868347]\n",
      "2021-05-06 16:58:46.304397  Epoch 2 : Average Loss [2.3046165373325347, 2.30314210999012, 2.3044151314496992]\n",
      "2021-05-06 16:59:46.528313  Epoch 3 : Average Loss [2.3041057963371276, 2.3044942957162857, 2.302790130376816]\n",
      "2021-05-06 17:00:44.464475  Epoch 4 : Average Loss [2.3030620889663695, 2.3043007060289384, 2.3051239005327226]\n",
      "2021-05-06 17:01:41.858777  Epoch 5 : Average Loss [2.3036599777936937, 2.3042588460445406, 2.303579719185829]\n",
      "Finished Training\n",
      "Training accuracy: 10 %\n",
      "Testing accuracy: 10 %\n",
      "Testing accuracy (each class): \n",
      "0: 0.0%;   1: 0.0%;   2: 0.0%;   3: 0.0%;   4: 0.0%;   5: 0.0%;   6: 100.0%;   7: 0.0%;   8: 0.0%;   9: 0.0%;   \n",
      "2021-05-06 17:02:18.805655\n",
      "epoch range:  21  to  25\n",
      "2021-05-06 17:03:16.794139  Epoch 1 : Average Loss [2.3038649369478224, 2.303974863409996, 2.3036461873054503]\n",
      "2021-05-06 17:04:15.208943  Epoch 2 : Average Loss [2.303770859837532, 2.3039993188381196, 2.304053900718689]\n",
      "2021-05-06 17:05:13.148563  Epoch 3 : Average Loss [2.3032824845314024, 2.3038496092557907, 2.304706813573837]\n",
      "2021-05-06 17:06:11.770859  Epoch 4 : Average Loss [2.3029425753355026, 2.3048788138628007, 2.3040023159980776]\n",
      "2021-05-06 17:07:09.791501  Epoch 5 : Average Loss [2.303676019668579, 2.3045263357162478, 2.302822575211525]\n",
      "Finished Training\n",
      "Training accuracy: 10 %\n",
      "Testing accuracy: 10 %\n",
      "Testing accuracy (each class): \n",
      "0: 0.0%;   1: 0.0%;   2: 0.0%;   3: 0.0%;   4: 0.0%;   5: 0.0%;   6: 100.0%;   7: 0.0%;   8: 0.0%;   9: 0.0%;   \n",
      "2021-05-06 17:07:44.320517\n",
      "epoch range:  26  to  30\n",
      "2021-05-06 17:08:42.934054  Epoch 1 : Average Loss [2.3038332531452177, 2.3035614956617354, 2.3044558255672456]\n",
      "2021-05-06 17:09:40.828705  Epoch 2 : Average Loss [2.3039750871658327, 2.303462601661682, 2.3038741139173506]\n",
      "2021-05-06 17:10:38.290767  Epoch 3 : Average Loss [2.303543598175049, 2.303546508193016, 2.3033560190200806]\n",
      "2021-05-06 17:11:35.811704  Epoch 4 : Average Loss [2.3032486494779585, 2.304058141231537, 2.3034237711429597]\n",
      "2021-05-06 17:12:33.958261  Epoch 5 : Average Loss [2.3039625207185743, 2.303689599990845, 2.3034341799020766]\n",
      "Finished Training\n",
      "Training accuracy: 10 %\n",
      "Testing accuracy: 10 %\n",
      "Testing accuracy (each class): \n",
      "0: 0.0%;   1: 0.0%;   2: 0.0%;   3: 0.0%;   4: 0.0%;   5: 0.0%;   6: 100.0%;   7: 0.0%;   8: 0.0%;   9: 0.0%;   \n"
     ]
    }
   ],
   "source": [
    "# optim.Adadelta\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import datasets, transforms, utils\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import datetime\n",
    "\n",
    "# TODO: Implement a convolutional neural network (https://pytorch.org/tutorials/recipes/recipes/defining_a_neural_network.html)\n",
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Input - 1x32x32\n",
    "    Output - 10\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.params = {'conv':[(), \n",
    "                               (3, 16, 5, 1, 1), \n",
    "                               (16, 32, 3, 1, 1),\n",
    "                               (32, 32, 3, 1, 0),\n",
    "                               (32, 16, 3, 1, 1)], # in_channels, out_channels, kernel_size, stride, padding\n",
    "                       'pool':[(), \n",
    "                               (2, 2, 0),\n",
    "                               (2, 2, 0)], # kernel_size, stride, padding\n",
    "                       'fc':[(), \n",
    "                             (16*6*6, 120),\n",
    "                             (120, 90), \n",
    "                             (90, 10)], # in_channels, out_channels\n",
    "                       'drop':[0, \n",
    "                               0.25, \n",
    "                               0.25]\n",
    "                      }\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(*self.params['conv'][1])\n",
    "        self.conv2 = nn.Conv2d(*self.params['conv'][2])\n",
    "        self.conv3 = nn.Conv2d(*self.params['conv'][3])\n",
    "        self.conv4 = nn.Conv2d(*self.params['conv'][4])\n",
    "        \n",
    "        self.pool1 = nn.MaxPool2d(*self.params['pool'][1])\n",
    "        self.pool2 = nn.MaxPool2d(*self.params['pool'][2])\n",
    "        \n",
    "        self.fc1 = nn.Linear(*self.params['fc'][1])\n",
    "        self.fc2 = nn.Linear(*self.params['fc'][2])\n",
    "        self.fc3 = nn.Linear(*self.params['fc'][3])\n",
    "        \n",
    "        # self.drop1 = nn.Dropout2d(self.params['drop'][1])\n",
    "        # self.drop2 = nn.Dropout2d(self.params['drop'][2])\n",
    "        \n",
    "        self.printed = False\n",
    "\n",
    "        # TODO: Initialize layers\n",
    "\n",
    "    def forward(self, img):\n",
    "\n",
    "        # TODO: Implement forward pass\n",
    "        x = img\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        if not self.printed: \n",
    "            print(\"CONV1\", x.size())\n",
    "        x = F.relu(self.conv2(x))\n",
    "        if not self.printed: \n",
    "            print(\"CONV2\", x.size())\n",
    "        x = self.pool1(x)\n",
    "        if not self.printed: \n",
    "            print(\"POOL1\", x.size())\n",
    "        \n",
    "        x = F.relu(self.conv3(x))\n",
    "        if not self.printed: \n",
    "            print(\"CONV3\", x.size())\n",
    "        x = F.relu(self.conv4(x))\n",
    "        if not self.printed: \n",
    "            print(\"CONV4\", x.size())\n",
    "        x = self.pool2(x)\n",
    "        if not self.printed: \n",
    "            print(\"POOL2\", x.size())\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        if not self.printed: \n",
    "            print(\"FC1\", x.size())\n",
    "        '''x = self.drop1(x)\n",
    "        if not self.printed: \n",
    "            print(\"DROP1\", x.size())'''\n",
    "        x = F.relu(self.fc2(x))\n",
    "        if not self.printed: \n",
    "            print(\"FC2\", x.size())\n",
    "        x = self.fc3(x)\n",
    "        if not self.printed: \n",
    "            print(\"FC3\", x.size())\n",
    "            self.printed = True\n",
    "\n",
    "        return x\n",
    "\n",
    "# TODO: You can change these data augmentation and normalization strategies for\n",
    "#  better training and testing (https://pytorch.org/vision/stable/transforms.html)\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((32,32)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((32,32)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Dataset initialization\n",
    "data_dir = 'data' # Suppose the dataset is stored under this folder\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'test']} # Read train and test sets, respectively.\n",
    "\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=0) for x in ['train', 'test']}\n",
    "# trainloader = torch.utils.data.DataLoader(image_datasets['train'], batch_size=4, shuffle=True, num_workers=2)\n",
    "# teatloader = torch.utils.data.DataLoader(image_datasets['test'], batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'test']}\n",
    "\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # Set device to \"cpu\" if you have no gpu\n",
    "\n",
    "# TODO: Implement training and testing procedures (https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html)\n",
    "def train_test(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    \n",
    "    for epoch in range(num_epochs):  \n",
    "\n",
    "        running_loss = 0.0\n",
    "        loss_record=[]\n",
    "        for i, data in enumerate(dataloaders['train'], 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "                loss_record.append(running_loss / 2000)\n",
    "                # print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000))\n",
    "                running_loss = 0.0\n",
    "                \n",
    "        print(datetime.datetime.now(), ' Epoch', (epoch + 1), ': Average Loss', loss_record)\n",
    "\n",
    "    print('Finished Training')\n",
    "    \n",
    "    \n",
    "    # save training results\n",
    "    PATH = './cifar_net.pth'\n",
    "    torch.save(model.state_dict(), PATH)\n",
    "    \n",
    "    \n",
    "    # testing overall correct rate\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloaders['train'], 0):\n",
    "            images, labels = data\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = model(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Training accuracy: %d %%' % (100 * correct / total))\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloaders['test'], 0):\n",
    "            images, labels = data\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = model(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Testing accuracy: %d %%' % (100 * correct / total))\n",
    "    \n",
    "    # prepare to count predictions for each class\n",
    "    correct_pred = {classname: 0 for classname in class_names}\n",
    "    total_pred = {classname: 0 for classname in class_names}\n",
    "\n",
    "    # again no gradients needed\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloaders['test'], 0):\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            # collect the correct predictions for each class\n",
    "            for label, prediction in zip(labels, predictions):\n",
    "                if label == prediction:\n",
    "                    correct_pred[class_names[label]] += 1\n",
    "                total_pred[class_names[label]] += 1\n",
    "\n",
    "\n",
    "    # print accuracy for each class\n",
    "    print(\"Testing accuracy (each class): \")\n",
    "    for classname, correct_count in correct_pred.items():\n",
    "        accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "        print(\"{:1s}: {:.1f}%;  \".format(classname, accuracy), end=' ')\n",
    "    print()    \n",
    "    \n",
    "    return None\n",
    "\n",
    "model_ft = Net() # Model initialization\n",
    "\n",
    "model_ft = model_ft.to(device) # Move model to cpu\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # Loss function initialization\n",
    "\n",
    "# TODO: Adjust the following hyper-parameters: learning rate, decay strategy, number of training epochs.\n",
    "optimizer_ft = optim.Adadelta(model_ft.parameters(), lr=1e-4) # Optimizer initialization\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=20, gamma=0.1) # Learning rate decay strategy\n",
    "\n",
    "for n in range(6):\n",
    "    print(datetime.datetime.now())\n",
    "    epo = 5*n+5\n",
    "    print(\"epoch range: \", epo-4, \" to \", epo)\n",
    "    train_test(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-06 18:13:19.463533\n",
      "epoch range:  1  to  5\n",
      "CONV1 torch.Size([4, 16, 30, 30])\n",
      "CONV2 torch.Size([4, 32, 30, 30])\n",
      "POOL1 torch.Size([4, 32, 15, 15])\n",
      "CONV3 torch.Size([4, 32, 13, 13])\n",
      "CONV4 torch.Size([4, 16, 13, 13])\n",
      "POOL2 torch.Size([4, 16, 6, 6])\n",
      "FC1 torch.Size([4, 120])\n",
      "FC2 torch.Size([4, 90])\n",
      "FC3 torch.Size([4, 10])\n",
      "2021-05-06 18:14:12.039795  Epoch 1 : Average Loss [2.3046225954294206, 2.3030373970270155, 2.3034356145858763]\n",
      "2021-05-06 18:15:10.813733  Epoch 2 : Average Loss [2.3033910413980485, 2.302909638404846, 2.303143483519554]\n",
      "2021-05-06 18:16:11.405797  Epoch 3 : Average Loss [2.303170191168785, 2.3029901208877566, 2.3031493718624114]\n",
      "2021-05-06 18:17:11.464257  Epoch 4 : Average Loss [2.3031673308610916, 2.302951886296272, 2.3031302314996718]\n",
      "2021-05-06 18:18:09.657266  Epoch 5 : Average Loss [2.303080519795418, 2.303159546971321, 2.302984843850136]\n",
      "Finished Training\n",
      "Training accuracy: 10 %\n",
      "Testing accuracy: 10 %\n",
      "Testing accuracy (each class): \n",
      "0: 0.0%;   1: 0.0%;   2: 0.0%;   3: 0.0%;   4: 0.0%;   5: 100.0%;   6: 0.0%;   7: 0.0%;   8: 0.0%;   9: 0.0%;   \n",
      "2021-05-06 18:18:44.298406\n",
      "epoch range:  6  to  10\n",
      "2021-05-06 18:19:45.863893  Epoch 1 : Average Loss [2.302988816022873, 2.3031172016859056, 2.303084499359131]\n",
      "2021-05-06 18:20:52.223174  Epoch 2 : Average Loss [2.302930813193321, 2.3029691984653473, 2.3033941898345947]\n",
      "2021-05-06 18:21:56.444537  Epoch 3 : Average Loss [2.302762973666191, 2.3033040401935576, 2.3027752075195314]\n",
      "2021-05-06 18:23:01.248564  Epoch 4 : Average Loss [2.3031181223392485, 2.3027935869693756, 2.3032601348161696]\n",
      "2021-05-06 18:24:15.802292  Epoch 5 : Average Loss [2.3029578433036804, 2.3029731509685516, 2.3032932599782945]\n",
      "Finished Training\n",
      "Training accuracy: 10 %\n",
      "Testing accuracy: 10 %\n",
      "Testing accuracy (each class): \n",
      "0: 0.0%;   1: 0.0%;   2: 0.0%;   3: 0.0%;   4: 0.0%;   5: 0.0%;   6: 0.0%;   7: 100.0%;   8: 0.0%;   9: 0.0%;   \n",
      "2021-05-06 18:24:50.900682\n",
      "epoch range:  11  to  15\n",
      "2021-05-06 18:26:05.641274  Epoch 1 : Average Loss [2.303137485742569, 2.303155721902847, 2.3030978761911394]\n",
      "2021-05-06 18:27:22.889715  Epoch 2 : Average Loss [2.302996984243393, 2.303167954325676, 2.3032531512975694]\n",
      "2021-05-06 18:28:37.416700  Epoch 3 : Average Loss [2.302945070385933, 2.3031600028276444, 2.3030513832569124]\n",
      "2021-05-06 18:29:54.241441  Epoch 4 : Average Loss [2.3031206600666048, 2.303090579509735, 2.3031738806962965]\n",
      "2021-05-06 18:31:06.753925  Epoch 5 : Average Loss [2.303327810049057, 2.303128144979477, 2.303218946814537]\n",
      "Finished Training\n",
      "Training accuracy: 10 %\n",
      "Testing accuracy: 10 %\n",
      "Testing accuracy (each class): \n",
      "0: 0.0%;   1: 100.0%;   2: 0.0%;   3: 0.0%;   4: 0.0%;   5: 0.0%;   6: 0.0%;   7: 0.0%;   8: 0.0%;   9: 0.0%;   \n",
      "2021-05-06 18:31:40.763425\n",
      "epoch range:  16  to  20\n",
      "2021-05-06 18:32:59.550378  Epoch 1 : Average Loss [2.3025819537639616, 2.30319626390934, 2.3030011345148087]\n",
      "2021-05-06 18:34:14.112705  Epoch 2 : Average Loss [2.303317208647728, 2.3030441851615904, 2.303216817140579]\n",
      "2021-05-06 18:35:31.953076  Epoch 3 : Average Loss [2.3027499269247054, 2.3031123461723326, 2.303163942337036]\n",
      "2021-05-06 18:36:47.458074  Epoch 4 : Average Loss [2.3031005942821503, 2.302932648897171, 2.3031473327875136]\n",
      "2021-05-06 18:37:59.682699  Epoch 5 : Average Loss [2.3032949241399767, 2.3028807998895644, 2.3033412606716155]\n",
      "Finished Training\n",
      "Training accuracy: 10 %\n",
      "Testing accuracy: 10 %\n",
      "Testing accuracy (each class): \n",
      "0: 0.0%;   1: 0.0%;   2: 0.0%;   3: 0.0%;   4: 0.0%;   5: 0.0%;   6: 0.0%;   7: 0.0%;   8: 0.0%;   9: 100.0%;   \n",
      "2021-05-06 18:38:39.584982\n",
      "epoch range:  21  to  25\n",
      "2021-05-06 18:41:57.486410  Epoch 1 : Average Loss [2.3029049280881884, 2.303036827683449, 2.303074522614479]\n",
      "2021-05-06 18:44:36.112199  Epoch 2 : Average Loss [2.3031079552173614, 2.3029874603748324, 2.30324910235405]\n",
      "2021-05-06 18:48:26.084169  Epoch 3 : Average Loss [2.303048715829849, 2.3030890485048294, 2.3032003887891768]\n",
      "2021-05-06 18:52:49.663453  Epoch 4 : Average Loss [2.303172029495239, 2.3028912554979324, 2.303302928328514]\n",
      "2021-05-06 18:57:02.128015  Epoch 5 : Average Loss [2.302769396662712, 2.3032881774902343, 2.303163277387619]\n",
      "Finished Training\n",
      "Training accuracy: 10 %\n",
      "Testing accuracy: 10 %\n",
      "Testing accuracy (each class): \n",
      "0: 0.0%;   1: 0.0%;   2: 0.0%;   3: 0.0%;   4: 0.0%;   5: 0.0%;   6: 0.0%;   7: 100.0%;   8: 0.0%;   9: 0.0%;   \n",
      "2021-05-06 18:59:39.540488\n",
      "epoch range:  26  to  30\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-7ec2e85a1b95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0mepo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"epoch range: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepo\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" to \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m     \u001b[0mtrain_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_lr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-7ec2e85a1b95>\u001b[0m in \u001b[0;36mtrain_test\u001b[0;34m(model, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0;31m# zero the parameter gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jiarui/snap/jupyter/common/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-7ec2e85a1b95>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprinted\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CONV1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jiarui/snap/jupyter/common/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jiarui/snap/jupyter/common/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jiarui/snap/jupyter/common/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    394\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    395\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 396\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 1e-5\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import datasets, transforms, utils\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import datetime\n",
    "\n",
    "# TODO: Implement a convolutional neural network (https://pytorch.org/tutorials/recipes/recipes/defining_a_neural_network.html)\n",
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Input - 1x32x32\n",
    "    Output - 10\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.params = {'conv':[(), \n",
    "                               (3, 16, 5, 1, 1), \n",
    "                               (16, 32, 3, 1, 1),\n",
    "                               (32, 32, 3, 1, 0),\n",
    "                               (32, 16, 3, 1, 1)], # in_channels, out_channels, kernel_size, stride, padding\n",
    "                       'pool':[(), \n",
    "                               (2, 2, 0),\n",
    "                               (2, 2, 0)], # kernel_size, stride, padding\n",
    "                       'fc':[(), \n",
    "                             (16*6*6, 120),\n",
    "                             (120, 90), \n",
    "                             (90, 10)], # in_channels, out_channels\n",
    "                       'drop':[0, \n",
    "                               0.25, \n",
    "                               0.25]\n",
    "                      }\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(*self.params['conv'][1])\n",
    "        self.conv2 = nn.Conv2d(*self.params['conv'][2])\n",
    "        self.conv3 = nn.Conv2d(*self.params['conv'][3])\n",
    "        self.conv4 = nn.Conv2d(*self.params['conv'][4])\n",
    "        \n",
    "        self.pool1 = nn.MaxPool2d(*self.params['pool'][1])\n",
    "        self.pool2 = nn.MaxPool2d(*self.params['pool'][2])\n",
    "        \n",
    "        self.fc1 = nn.Linear(*self.params['fc'][1])\n",
    "        self.fc2 = nn.Linear(*self.params['fc'][2])\n",
    "        self.fc3 = nn.Linear(*self.params['fc'][3])\n",
    "        \n",
    "        # self.drop1 = nn.Dropout2d(self.params['drop'][1])\n",
    "        # self.drop2 = nn.Dropout2d(self.params['drop'][2])\n",
    "        \n",
    "        self.printed = False\n",
    "\n",
    "        # TODO: Initialize layers\n",
    "\n",
    "    def forward(self, img):\n",
    "\n",
    "        # TODO: Implement forward pass\n",
    "        x = img\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        if not self.printed: \n",
    "            print(\"CONV1\", x.size())\n",
    "        x = F.relu(self.conv2(x))\n",
    "        if not self.printed: \n",
    "            print(\"CONV2\", x.size())\n",
    "        x = self.pool1(x)\n",
    "        if not self.printed: \n",
    "            print(\"POOL1\", x.size())\n",
    "        \n",
    "        x = F.relu(self.conv3(x))\n",
    "        if not self.printed: \n",
    "            print(\"CONV3\", x.size())\n",
    "        x = F.relu(self.conv4(x))\n",
    "        if not self.printed: \n",
    "            print(\"CONV4\", x.size())\n",
    "        x = self.pool2(x)\n",
    "        if not self.printed: \n",
    "            print(\"POOL2\", x.size())\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        if not self.printed: \n",
    "            print(\"FC1\", x.size())\n",
    "        '''x = self.drop1(x)\n",
    "        if not self.printed: \n",
    "            print(\"DROP1\", x.size())'''\n",
    "        x = F.relu(self.fc2(x))\n",
    "        if not self.printed: \n",
    "            print(\"FC2\", x.size())\n",
    "        x = self.fc3(x)\n",
    "        if not self.printed: \n",
    "            print(\"FC3\", x.size())\n",
    "            self.printed = True\n",
    "\n",
    "        return x\n",
    "\n",
    "# TODO: You can change these data augmentation and normalization strategies for\n",
    "#  better training and testing (https://pytorch.org/vision/stable/transforms.html)\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((32,32)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((32,32)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Dataset initialization\n",
    "data_dir = 'data' # Suppose the dataset is stored under this folder\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'test']} # Read train and test sets, respectively.\n",
    "\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=0) for x in ['train', 'test']}\n",
    "# trainloader = torch.utils.data.DataLoader(image_datasets['train'], batch_size=4, shuffle=True, num_workers=2)\n",
    "# teatloader = torch.utils.data.DataLoader(image_datasets['test'], batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'test']}\n",
    "\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # Set device to \"cpu\" if you have no gpu\n",
    "\n",
    "# TODO: Implement training and testing procedures (https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html)\n",
    "def train_test(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    \n",
    "    for epoch in range(num_epochs):  \n",
    "\n",
    "        running_loss = 0.0\n",
    "        loss_record=[]\n",
    "        for i, data in enumerate(dataloaders['train'], 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "                loss_record.append(running_loss / 2000)\n",
    "                # print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000))\n",
    "                running_loss = 0.0\n",
    "                \n",
    "        print(datetime.datetime.now(), ' Epoch', (epoch + 1), ': Average Loss', loss_record)\n",
    "\n",
    "    print('Finished Training')\n",
    "    \n",
    "    \n",
    "    # save training results\n",
    "    PATH = './cifar_net.pth'\n",
    "    torch.save(model.state_dict(), PATH)\n",
    "    \n",
    "    \n",
    "    # testing overall correct rate\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloaders['train'], 0):\n",
    "            images, labels = data\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = model(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Training accuracy: %d %%' % (100 * correct / total))\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloaders['test'], 0):\n",
    "            images, labels = data\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = model(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Testing accuracy: %d %%' % (100 * correct / total))\n",
    "    \n",
    "    # prepare to count predictions for each class\n",
    "    correct_pred = {classname: 0 for classname in class_names}\n",
    "    total_pred = {classname: 0 for classname in class_names}\n",
    "\n",
    "    # again no gradients needed\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloaders['test'], 0):\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            # collect the correct predictions for each class\n",
    "            for label, prediction in zip(labels, predictions):\n",
    "                if label == prediction:\n",
    "                    correct_pred[class_names[label]] += 1\n",
    "                total_pred[class_names[label]] += 1\n",
    "\n",
    "\n",
    "    # print accuracy for each class\n",
    "    print(\"Testing accuracy (each class): \")\n",
    "    for classname, correct_count in correct_pred.items():\n",
    "        accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "        print(\"{:1s}: {:.1f}%;  \".format(classname, accuracy), end=' ')\n",
    "    print()    \n",
    "    \n",
    "    return None\n",
    "\n",
    "model_ft = Net() # Model initialization\n",
    "\n",
    "model_ft = model_ft.to(device) # Move model to cpu\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # Loss function initialization\n",
    "\n",
    "# TODO: Adjust the following hyper-parameters: learning rate, decay strategy, number of training epochs.\n",
    "optimizer_ft = optim.Adam(model_ft.parameters(), lr=1e-3) # Optimizer initialization\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=20, gamma=0.1) # Learning rate decay strategy\n",
    "\n",
    "for n in range(6):\n",
    "    print(datetime.datetime.now())\n",
    "    epo = 5*n+5\n",
    "    print(\"epoch range: \", epo-4, \" to \", epo)\n",
    "    train_test(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exponential\n",
      "2021-05-06 21:21:43.212723\n",
      "epoch range:  1  to  5\n",
      "CONV1 torch.Size([4, 16, 30, 30])\n",
      "CONV2 torch.Size([4, 32, 30, 30])\n",
      "POOL1 torch.Size([4, 32, 15, 15])\n",
      "CONV3 torch.Size([4, 32, 13, 13])\n",
      "CONV4 torch.Size([4, 16, 13, 13])\n",
      "POOL2 torch.Size([4, 16, 6, 6])\n",
      "FC1 torch.Size([4, 120])\n",
      "FC2 torch.Size([4, 90])\n",
      "FC3 torch.Size([4, 10])\n",
      "2021-05-06 21:22:41.814435  Epoch 1 : Average Loss [2.018582885324955, 1.3273267338052392, 1.111500558863394]\n",
      "2021-05-06 21:23:49.766135  Epoch 2 : Average Loss [0.9188609193004668, 0.9009850593106821, 0.8203994854008779]\n",
      "2021-05-06 21:24:47.068376  Epoch 3 : Average Loss [0.7436504960694583, 0.7164136390537024, 0.6825564343965379]\n",
      "2021-05-06 21:25:48.390452  Epoch 4 : Average Loss [0.6393269818884146, 0.5889167256407091, 0.593654383557936]\n",
      "2021-05-06 21:26:48.614278  Epoch 5 : Average Loss [0.5642612914105121, 0.5381727868962626, 0.5376628556099459]\n",
      "Finished Training\n",
      "Training accuracy: 84 %\n",
      "Testing accuracy: 82 %\n",
      "Testing accuracy (each class): \n",
      "0: 88.4%;   1: 81.8%;   2: 83.2%;   3: 79.8%;   4: 86.2%;   5: 81.2%;   6: 74.8%;   7: 89.0%;   8: 82.4%;   9: 80.6%;   \n",
      "2021-05-06 21:27:22.661967\n",
      "epoch range:  6  to  10\n",
      "2021-05-06 21:28:14.834441  Epoch 1 : Average Loss [0.5144941870293114, 0.4979205117062083, 0.4843392659984529]\n",
      "2021-05-06 21:29:11.944719  Epoch 2 : Average Loss [0.45794832518171463, 0.446716624513555, 0.4629358163913275]\n",
      "2021-05-06 21:30:09.874199  Epoch 3 : Average Loss [0.4211303582175944, 0.43198269248584986, 0.42670964458853816]\n",
      "2021-05-06 21:31:05.547797  Epoch 4 : Average Loss [0.3830097587704731, 0.4030137545137204, 0.4091592346102152]\n",
      "2021-05-06 21:32:04.875200  Epoch 5 : Average Loss [0.37022493716325083, 0.3876101129962353, 0.3896517573404435]\n",
      "Finished Training\n",
      "Training accuracy: 89 %\n",
      "Testing accuracy: 86 %\n",
      "Testing accuracy (each class): \n",
      "0: 89.2%;   1: 84.8%;   2: 88.2%;   3: 80.6%;   4: 90.2%;   5: 87.4%;   6: 76.8%;   7: 93.4%;   8: 82.4%;   9: 87.8%;   \n",
      "2021-05-06 21:32:37.729830\n",
      "epoch range:  11  to  15\n",
      "2021-05-06 21:33:32.181023  Epoch 1 : Average Loss [0.35683646905564453, 0.36569801181726325, 0.3483800312160125]\n",
      "2021-05-06 21:34:31.137424  Epoch 2 : Average Loss [0.3378411480516811, 0.34195173515025545, 0.3353137375817896]\n",
      "2021-05-06 21:35:30.230152  Epoch 3 : Average Loss [0.31768190436995974, 0.32110922131592406, 0.3169951433799597]\n",
      "2021-05-06 21:36:25.856721  Epoch 4 : Average Loss [0.3020319387257341, 0.305859090441269, 0.30548668299388554]\n",
      "2021-05-06 21:37:28.385587  Epoch 5 : Average Loss [0.2857328199237854, 0.28572216645267323, 0.3017652572622587]\n",
      "Finished Training\n",
      "Training accuracy: 91 %\n",
      "Testing accuracy: 86 %\n",
      "Testing accuracy (each class): \n",
      "0: 86.8%;   1: 88.2%;   2: 87.6%;   3: 73.6%;   4: 90.8%;   5: 90.6%;   6: 83.4%;   7: 93.6%;   8: 85.6%;   9: 88.6%;   \n",
      "2021-05-06 21:38:03.533809\n",
      "epoch range:  16  to  20\n",
      "2021-05-06 21:38:59.143980  Epoch 1 : Average Loss [0.26666308076640644, 0.28703850739673065, 0.2773244978552348]\n",
      "2021-05-06 21:39:58.040873  Epoch 2 : Average Loss [0.24826073523935951, 0.2677971773126765, 0.2645811242022853]\n",
      "2021-05-06 21:41:49.260907  Epoch 3 : Average Loss [0.2401833441352119, 0.25896172038952764, 0.2649507526597266]\n",
      "2021-05-06 21:44:24.041512  Epoch 4 : Average Loss [0.21983936699620654, 0.25416933211166104, 0.24682559640342605]\n",
      "2021-05-06 21:46:53.099111  Epoch 5 : Average Loss [0.21851919441065673, 0.22913319121507392, 0.24372610782420248]\n",
      "Finished Training\n",
      "Training accuracy: 93 %\n",
      "Testing accuracy: 87 %\n",
      "Testing accuracy (each class): \n",
      "0: 88.8%;   1: 89.8%;   2: 87.8%;   3: 78.8%;   4: 93.0%;   5: 84.6%;   6: 88.2%;   7: 92.4%;   8: 85.8%;   9: 87.8%;   \n",
      "2021-05-06 21:48:27.264984\n",
      "epoch range:  21  to  25\n",
      "2021-05-06 21:51:07.401377  Epoch 1 : Average Loss [0.22171842856293164, 0.2122059957760414, 0.2344196938893856]\n",
      "2021-05-06 21:53:37.633190  Epoch 2 : Average Loss [0.19667291046329757, 0.2193490912286166, 0.20635199179375274]\n",
      "2021-05-06 21:56:03.291805  Epoch 3 : Average Loss [0.18870862782181805, 0.1931764515041019, 0.20578643318746348]\n",
      "2021-05-06 21:58:31.105942  Epoch 4 : Average Loss [0.18312347797049278, 0.18714383725380668, 0.19174762493352335]\n",
      "2021-05-06 22:00:51.921156  Epoch 5 : Average Loss [0.16858370447037885, 0.17694922263590657, 0.19377374125654492]\n",
      "Finished Training\n",
      "Training accuracy: 94 %\n",
      "Testing accuracy: 87 %\n",
      "Testing accuracy (each class): \n",
      "0: 88.6%;   1: 87.0%;   2: 88.0%;   3: 83.0%;   4: 89.8%;   5: 83.6%;   6: 84.0%;   7: 91.6%;   8: 89.0%;   9: 86.8%;   \n",
      "2021-05-06 22:02:17.683372\n",
      "epoch range:  26  to  30\n",
      "2021-05-06 22:04:43.131014  Epoch 1 : Average Loss [0.16867147622966205, 0.16878087080537024, 0.1819693938699641]\n",
      "2021-05-06 22:07:07.232547  Epoch 2 : Average Loss [0.15885018607441442, 0.1591422010023524, 0.17827361444637302]\n",
      "2021-05-06 22:09:36.189647  Epoch 3 : Average Loss [0.14227973693116416, 0.16952657765445747, 0.16276538995736153]\n",
      "2021-05-06 22:12:04.823309  Epoch 4 : Average Loss [0.13078992574153137, 0.14550954862486667, 0.16528786444799093]\n",
      "2021-05-06 22:14:37.220335  Epoch 5 : Average Loss [0.13525672786860957, 0.14543133058235555, 0.14721246410725922]\n",
      "Finished Training\n",
      "Training accuracy: 96 %\n",
      "Testing accuracy: 87 %\n",
      "Testing accuracy (each class): \n",
      "0: 86.2%;   1: 86.0%;   2: 86.6%;   3: 83.0%;   4: 90.8%;   5: 87.2%;   6: 84.6%;   7: 91.0%;   8: 89.0%;   9: 89.2%;   \n"
     ]
    }
   ],
   "source": [
    "# ExponentialLR\n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import datasets, transforms, utils\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import datetime\n",
    "\n",
    "# TODO: Implement a convolutional neural network (https://pytorch.org/tutorials/recipes/recipes/defining_a_neural_network.html)\n",
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Input - 1x32x32\n",
    "    Output - 10\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.params = {'conv':[(), \n",
    "                               (3, 16, 5, 1, 1), \n",
    "                               (16, 32, 3, 1, 1),\n",
    "                               (32, 32, 3, 1, 0),\n",
    "                               (32, 16, 3, 1, 1)], # in_channels, out_channels, kernel_size, stride, padding\n",
    "                       'pool':[(), \n",
    "                               (2, 2, 0),\n",
    "                               (2, 2, 0)], # kernel_size, stride, padding\n",
    "                       'fc':[(), \n",
    "                             (16*6*6, 120),\n",
    "                             (120, 90), \n",
    "                             (90, 10)], # in_channels, out_channels\n",
    "                       'drop':[0, \n",
    "                               0.25, \n",
    "                               0.25]\n",
    "                      }\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(*self.params['conv'][1])\n",
    "        self.conv2 = nn.Conv2d(*self.params['conv'][2])\n",
    "        self.conv3 = nn.Conv2d(*self.params['conv'][3])\n",
    "        self.conv4 = nn.Conv2d(*self.params['conv'][4])\n",
    "        \n",
    "        self.pool1 = nn.MaxPool2d(*self.params['pool'][1])\n",
    "        self.pool2 = nn.MaxPool2d(*self.params['pool'][2])\n",
    "        \n",
    "        self.fc1 = nn.Linear(*self.params['fc'][1])\n",
    "        self.fc2 = nn.Linear(*self.params['fc'][2])\n",
    "        self.fc3 = nn.Linear(*self.params['fc'][3])\n",
    "        \n",
    "        # self.drop1 = nn.Dropout2d(self.params['drop'][1])\n",
    "        # self.drop2 = nn.Dropout2d(self.params['drop'][2])\n",
    "        \n",
    "        self.printed = False\n",
    "\n",
    "        # TODO: Initialize layers\n",
    "\n",
    "    def forward(self, img):\n",
    "\n",
    "        # TODO: Implement forward pass\n",
    "        x = img\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        if not self.printed: \n",
    "            print(\"CONV1\", x.size())\n",
    "        x = F.relu(self.conv2(x))\n",
    "        if not self.printed: \n",
    "            print(\"CONV2\", x.size())\n",
    "        x = self.pool1(x)\n",
    "        if not self.printed: \n",
    "            print(\"POOL1\", x.size())\n",
    "        \n",
    "        x = F.relu(self.conv3(x))\n",
    "        if not self.printed: \n",
    "            print(\"CONV3\", x.size())\n",
    "        x = F.relu(self.conv4(x))\n",
    "        if not self.printed: \n",
    "            print(\"CONV4\", x.size())\n",
    "        x = self.pool2(x)\n",
    "        if not self.printed: \n",
    "            print(\"POOL2\", x.size())\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        if not self.printed: \n",
    "            print(\"FC1\", x.size())\n",
    "        '''x = self.drop1(x)\n",
    "        if not self.printed: \n",
    "            print(\"DROP1\", x.size())'''\n",
    "        x = F.relu(self.fc2(x))\n",
    "        if not self.printed: \n",
    "            print(\"FC2\", x.size())\n",
    "        x = self.fc3(x)\n",
    "        if not self.printed: \n",
    "            print(\"FC3\", x.size())\n",
    "            self.printed = True\n",
    "\n",
    "        return x\n",
    "\n",
    "# TODO: You can change these data augmentation and normalization strategies for\n",
    "#  better training and testing (https://pytorch.org/vision/stable/transforms.html)\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((32,32)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((32,32)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Dataset initialization\n",
    "data_dir = 'data' # Suppose the dataset is stored under this folder\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'test']} # Read train and test sets, respectively.\n",
    "\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=0) for x in ['train', 'test']}\n",
    "# trainloader = torch.utils.data.DataLoader(image_datasets['train'], batch_size=4, shuffle=True, num_workers=2)\n",
    "# teatloader = torch.utils.data.DataLoader(image_datasets['test'], batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'test']}\n",
    "\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # Set device to \"cpu\" if you have no gpu\n",
    "\n",
    "# TODO: Implement training and testing procedures (https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html)\n",
    "def train_test(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    \n",
    "    for epoch in range(num_epochs):  \n",
    "\n",
    "        running_loss = 0.0\n",
    "        loss_record=[]\n",
    "        for i, data in enumerate(dataloaders['train'], 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "                loss_record.append(running_loss / 2000)\n",
    "                # print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000))\n",
    "                running_loss = 0.0\n",
    "                \n",
    "        print(datetime.datetime.now(), ' Epoch', (epoch + 1), ': Average Loss', loss_record)\n",
    "\n",
    "    print('Finished Training')\n",
    "    \n",
    "    \n",
    "    # save training results\n",
    "    PATH = './cifar_net.pth'\n",
    "    torch.save(model.state_dict(), PATH)\n",
    "    \n",
    "    \n",
    "    # testing overall correct rate\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloaders['train'], 0):\n",
    "            images, labels = data\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = model(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Training accuracy: %d %%' % (100 * correct / total))\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloaders['test'], 0):\n",
    "            images, labels = data\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = model(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Testing accuracy: %d %%' % (100 * correct / total))\n",
    "    \n",
    "    # prepare to count predictions for each class\n",
    "    correct_pred = {classname: 0 for classname in class_names}\n",
    "    total_pred = {classname: 0 for classname in class_names}\n",
    "\n",
    "    # again no gradients needed\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloaders['test'], 0):\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            # collect the correct predictions for each class\n",
    "            for label, prediction in zip(labels, predictions):\n",
    "                if label == prediction:\n",
    "                    correct_pred[class_names[label]] += 1\n",
    "                total_pred[class_names[label]] += 1\n",
    "\n",
    "\n",
    "    # print accuracy for each class\n",
    "    print(\"Testing accuracy (each class): \")\n",
    "    for classname, correct_count in correct_pred.items():\n",
    "        accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "        print(\"{:1s}: {:.1f}%;  \".format(classname, accuracy), end=' ')\n",
    "    print()    \n",
    "    \n",
    "    return None\n",
    "\n",
    "model_ft = Net() # Model initialization\n",
    "\n",
    "model_ft = model_ft.to(device) # Move model to cpu\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # Loss function initialization\n",
    "\n",
    "# TODO: Adjust the following hyper-parameters: learning rate, decay strategy, number of training epochs.\n",
    "optimizer_ft = optim.Adam(model_ft.parameters(), lr=1e-4) # Optimizer initialization\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=20, gamma=0.1) # Learning rate decay strategy\n",
    "#cyc_lr_scheduler = lr_scheduler.CyclicLR(optimizer_ft, base_lr=1e-4, max_lr=5e-4, step_size_up=21, step_size_down=19, mode='triangular', gamma=0.1, scale_fn=None, scale_mode='cycle', cycle_momentum=True, base_momentum=0.8, max_momentum=0.9, last_epoch=-1)\n",
    "expon_lr_scheduler = lr_scheduler.ExponentialLR(optimizer_ft, gamma=0.1)\n",
    "\n",
    "'''print(\"cyclic\")\n",
    "for n in range(6):\n",
    "    print(datetime.datetime.now())\n",
    "    epo = 5*n+5\n",
    "    print(\"epoch range: \", epo-4, \" to \", epo)\n",
    "    train_test(model_ft, criterion, optimizer_ft, cyc_lr_scheduler, num_epochs=5)'''\n",
    "print(\"exponential\")\n",
    "for n in range(6):\n",
    "    print(datetime.datetime.now())\n",
    "    epo = 5*n+5\n",
    "    print(\"epoch range: \", epo-4, \" to \", epo)\n",
    "    train_test(model_ft, criterion, optimizer_ft, expon_lr_scheduler, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exponential\n",
      "2021-05-07 15:58:43.725625\n",
      "epoch range:  1  to  5\n",
      "CONV1 torch.Size([4, 16, 30, 30])\n",
      "CONV2 torch.Size([4, 32, 30, 30])\n",
      "POOL1 torch.Size([4, 32, 15, 15])\n",
      "CONV3 torch.Size([4, 32, 13, 13])\n",
      "CONV4 torch.Size([4, 16, 13, 13])\n",
      "POOL2 torch.Size([4, 16, 6, 6])\n",
      "FC1 torch.Size([4, 120])\n",
      "FC2 torch.Size([4, 90])\n",
      "FC3 torch.Size([4, 10])\n",
      "2021-05-07 15:59:32.621721  Epoch 1 : Average Loss [2.140716392248869, 1.432954012028873, 1.1940439390111715]\n",
      "2021-05-07 16:00:41.053668  Epoch 2 : Average Loss [0.9760816143546254, 0.8912500327262096, 0.8436358041013591]\n",
      "2021-05-07 16:01:52.143576  Epoch 3 : Average Loss [0.7530516283386387, 0.7475160048725665, 0.7027701566662872]\n",
      "2021-05-07 16:02:49.144576  Epoch 4 : Average Loss [0.6486936410089256, 0.6308916582132224, 0.6333620286413061]\n",
      "2021-05-07 16:03:48.438969  Epoch 5 : Average Loss [0.5813864413538249, 0.5565790085202897, 0.55848952240785]\n",
      "Finished Training\n",
      "Training accuracy: 83 %\n",
      "Testing accuracy: 81 %\n",
      "Testing accuracy (each class): \n",
      "0: 85.0%;   1: 84.6%;   2: 84.4%;   3: 76.6%;   4: 84.8%;   5: 73.8%;   6: 86.2%;   7: 89.0%;   8: 71.4%;   9: 80.4%;   \n",
      "2021-05-07 16:04:28.072166\n",
      "epoch range:  6  to  10\n",
      "2021-05-07 16:05:31.818530  Epoch 1 : Average Loss [0.509073946509132, 0.509554776081699, 0.5153394239515474]\n",
      "2021-05-07 16:06:29.937675  Epoch 2 : Average Loss [0.47242003186653164, 0.46436184442420925, 0.4798651669649335]\n",
      "2021-05-07 16:07:33.559772  Epoch 3 : Average Loss [0.4218602456375811, 0.43984063608547147, 0.4429082642397589]\n",
      "2021-05-07 16:08:41.781065  Epoch 4 : Average Loss [0.3963413756976297, 0.4262319439947205, 0.40862576248642085]\n",
      "2021-05-07 16:09:47.012110  Epoch 5 : Average Loss [0.36201773426640216, 0.39544025101961144, 0.3838814397973219]\n",
      "Finished Training\n",
      "Training accuracy: 88 %\n",
      "Testing accuracy: 85 %\n",
      "Testing accuracy (each class): \n",
      "0: 85.2%;   1: 87.8%;   2: 85.6%;   3: 85.0%;   4: 90.4%;   5: 86.2%;   6: 81.4%;   7: 90.0%;   8: 75.6%;   9: 83.2%;   \n",
      "2021-05-07 16:10:25.385910\n",
      "epoch range:  11  to  15\n",
      "2021-05-07 16:12:01.355623  Epoch 1 : Average Loss [0.35436613555363966, 0.3791976152196403, 0.3549172449136477]\n",
      "2021-05-07 16:13:13.945968  Epoch 2 : Average Loss [0.3455868884088086, 0.3569466630849929, 0.34044839390768583]\n",
      "2021-05-07 16:14:17.579579  Epoch 3 : Average Loss [0.31712733957382944, 0.32691556968313035, 0.32668529321059964]\n",
      "2021-05-07 16:15:22.237520  Epoch 4 : Average Loss [0.30988528511431557, 0.31641290557309026, 0.29395666622698174]\n",
      "2021-05-07 16:16:25.584019  Epoch 5 : Average Loss [0.2913561970890978, 0.2912884916299463, 0.2886867068986842]\n",
      "Finished Training\n",
      "Training accuracy: 91 %\n",
      "Testing accuracy: 86 %\n",
      "Testing accuracy (each class): \n",
      "0: 89.2%;   1: 88.4%;   2: 86.2%;   3: 76.4%;   4: 91.2%;   5: 83.6%;   6: 84.0%;   7: 90.8%;   8: 88.8%;   9: 82.2%;   \n",
      "2021-05-07 16:17:07.916763\n",
      "epoch range:  16  to  20\n",
      "2021-05-07 16:18:10.036266  Epoch 1 : Average Loss [0.26891244884831067, 0.28021240769875705, 0.26545911007834183]\n",
      "2021-05-07 16:19:08.863961  Epoch 2 : Average Loss [0.26049463143070023, 0.2576753715116721, 0.2777194880872678]\n",
      "2021-05-07 16:20:24.830997  Epoch 3 : Average Loss [0.24631361528306012, 0.24578065329076484, 0.26388520308443675]\n",
      "2021-05-07 16:21:59.027538  Epoch 4 : Average Loss [0.22997849324307582, 0.24127980193429482, 0.25231974898335124]\n",
      "2021-05-07 16:23:07.279680  Epoch 5 : Average Loss [0.22336585909610118, 0.24368268372739396, 0.22775240294709648]\n",
      "Finished Training\n",
      "Training accuracy: 93 %\n",
      "Testing accuracy: 86 %\n",
      "Testing accuracy (each class): \n",
      "0: 89.2%;   1: 85.6%;   2: 89.4%;   3: 74.6%;   4: 90.4%;   5: 90.0%;   6: 83.8%;   7: 91.0%;   8: 88.2%;   9: 84.0%;   \n",
      "2021-05-07 16:23:51.162896\n",
      "epoch range:  21  to  25\n",
      "2021-05-07 16:25:05.601463  Epoch 1 : Average Loss [0.21031199351427313, 0.21713271368090165, 0.22225657957317252]\n",
      "2021-05-07 16:26:28.616539  Epoch 2 : Average Loss [0.18984921675980249, 0.200139539559721, 0.21942378841971102]\n",
      "2021-05-07 16:27:40.366906  Epoch 3 : Average Loss [0.1877404777989014, 0.18820686780731732, 0.19251179285183567]\n",
      "2021-05-07 16:28:52.802701  Epoch 4 : Average Loss [0.16691182020409992, 0.1901714510118429, 0.18931803219918855]\n",
      "2021-05-07 16:30:01.648802  Epoch 5 : Average Loss [0.16669427628100156, 0.17848222469217861, 0.17232268519536081]\n",
      "Finished Training\n",
      "Training accuracy: 95 %\n",
      "Testing accuracy: 86 %\n",
      "Testing accuracy (each class): \n",
      "0: 87.2%;   1: 90.0%;   2: 87.6%;   3: 78.6%;   4: 89.6%;   5: 87.8%;   6: 84.4%;   7: 89.8%;   8: 84.4%;   9: 86.6%;   \n",
      "2021-05-07 16:30:42.593976\n",
      "epoch range:  26  to  30\n",
      "2021-05-07 16:31:54.474532  Epoch 1 : Average Loss [0.15730227798713053, 0.15905195038038022, 0.1828361596966131]\n",
      "2021-05-07 16:33:12.155936  Epoch 2 : Average Loss [0.14830684161343702, 0.15845066505010408, 0.1675592920279527]\n",
      "2021-05-07 16:34:24.215975  Epoch 3 : Average Loss [0.12907425310543488, 0.15399424008607288, 0.15288642179434367]\n",
      "2021-05-07 16:35:26.298792  Epoch 4 : Average Loss [0.1342350196956536, 0.14230159317861113, 0.1450314342819885]\n",
      "2021-05-07 16:36:27.967156  Epoch 5 : Average Loss [0.1192202467215606, 0.1354300824606431, 0.1435773833221781]\n",
      "Finished Training\n",
      "Training accuracy: 96 %\n",
      "Testing accuracy: 86 %\n",
      "Testing accuracy (each class): \n",
      "0: 87.2%;   1: 89.0%;   2: 83.4%;   3: 81.6%;   4: 90.2%;   5: 87.6%;   6: 86.6%;   7: 89.4%;   8: 79.4%;   9: 88.0%;   \n"
     ]
    }
   ],
   "source": [
    "# different CONV1 ExponentialLR\n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import datasets, transforms, utils\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import datetime\n",
    "\n",
    "# TODO: Implement a convolutional neural network (https://pytorch.org/tutorials/recipes/recipes/defining_a_neural_network.html)\n",
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Input - 1x32x32\n",
    "    Output - 10\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.params = {'conv':[(), \n",
    "                               (3, 16, 3, 1, 0), \n",
    "                               (16, 32, 3, 1, 1),\n",
    "                               (32, 32, 3, 1, 0),\n",
    "                               (32, 16, 3, 1, 1)], # in_channels, out_channels, kernel_size, stride, padding\n",
    "                       'pool':[(), \n",
    "                               (2, 2, 0),\n",
    "                               (2, 2, 0)], # kernel_size, stride, padding\n",
    "                       'fc':[(), \n",
    "                             (16*6*6, 120),\n",
    "                             (120, 90), \n",
    "                             (90, 10)], # in_channels, out_channels\n",
    "                       'drop':[0, \n",
    "                               0.25, \n",
    "                               0.25]\n",
    "                      }\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(*self.params['conv'][1])\n",
    "        self.conv2 = nn.Conv2d(*self.params['conv'][2])\n",
    "        self.conv3 = nn.Conv2d(*self.params['conv'][3])\n",
    "        self.conv4 = nn.Conv2d(*self.params['conv'][4])\n",
    "        \n",
    "        self.pool1 = nn.MaxPool2d(*self.params['pool'][1])\n",
    "        self.pool2 = nn.MaxPool2d(*self.params['pool'][2])\n",
    "        \n",
    "        self.fc1 = nn.Linear(*self.params['fc'][1])\n",
    "        self.fc2 = nn.Linear(*self.params['fc'][2])\n",
    "        self.fc3 = nn.Linear(*self.params['fc'][3])\n",
    "        \n",
    "        # self.drop1 = nn.Dropout2d(self.params['drop'][1])\n",
    "        # self.drop2 = nn.Dropout2d(self.params['drop'][2])\n",
    "        \n",
    "        self.printed = False\n",
    "\n",
    "        # TODO: Initialize layers\n",
    "\n",
    "    def forward(self, img):\n",
    "\n",
    "        # TODO: Implement forward pass\n",
    "        x = img\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        if not self.printed: \n",
    "            print(\"CONV1\", x.size())\n",
    "        x = F.relu(self.conv2(x))\n",
    "        if not self.printed: \n",
    "            print(\"CONV2\", x.size())\n",
    "        x = self.pool1(x)\n",
    "        if not self.printed: \n",
    "            print(\"POOL1\", x.size())\n",
    "        \n",
    "        x = F.relu(self.conv3(x))\n",
    "        if not self.printed: \n",
    "            print(\"CONV3\", x.size())\n",
    "        x = F.relu(self.conv4(x))\n",
    "        if not self.printed: \n",
    "            print(\"CONV4\", x.size())\n",
    "        x = self.pool2(x)\n",
    "        if not self.printed: \n",
    "            print(\"POOL2\", x.size())\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        if not self.printed: \n",
    "            print(\"FC1\", x.size())\n",
    "        '''x = self.drop1(x)\n",
    "        if not self.printed: \n",
    "            print(\"DROP1\", x.size())'''\n",
    "        x = F.relu(self.fc2(x))\n",
    "        if not self.printed: \n",
    "            print(\"FC2\", x.size())\n",
    "        x = self.fc3(x)\n",
    "        if not self.printed: \n",
    "            print(\"FC3\", x.size())\n",
    "            self.printed = True\n",
    "\n",
    "        return x\n",
    "\n",
    "# TODO: You can change these data augmentation and normalization strategies for\n",
    "#  better training and testing (https://pytorch.org/vision/stable/transforms.html)\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((32,32)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((32,32)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Dataset initialization\n",
    "data_dir = 'data' # Suppose the dataset is stored under this folder\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'test']} # Read train and test sets, respectively.\n",
    "\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=0) for x in ['train', 'test']}\n",
    "# trainloader = torch.utils.data.DataLoader(image_datasets['train'], batch_size=4, shuffle=True, num_workers=2)\n",
    "# teatloader = torch.utils.data.DataLoader(image_datasets['test'], batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'test']}\n",
    "\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # Set device to \"cpu\" if you have no gpu\n",
    "\n",
    "# TODO: Implement training and testing procedures (https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html)\n",
    "def train_test(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    \n",
    "    for epoch in range(num_epochs):  \n",
    "\n",
    "        running_loss = 0.0\n",
    "        loss_record=[]\n",
    "        for i, data in enumerate(dataloaders['train'], 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "                loss_record.append(running_loss / 2000)\n",
    "                # print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000))\n",
    "                running_loss = 0.0\n",
    "                \n",
    "        print(datetime.datetime.now(), ' Epoch', (epoch + 1), ': Average Loss', loss_record)\n",
    "\n",
    "    print('Finished Training')\n",
    "    \n",
    "    \n",
    "    # save training results\n",
    "    PATH = './cifar_net.pth'\n",
    "    torch.save(model.state_dict(), PATH)\n",
    "    \n",
    "    \n",
    "    # testing overall correct rate\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloaders['train'], 0):\n",
    "            images, labels = data\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = model(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Training accuracy: %d %%' % (100 * correct / total))\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloaders['test'], 0):\n",
    "            images, labels = data\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = model(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Testing accuracy: %d %%' % (100 * correct / total))\n",
    "    \n",
    "    # prepare to count predictions for each class\n",
    "    correct_pred = {classname: 0 for classname in class_names}\n",
    "    total_pred = {classname: 0 for classname in class_names}\n",
    "\n",
    "    # again no gradients needed\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloaders['test'], 0):\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            # collect the correct predictions for each class\n",
    "            for label, prediction in zip(labels, predictions):\n",
    "                if label == prediction:\n",
    "                    correct_pred[class_names[label]] += 1\n",
    "                total_pred[class_names[label]] += 1\n",
    "\n",
    "\n",
    "    # print accuracy for each class\n",
    "    print(\"Testing accuracy (each class): \")\n",
    "    for classname, correct_count in correct_pred.items():\n",
    "        accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "        print(\"{:1s}: {:.1f}%;  \".format(classname, accuracy), end=' ')\n",
    "    print()    \n",
    "    \n",
    "    return None\n",
    "\n",
    "model_ft = Net() # Model initialization\n",
    "\n",
    "model_ft = model_ft.to(device) # Move model to cpu\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # Loss function initialization\n",
    "\n",
    "# TODO: Adjust the following hyper-parameters: learning rate, decay strategy, number of training epochs.\n",
    "optimizer_ft = optim.Adam(model_ft.parameters(), lr=1e-4) # Optimizer initialization\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=20, gamma=0.1) # Learning rate decay strategy\n",
    "#cyc_lr_scheduler = lr_scheduler.CyclicLR(optimizer_ft, base_lr=1e-4, max_lr=5e-4, step_size_up=21, step_size_down=19, mode='triangular', gamma=0.1, scale_fn=None, scale_mode='cycle', cycle_momentum=True, base_momentum=0.8, max_momentum=0.9, last_epoch=-1)\n",
    "expon_lr_scheduler = lr_scheduler.ExponentialLR(optimizer_ft, gamma=0.1)\n",
    "\n",
    "'''print(\"cyclic\")\n",
    "for n in range(6):\n",
    "    print(datetime.datetime.now())\n",
    "    epo = 5*n+5\n",
    "    print(\"epoch range: \", epo-4, \" to \", epo)\n",
    "    train_test(model_ft, criterion, optimizer_ft, cyc_lr_scheduler, num_epochs=5)'''\n",
    "print(\"exponential\")\n",
    "for n in range(6):\n",
    "    print(datetime.datetime.now())\n",
    "    epo = 5*n+5\n",
    "    print(\"epoch range: \", epo-4, \" to \", epo)\n",
    "    train_test(model_ft, criterion, optimizer_ft, expon_lr_scheduler, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-07 22:11:00.303391\n",
      "epoch range:  1  to  5\n",
      "CONV1 torch.Size([4, 12, 24, 24])\n",
      "POOL1 torch.Size([4, 12, 12, 12])\n",
      "CONV2 torch.Size([4, 36, 8, 8])\n",
      "POOL2 torch.Size([4, 36, 4, 4])\n",
      "FC1 torch.Size([4, 120])\n",
      "FC2 torch.Size([4, 90])\n",
      "FC3 torch.Size([4, 10])\n",
      "2021-05-07 22:11:47.097464  Epoch 1 : Average Loss [2.112942450135946, 1.5385193206220866, 1.361519385561347]\n",
      "2021-05-07 22:12:44.365842  Epoch 2 : Average Loss [1.0534058191366493, 0.9741368007035927, 0.889476210184861]\n",
      "2021-05-07 22:13:41.394395  Epoch 3 : Average Loss [0.814332571022911, 0.7984415438361466, 0.7524605754625517]\n",
      "2021-05-07 22:14:34.687408  Epoch 4 : Average Loss [0.7169015335887671, 0.6947488844858016, 0.6904889244963415]\n",
      "2021-05-07 22:15:23.488324  Epoch 5 : Average Loss [0.6492560619564319, 0.6484069976648897, 0.6209657117625101]\n",
      "Finished Training\n",
      "Training accuracy: 82 %\n",
      "Testing accuracy: 79 %\n",
      "Testing accuracy (each class): \n",
      "0: 87.8%;   1: 87.2%;   2: 79.8%;   3: 68.0%;   4: 88.4%;   5: 75.6%;   6: 66.8%;   7: 85.6%;   8: 77.4%;   9: 81.4%;   \n",
      "2021-05-07 22:15:57.149586\n",
      "epoch range:  6  to  10\n",
      "2021-05-07 22:16:50.991336  Epoch 1 : Average Loss [0.5845031145885586, 0.5850704968900936, 0.5920337890019873]\n",
      "2021-05-07 22:17:46.110105  Epoch 2 : Average Loss [0.5526904643062007, 0.5518359047588892, 0.5167325900758151]\n",
      "2021-05-07 22:18:45.149291  Epoch 3 : Average Loss [0.5172279391330085, 0.5183966177181719, 0.490223663778248]\n",
      "2021-05-07 22:19:39.853614  Epoch 4 : Average Loss [0.48271616220688157, 0.48762820759529857, 0.48213435908449176]\n",
      "2021-05-07 22:20:30.223377  Epoch 5 : Average Loss [0.458541408487923, 0.468016275147158, 0.4699428322966196]\n",
      "Finished Training\n",
      "Training accuracy: 86 %\n",
      "Testing accuracy: 83 %\n",
      "Testing accuracy (each class): \n",
      "0: 88.4%;   1: 81.8%;   2: 85.8%;   3: 69.6%;   4: 86.8%;   5: 88.2%;   6: 78.6%;   7: 92.6%;   8: 77.0%;   9: 85.4%;   \n",
      "2021-05-07 22:21:05.522225\n",
      "epoch range:  11  to  15\n",
      "2021-05-07 22:22:08.681437  Epoch 1 : Average Loss [0.4313231576002727, 0.4426358818976732, 0.4367566218260217]\n",
      "2021-05-07 22:23:05.602832  Epoch 2 : Average Loss [0.42707786925665275, 0.4302064669076062, 0.4057464449441686]\n",
      "2021-05-07 22:23:59.682485  Epoch 3 : Average Loss [0.412639427107586, 0.3882152545611609, 0.40042463103671616]\n",
      "2021-05-07 22:24:55.614012  Epoch 4 : Average Loss [0.37306846798997867, 0.3880469534916292, 0.39954200865573875]\n",
      "2021-05-07 22:25:59.433101  Epoch 5 : Average Loss [0.3704832414260236, 0.3723328286251181, 0.3726334546923199]\n",
      "Finished Training\n",
      "Training accuracy: 89 %\n",
      "Testing accuracy: 84 %\n",
      "Testing accuracy (each class): \n",
      "0: 82.2%;   1: 86.4%;   2: 82.8%;   3: 75.0%;   4: 88.2%;   5: 84.8%;   6: 82.8%;   7: 88.6%;   8: 86.4%;   9: 88.8%;   \n",
      "2021-05-07 22:26:36.142874\n",
      "epoch range:  16  to  20\n",
      "2021-05-07 22:27:32.454215  Epoch 1 : Average Loss [0.3458240822536827, 0.3491214441425454, 0.3719560111858673]\n",
      "2021-05-07 22:28:28.212057  Epoch 2 : Average Loss [0.34244154718783737, 0.3584246950381057, 0.3366836867417169]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-e43e7206ee36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0mepo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"epoch range: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepo\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" to \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m     \u001b[0mtrain_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_lr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-e43e7206ee36>\u001b[0m in \u001b[0;36mtrain_test\u001b[0;34m(model, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;31m# zero the parameter gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jiarui/snap/jupyter/common/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-e43e7206ee36>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprinted\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CONV1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jiarui/snap/jupyter/common/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jiarui/snap/jupyter/common/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jiarui/snap/jupyter/common/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    394\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    395\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 396\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import datasets, transforms, utils\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import datetime\n",
    "\n",
    "# TODO: Implement a convolutional neural network (https://pytorch.org/tutorials/recipes/recipes/defining_a_neural_network.html)\n",
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Input - 1x32x32\n",
    "    Output - 10\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.params = {'conv':[(), \n",
    "                               (3, 12, 5, 1, 0), \n",
    "                               (12, 36, 5, 1, 0)], # in_channels, out_channels, kernel_size, stride, padding\n",
    "                       'pool':[(), \n",
    "                               (2, 2, 0),\n",
    "                               (2, 2, 0)], # kernel_size, stride, padding\n",
    "                       'fc':[(), \n",
    "                             (36*4*4, 120),\n",
    "                             (120, 90), \n",
    "                             (90, 10)], # in_channels, out_channels\n",
    "                      }\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(*self.params['conv'][1])\n",
    "        self.conv2 = nn.Conv2d(*self.params['conv'][2])\n",
    "        \n",
    "        self.pool1 = nn.MaxPool2d(*self.params['pool'][1])\n",
    "        self.pool2 = nn.MaxPool2d(*self.params['pool'][2])\n",
    "        \n",
    "        self.fc1 = nn.Linear(*self.params['fc'][1])\n",
    "        self.fc2 = nn.Linear(*self.params['fc'][2])\n",
    "        self.fc3 = nn.Linear(*self.params['fc'][3])\n",
    "        \n",
    "        # self.drop1 = nn.Dropout2d(self.params['drop'][1])\n",
    "        # self.drop2 = nn.Dropout2d(self.params['drop'][2])\n",
    "        \n",
    "        self.printed = False\n",
    "\n",
    "        # TODO: Initialize layers\n",
    "\n",
    "    def forward(self, img):\n",
    "\n",
    "        # TODO: Implement forward pass\n",
    "        x = img\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        if not self.printed: \n",
    "            print(\"CONV1\", x.size())\n",
    "        x = self.pool1(x)\n",
    "        if not self.printed: \n",
    "            print(\"POOL1\", x.size())\n",
    "        \n",
    "        x = F.relu(self.conv2(x))\n",
    "        if not self.printed: \n",
    "            print(\"CONV2\", x.size())\n",
    "        x = self.pool2(x)\n",
    "        if not self.printed: \n",
    "            print(\"POOL2\", x.size())\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        if not self.printed: \n",
    "            print(\"FC1\", x.size())\n",
    "        x = F.relu(self.fc2(x))\n",
    "        if not self.printed: \n",
    "            print(\"FC2\", x.size())\n",
    "        x = self.fc3(x)\n",
    "        if not self.printed: \n",
    "            print(\"FC3\", x.size())\n",
    "            self.printed = True\n",
    "\n",
    "        return x\n",
    "\n",
    "# TODO: You can change these data augmentation and normalization strategies for\n",
    "#  better training and testing (https://pytorch.org/vision/stable/transforms.html)\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((28, 28)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((28, 28)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Dataset initialization\n",
    "data_dir = 'data' # Suppose the dataset is stored under this folder\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'test']} # Read train and test sets, respectively.\n",
    "\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=0) for x in ['train', 'test']}\n",
    "# trainloader = torch.utils.data.DataLoader(image_datasets['train'], batch_size=4, shuffle=True, num_workers=2)\n",
    "# teatloader = torch.utils.data.DataLoader(image_datasets['test'], batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'test']}\n",
    "\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # Set device to \"cpu\" if you have no gpu\n",
    "\n",
    "# TODO: Implement training and testing procedures (https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html)\n",
    "def train_test(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    \n",
    "    for epoch in range(num_epochs):  \n",
    "\n",
    "        running_loss = 0.0\n",
    "        loss_record=[]\n",
    "        for i, data in enumerate(dataloaders['train'], 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "                loss_record.append(running_loss / 2000)\n",
    "                # print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000))\n",
    "                running_loss = 0.0\n",
    "                \n",
    "        print(datetime.datetime.now(), ' Epoch', (epoch + 1), ': Average Loss', loss_record)\n",
    "\n",
    "    print('Finished Training')\n",
    "    \n",
    "    \n",
    "    # save training results\n",
    "    PATH = './cifar_net.pth'\n",
    "    torch.save(model.state_dict(), PATH)\n",
    "    \n",
    "    \n",
    "    # testing overall correct rate\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloaders['train'], 0):\n",
    "            images, labels = data\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = model(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Training accuracy: %d %%' % (100 * correct / total))\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloaders['test'], 0):\n",
    "            images, labels = data\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = model(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Testing accuracy: %d %%' % (100 * correct / total))\n",
    "    \n",
    "    # prepare to count predictions for each class\n",
    "    correct_pred = {classname: 0 for classname in class_names}\n",
    "    total_pred = {classname: 0 for classname in class_names}\n",
    "\n",
    "    # again no gradients needed\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloaders['test'], 0):\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            # collect the correct predictions for each class\n",
    "            for label, prediction in zip(labels, predictions):\n",
    "                if label == prediction:\n",
    "                    correct_pred[class_names[label]] += 1\n",
    "                total_pred[class_names[label]] += 1\n",
    "\n",
    "\n",
    "    # print accuracy for each class\n",
    "    print(\"Testing accuracy (each class): \")\n",
    "    for classname, correct_count in correct_pred.items():\n",
    "        accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "        print(\"{:1s}: {:.1f}%;  \".format(classname, accuracy), end=' ')\n",
    "    print()    \n",
    "    \n",
    "    return None\n",
    "\n",
    "model_ft = Net() # Model initialization\n",
    "\n",
    "model_ft = model_ft.to(device) # Move model to cpu\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # Loss function initialization\n",
    "\n",
    "# TODO: Adjust the following hyper-parameters: learning rate, decay strategy, number of training epochs.\n",
    "optimizer_ft = optim.Adam(model_ft.parameters(), lr=1e-4) # Optimizer initialization\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=20, gamma=0.1) # Learning rate decay strategy\n",
    "\n",
    "for n in range(6):\n",
    "    print(datetime.datetime.now())\n",
    "    epo = 5*n+5\n",
    "    print(\"epoch range: \", epo-4, \" to \", epo)\n",
    "    train_test(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONV1 torch.Size([4, 12, 24, 24])\n",
      "POOL1 torch.Size([4, 12, 12, 12])\n",
      "CONV2 torch.Size([4, 36, 8, 8])\n",
      "POOL2 torch.Size([4, 36, 4, 4])\n",
      "FC1 torch.Size([4, 120])\n",
      "FC2 torch.Size([4, 90])\n",
      "FC3 torch.Size([4, 10])\n",
      "Training accuracy: 89 %\n",
      "Testing accuracy: 84 %\n",
      "Testing accuracy (each class): \n",
      "0: 82.2%;   1: 86.4%;   2: 82.8%;   3: 75.0%;   4: 88.2%;   5: 84.8%;   6: 82.8%;   7: 88.6%;   8: 86.4%;   9: 88.8%;   \n"
     ]
    }
   ],
   "source": [
    "PATH = './cifar_net.pth'\n",
    "net = Net()\n",
    "net.load_state_dict(torch.load(PATH))\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "    \n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(dataloaders['train'], 0):\n",
    "        images, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Training accuracy: %d %%' % (100 * correct / total))\n",
    "    \n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(dataloaders['test'], 0):\n",
    "        images, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Testing accuracy: %d %%' % (100 * correct / total))\n",
    "    \n",
    "# prepare to count predictions for each class\n",
    "correct_pred = {classname: 0 for classname in class_names}\n",
    "total_pred = {classname: 0 for classname in class_names}\n",
    "\n",
    "# again no gradients needed\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(dataloaders['test'], 0):\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[class_names[label]] += 1\n",
    "            total_pred[class_names[label]] += 1\n",
    "\n",
    "\n",
    "# print accuracy for each class\n",
    "print(\"Testing accuracy (each class): \")\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(\"{:1s}: {:.1f}%;  \".format(classname, accuracy), end=' ')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
