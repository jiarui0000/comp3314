{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this document is for the trial CNN structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import datasets, transforms, utils\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import datetime\n",
    "\n",
    "# TODO: Implement a convolutional neural network (https://pytorch.org/tutorials/recipes/recipes/defining_a_neural_network.html)\n",
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Input - 1x32x32\n",
    "    Output - 10\n",
    "    CONV1->CONV2->POOL1->CONV3->CONV4->POOL2->FC1->FC2->FC3\n",
    "    \"\"\"\n",
    "    def __init__(self, params):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        #Initialize layers\n",
    "        self.params = params\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(*self.params['conv'][1])\n",
    "        self.conv2 = nn.Conv2d(*self.params['conv'][2])\n",
    "        self.conv3 = nn.Conv2d(*self.params['conv'][3])\n",
    "        self.conv4 = nn.Conv2d(*self.params['conv'][4])\n",
    "        \n",
    "        self.pool1 = nn.MaxPool2d(*self.params['pool'][1])\n",
    "        self.pool2 = nn.MaxPool2d(*self.params['pool'][2])\n",
    "        \n",
    "        self.fc1 = nn.Linear(*self.params['fc'][1])\n",
    "        self.fc2 = nn.Linear(*self.params['fc'][2])\n",
    "        self.fc3 = nn.Linear(*self.params['fc'][3])\n",
    "        \n",
    "        self.printed = False\n",
    "\n",
    "    def forward(self, img):\n",
    "        # Implement forward pass\n",
    "        x = img\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        if not self.printed: \n",
    "            print(\"CONV1\", x.size(), end=\" || \")\n",
    "        x = F.relu(self.conv2(x))\n",
    "        if not self.printed: \n",
    "            print(\"CONV2\", x.size(), end=\" || \")\n",
    "        x = self.pool1(x)\n",
    "        if not self.printed: \n",
    "            print(\"POOL1\", x.size())\n",
    "        \n",
    "        x = F.relu(self.conv3(x))\n",
    "        if not self.printed: \n",
    "            print(\"CONV3\", x.size(), end=\" || \")\n",
    "        x = F.relu(self.conv4(x))\n",
    "        if not self.printed: \n",
    "            print(\"CONV4\", x.size(), end=\" || \")\n",
    "        x = self.pool2(x)\n",
    "        if not self.printed: \n",
    "            print(\"POOL2\", x.size())\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        if not self.printed: \n",
    "            print(\"FC1\", x.size(), end=\" || \")\n",
    "        x = F.relu(self.fc2(x))\n",
    "        if not self.printed: \n",
    "            print(\"FC2\", x.size(), end=\" || \")\n",
    "        x = self.fc3(x)\n",
    "        if not self.printed: \n",
    "            print(\"FC3\", x.size())\n",
    "            self.printed = True\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    \n",
    "    # overall training correct rate\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloaders['train'], 0):\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print('Training accuracy: %d %%' % (100 * correct / total))\n",
    "    \n",
    "    # overall testing correct rate\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloaders['test'], 0):\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print('Testing accuracy: %d %%' % (100 * correct / total))\n",
    "    \n",
    "    # count testing predictions for each class\n",
    "    correct_pred = {classname: 0 for classname in class_names}\n",
    "    total_pred = {classname: 0 for classname in class_names}\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloaders['test'], 0):\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            for label, prediction in zip(labels, predictions):\n",
    "                if label == prediction:\n",
    "                    correct_pred[class_names[label]] += 1\n",
    "                total_pred[class_names[label]] += 1\n",
    "\n",
    "    # print accuracy for each class\n",
    "    print(\"Testing accuracy (each class): \")\n",
    "    for classname, correct_count in correct_pred.items():\n",
    "        accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "        print(\"{:1s}: {:.1f}%;  \".format(classname, accuracy), end=' ')\n",
    "        if classname == \"5\":\n",
    "            print()\n",
    "    print()\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "def train_test(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    \n",
    "    for epoch in range(num_epochs):  \n",
    "\n",
    "        running_loss = 0.0\n",
    "        loss_record=[]\n",
    "        for i, data in enumerate(dataloaders['train'], 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "                loss_record.append(round(running_loss / 2000, 4))\n",
    "                # print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000))\n",
    "                running_loss = 0.0\n",
    "                \n",
    "        print(datetime.datetime.now(), ' Epoch', (epoch + 1), ': Average Loss', loss_record)\n",
    "\n",
    "    print('Finished Training')\n",
    "    \n",
    "    test(model)\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Data transformer\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((32,32)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((32,32)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Dataset initialization\n",
    "data_dir = 'data' \n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'test']} # Read train and test sets, respectively.\n",
    "\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=0) for x in ['train', 'test']}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'test']}\n",
    "\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # Set device to \"cpu\" if you have no gpu\n",
    "\n",
    "paths = []\n",
    "params = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-08 12:11:26.344470\n",
      "epoch range:  1  to  5\n",
      "CONV1 torch.Size([4, 16, 30, 30]) || CONV2 torch.Size([4, 32, 30, 30]) || POOL1 torch.Size([4, 32, 15, 15])\n",
      "CONV3 torch.Size([4, 32, 13, 13]) || CONV4 torch.Size([4, 16, 13, 13]) || POOL2 torch.Size([4, 16, 6, 6])\n",
      "FC1 torch.Size([4, 120]) || FC2 torch.Size([4, 90]) || FC3 torch.Size([4, 10])\n",
      "2021-05-08 12:12:25.040003  Epoch 1 : Average Loss [2.1886, 1.4595, 1.2883]\n",
      "2021-05-08 12:13:16.364284  Epoch 2 : Average Loss [1.0321, 0.9216, 0.8706]\n",
      "2021-05-08 12:14:09.959245  Epoch 3 : Average Loss [0.7593, 0.742, 0.7072]\n",
      "2021-05-08 12:14:59.844014  Epoch 4 : Average Loss [0.6387, 0.6195, 0.6]\n",
      "2021-05-08 12:15:49.619419  Epoch 5 : Average Loss [0.5561, 0.5468, 0.54]\n",
      "Finished Training\n",
      "Training accuracy: 84 %\n",
      "Testing accuracy: 81 %\n",
      "Testing accuracy (each class): \n",
      "0: 87.2%;   1: 90.4%;   2: 86.2%;   3: 65.6%;   4: 86.4%;   5: 81.2%;   \n",
      "6: 78.8%;   7: 87.0%;   8: 71.2%;   9: 81.8%;   \n",
      "2021-05-08 12:16:23.105982\n",
      "epoch range:  6  to  10\n",
      "2021-05-08 12:17:10.314482  Epoch 1 : Average Loss [0.4996, 0.4931, 0.5006]\n",
      "2021-05-08 12:17:57.327166  Epoch 2 : Average Loss [0.4563, 0.4608, 0.4671]\n",
      "2021-05-08 12:18:52.059131  Epoch 3 : Average Loss [0.428, 0.4419, 0.4144]\n",
      "2021-05-08 12:19:55.056863  Epoch 4 : Average Loss [0.3917, 0.4027, 0.4336]\n",
      "2021-05-08 12:20:50.452173  Epoch 5 : Average Loss [0.3886, 0.3649, 0.4025]\n",
      "Finished Training\n",
      "Training accuracy: 88 %\n",
      "Testing accuracy: 85 %\n",
      "Testing accuracy (each class): \n",
      "0: 91.2%;   1: 82.0%;   2: 85.0%;   3: 81.2%;   4: 88.0%;   5: 85.2%;   \n",
      "6: 82.4%;   7: 91.6%;   8: 84.4%;   9: 84.4%;   \n",
      "2021-05-08 12:21:22.171575\n",
      "epoch range:  11  to  15\n",
      "2021-05-08 12:22:19.201540  Epoch 1 : Average Loss [0.3416, 0.3741, 0.3677]\n",
      "2021-05-08 12:23:16.150283  Epoch 2 : Average Loss [0.346, 0.3376, 0.36]\n",
      "2021-05-08 12:24:03.365833  Epoch 3 : Average Loss [0.3129, 0.3418, 0.3216]\n",
      "2021-05-08 12:24:53.275733  Epoch 4 : Average Loss [0.2987, 0.3054, 0.3246]\n",
      "2021-05-08 12:25:48.869912  Epoch 5 : Average Loss [0.294, 0.2874, 0.2896]\n",
      "Finished Training\n",
      "Training accuracy: 91 %\n",
      "Testing accuracy: 87 %\n",
      "Testing accuracy (each class): \n",
      "0: 89.6%;   1: 88.8%;   2: 89.8%;   3: 81.6%;   4: 92.2%;   5: 85.8%;   \n",
      "6: 82.4%;   7: 92.6%;   8: 84.8%;   9: 84.8%;   \n",
      "2021-05-08 12:26:20.338586\n",
      "epoch range:  16  to  20\n",
      "2021-05-08 12:27:09.459046  Epoch 1 : Average Loss [0.2694, 0.2697, 0.3059]\n",
      "2021-05-08 12:28:00.583215  Epoch 2 : Average Loss [0.2543, 0.2721, 0.2662]\n",
      "2021-05-08 12:28:54.276421  Epoch 3 : Average Loss [0.248, 0.26, 0.2655]\n",
      "2021-05-08 12:29:42.123984  Epoch 4 : Average Loss [0.241, 0.2422, 0.2435]\n",
      "2021-05-08 12:30:29.433197  Epoch 5 : Average Loss [0.2281, 0.2489, 0.2294]\n",
      "Finished Training\n",
      "Training accuracy: 93 %\n",
      "Testing accuracy: 87 %\n",
      "Testing accuracy (each class): \n",
      "0: 85.6%;   1: 91.4%;   2: 89.2%;   3: 82.2%;   4: 92.6%;   5: 87.0%;   \n",
      "6: 83.4%;   7: 86.8%;   8: 88.6%;   9: 86.6%;   \n",
      "2021-05-08 12:30:58.535699\n",
      "epoch range:  21  to  25\n",
      "2021-05-08 12:32:00.086809  Epoch 1 : Average Loss [0.2173, 0.2224, 0.2439]\n",
      "2021-05-08 12:32:56.378171  Epoch 2 : Average Loss [0.2036, 0.2033, 0.2171]\n",
      "2021-05-08 12:33:44.161647  Epoch 3 : Average Loss [0.1913, 0.2031, 0.2004]\n",
      "2021-05-08 12:34:33.290725  Epoch 4 : Average Loss [0.1882, 0.1908, 0.1824]\n",
      "2021-05-08 12:35:27.384298  Epoch 5 : Average Loss [0.1634, 0.175, 0.1829]\n",
      "Finished Training\n",
      "Training accuracy: 93 %\n",
      "Testing accuracy: 86 %\n",
      "Testing accuracy (each class): \n",
      "0: 82.8%;   1: 89.6%;   2: 86.4%;   3: 76.2%;   4: 91.0%;   5: 88.2%;   \n",
      "6: 84.2%;   7: 89.8%;   8: 89.0%;   9: 90.0%;   \n",
      "2021-05-08 12:35:58.022961\n",
      "epoch range:  26  to  30\n",
      "2021-05-08 12:36:45.174003  Epoch 1 : Average Loss [0.1642, 0.1727, 0.1884]\n",
      "2021-05-08 12:37:34.794689  Epoch 2 : Average Loss [0.1501, 0.1816, 0.1664]\n",
      "2021-05-08 12:38:28.482916  Epoch 3 : Average Loss [0.1463, 0.1719, 0.1609]\n",
      "2021-05-08 12:39:17.693851  Epoch 4 : Average Loss [0.1318, 0.1533, 0.1516]\n",
      "2021-05-08 12:40:05.607280  Epoch 5 : Average Loss [0.1346, 0.1388, 0.1623]\n",
      "Finished Training\n",
      "Training accuracy: 96 %\n",
      "Testing accuracy: 86 %\n",
      "Testing accuracy (each class): \n",
      "0: 86.6%;   1: 87.0%;   2: 90.2%;   3: 83.0%;   4: 91.2%;   5: 85.0%;   \n",
      "6: 85.0%;   7: 89.0%;   8: 86.6%;   9: 84.8%;   \n"
     ]
    }
   ],
   "source": [
    "param = {'conv':[(), \n",
    "                  (3, 16, 5, 1, 1), \n",
    "                  (16, 32, 3, 1, 1),\n",
    "                  (32, 32, 3, 1, 0),\n",
    "                  (32, 16, 3, 1, 1)], # in_channels, out_channels, kernel_size, stride, padding\n",
    "          'pool':[(), \n",
    "                  (2, 2, 0),\n",
    "                  (2, 2, 0)], # kernel_size, stride, padding\n",
    "          'fc':[(), \n",
    "                (16*6*6, 120),\n",
    "                (120, 90), \n",
    "                (90, 10)] # in_channels, out_channels\n",
    "         }\n",
    "params.append(param)\n",
    "\n",
    "model_ft = Net(param) # Model initialization\n",
    "model_ft = model_ft.to(device) # Move model to cpu\n",
    "criterion = nn.CrossEntropyLoss() # Loss function initialization\n",
    "# TODO: Adjust the following hyper-parameters: learning rate, decay strategy, number of training epochs.\n",
    "optimizer_ft = optim.Adam(model_ft.parameters(), lr=1e-4) # Optimizer initialization\n",
    "\n",
    "exponen_lr_scheduler = lr_scheduler.ExponentialLR(optimizer_ft, gamma=0.1)\n",
    "epoch = 30\n",
    "\n",
    "for n in range((epoch // 5)):\n",
    "    print(datetime.datetime.now())\n",
    "    epo = 5*n+5\n",
    "    print(\"epoch range: \", epo-4, \" to \", epo)\n",
    "    train_test(model_ft, criterion, optimizer_ft, exponen_lr_scheduler, num_epochs=5)\n",
    "\n",
    "PATH = './9layer_30round_adam1e-4_exponlr.pth'\n",
    "paths.append(PATH)\n",
    "torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model_ft.state_dict(),\n",
    "        'optimizer_state_dict': optimizer_ft.state_dict()\n",
    "        }, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-08 12:40:35.209238\n",
      "epoch range:  1  to  5\n",
      "CONV1 torch.Size([4, 16, 30, 30]) || CONV2 torch.Size([4, 32, 30, 30]) || POOL1 torch.Size([4, 32, 15, 15])\n",
      "CONV3 torch.Size([4, 32, 13, 13]) || CONV4 torch.Size([4, 16, 13, 13]) || POOL2 torch.Size([4, 16, 6, 6])\n",
      "FC1 torch.Size([4, 120]) || FC2 torch.Size([4, 90]) || FC3 torch.Size([4, 10])\n",
      "2021-05-08 12:41:28.348102  Epoch 1 : Average Loss [2.2071, 1.3967, 1.1092]\n",
      "2021-05-08 12:42:15.807525  Epoch 2 : Average Loss [0.9484, 0.8519, 0.7937]\n",
      "2021-05-08 12:43:03.267877  Epoch 3 : Average Loss [0.7265, 0.6834, 0.6487]\n",
      "2021-05-08 12:43:54.520933  Epoch 4 : Average Loss [0.5878, 0.599, 0.5714]\n",
      "2021-05-08 12:44:46.268942  Epoch 5 : Average Loss [0.5317, 0.5088, 0.5096]\n",
      "Finished Training\n",
      "Training accuracy: 85 %\n",
      "Testing accuracy: 83 %\n",
      "Testing accuracy (each class): \n",
      "0: 87.6%;   1: 85.2%;   2: 87.2%;   3: 73.0%;   4: 92.0%;   5: 81.0%;   \n",
      "6: 76.2%;   7: 89.6%;   8: 80.0%;   9: 84.0%;   \n",
      "2021-05-08 12:45:13.740000\n",
      "epoch range:  6  to  10\n",
      "2021-05-08 12:46:02.187860  Epoch 1 : Average Loss [0.4762, 0.4797, 0.477]\n",
      "2021-05-08 12:46:59.159999  Epoch 2 : Average Loss [0.4634, 0.4352, 0.441]\n",
      "2021-05-08 12:47:54.281288  Epoch 3 : Average Loss [0.3954, 0.4138, 0.4203]\n",
      "2021-05-08 12:48:42.847189  Epoch 4 : Average Loss [0.3776, 0.3932, 0.3912]\n",
      "2021-05-08 12:49:31.162545  Epoch 5 : Average Loss [0.366, 0.3476, 0.369]\n",
      "Finished Training\n",
      "Training accuracy: 89 %\n",
      "Testing accuracy: 85 %\n",
      "Testing accuracy (each class): \n",
      "0: 90.6%;   1: 85.2%;   2: 89.2%;   3: 79.2%;   4: 86.6%;   5: 84.8%;   \n",
      "6: 78.8%;   7: 87.0%;   8: 85.0%;   9: 89.6%;   \n",
      "2021-05-08 12:50:00.959907\n",
      "epoch range:  11  to  15\n",
      "2021-05-08 12:50:55.667831  Epoch 1 : Average Loss [0.3412, 0.3464, 0.3585]\n",
      "2021-05-08 12:51:45.125312  Epoch 2 : Average Loss [0.3201, 0.3106, 0.3236]\n",
      "2021-05-08 12:52:34.405565  Epoch 3 : Average Loss [0.301, 0.3153, 0.3173]\n",
      "2021-05-08 12:53:27.979139  Epoch 4 : Average Loss [0.2794, 0.2932, 0.3054]\n",
      "2021-05-08 12:54:22.833735  Epoch 5 : Average Loss [0.2815, 0.2753, 0.2884]\n",
      "Finished Training\n",
      "Training accuracy: 92 %\n",
      "Testing accuracy: 86 %\n",
      "Testing accuracy (each class): \n",
      "0: 91.2%;   1: 85.4%;   2: 90.6%;   3: 80.8%;   4: 89.0%;   5: 87.0%;   \n",
      "6: 84.6%;   7: 91.0%;   8: 83.0%;   9: 87.2%;   \n",
      "2021-05-08 12:54:49.774471\n",
      "epoch range:  16  to  20\n",
      "2021-05-08 12:55:39.441986  Epoch 1 : Average Loss [0.2605, 0.2769, 0.2736]\n",
      "2021-05-08 12:56:32.918842  Epoch 2 : Average Loss [0.2547, 0.2612, 0.2492]\n",
      "2021-05-08 12:57:27.028401  Epoch 3 : Average Loss [0.2377, 0.2422, 0.2471]\n",
      "2021-05-08 12:58:15.205196  Epoch 4 : Average Loss [0.2148, 0.2472, 0.2293]\n",
      "2021-05-08 12:59:06.180277  Epoch 5 : Average Loss [0.2168, 0.1995, 0.2307]\n",
      "Finished Training\n",
      "Training accuracy: 94 %\n",
      "Testing accuracy: 86 %\n",
      "Testing accuracy (each class): \n",
      "0: 89.0%;   1: 86.8%;   2: 83.0%;   3: 77.8%;   4: 88.8%;   5: 88.6%;   \n",
      "6: 81.8%;   7: 90.8%;   8: 87.0%;   9: 90.4%;   \n",
      "2021-05-08 12:59:37.274854\n",
      "epoch range:  21  to  25\n",
      "2021-05-08 13:00:29.739575  Epoch 1 : Average Loss [0.1887, 0.214, 0.2278]\n",
      "2021-05-08 13:01:17.495385  Epoch 2 : Average Loss [0.1895, 0.2032, 0.2044]\n",
      "2021-05-08 13:02:08.631790  Epoch 3 : Average Loss [0.1855, 0.1918, 0.1965]\n",
      "2021-05-08 13:03:04.013824  Epoch 4 : Average Loss [0.182, 0.1812, 0.1828]\n",
      "2021-05-08 13:03:53.508087  Epoch 5 : Average Loss [0.161, 0.1713, 0.1796]\n",
      "Finished Training\n",
      "Training accuracy: 95 %\n",
      "Testing accuracy: 87 %\n",
      "Testing accuracy (each class): \n",
      "0: 89.2%;   1: 87.8%;   2: 84.8%;   3: 84.8%;   4: 90.2%;   5: 84.6%;   \n",
      "6: 84.8%;   7: 91.6%;   8: 83.2%;   9: 89.0%;   \n",
      "2021-05-08 13:04:20.551700\n",
      "epoch range:  26  to  30\n",
      "2021-05-08 13:05:11.438792  Epoch 1 : Average Loss [0.1556, 0.1743, 0.1701]\n",
      "2021-05-08 13:06:06.447483  Epoch 2 : Average Loss [0.1365, 0.168, 0.165]\n",
      "2021-05-08 13:06:56.272641  Epoch 3 : Average Loss [0.1448, 0.1533, 0.1454]\n",
      "2021-05-08 13:07:51.551578  Epoch 4 : Average Loss [0.1321, 0.1432, 0.1445]\n",
      "2021-05-08 13:08:52.018644  Epoch 5 : Average Loss [0.121, 0.1228, 0.1564]\n",
      "Finished Training\n",
      "Training accuracy: 96 %\n",
      "Testing accuracy: 86 %\n",
      "Testing accuracy (each class): \n",
      "0: 91.0%;   1: 87.4%;   2: 86.4%;   3: 85.0%;   4: 87.6%;   5: 85.8%;   \n",
      "6: 83.8%;   7: 88.6%;   8: 85.8%;   9: 84.6%;   \n"
     ]
    }
   ],
   "source": [
    "param = {'conv':[(), \n",
    "                  (3, 16, 5, 1, 1), \n",
    "                  (16, 32, 3, 1, 1),\n",
    "                  (32, 32, 3, 1, 0),\n",
    "                  (32, 16, 3, 1, 1)], # in_channels, out_channels, kernel_size, stride, padding\n",
    "          'pool':[(), \n",
    "                  (2, 2, 0),\n",
    "                  (2, 2, 0)], # kernel_size, stride, padding\n",
    "          'fc':[(), \n",
    "                (16*6*6, 120),\n",
    "                (120, 90), \n",
    "                (90, 10)] # in_channels, out_channels\n",
    "         }\n",
    "params.append(param)\n",
    "\n",
    "model_ft = Net(param) # Model initialization\n",
    "model_ft = model_ft.to(device) # Move model to cpu\n",
    "criterion = nn.CrossEntropyLoss() # Loss function initialization\n",
    "# TODO: Adjust the following hyper-parameters: learning rate, decay strategy, number of training epochs.\n",
    "optimizer_ft = optim.Adam(model_ft.parameters(), lr=1e-4) # Optimizer initialization\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=20, gamma=0.1) # Learning rate decay strategy\n",
    "\n",
    "epoch = 30\n",
    "\n",
    "for n in range((epoch // 5)):\n",
    "    print(datetime.datetime.now())\n",
    "    epo = 5*n+5\n",
    "    print(\"epoch range: \", epo-4, \" to \", epo)\n",
    "    train_test(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=5)\n",
    "\n",
    "PATH = './9layer_30round_adam1e-4_steplr.pth'\n",
    "paths.append(PATH)\n",
    "torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model_ft.state_dict(),\n",
    "        'optimizer_state_dict': optimizer_ft.state_dict()\n",
    "        }, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-08 13:09:28.653811\n",
      "epoch range:  1  to  5\n",
      "CONV1 torch.Size([4, 16, 30, 30]) || CONV2 torch.Size([4, 32, 30, 30]) || POOL1 torch.Size([4, 32, 15, 15])\n",
      "CONV3 torch.Size([4, 32, 13, 13]) || CONV4 torch.Size([4, 16, 13, 13]) || POOL2 torch.Size([4, 16, 6, 6])\n",
      "FC1 torch.Size([4, 120]) || FC2 torch.Size([4, 90]) || FC3 torch.Size([4, 10])\n",
      "2021-05-08 13:10:28.795619  Epoch 1 : Average Loss [2.3042, 2.3033, 2.2931]\n",
      "2021-05-08 13:11:22.631020  Epoch 2 : Average Loss [2.0532, 1.8665, 1.7453]\n",
      "2021-05-08 13:12:21.131912  Epoch 3 : Average Loss [1.6027, 1.5782, 1.5474]\n",
      "2021-05-08 13:13:12.941022  Epoch 4 : Average Loss [1.4713, 1.4471, 1.435]\n",
      "2021-05-08 13:13:59.063583  Epoch 5 : Average Loss [1.4231, 1.3721, 1.3719]\n",
      "Finished Training\n",
      "Training accuracy: 55 %\n",
      "Testing accuracy: 55 %\n",
      "Testing accuracy (each class): \n",
      "0: 66.6%;   1: 72.2%;   2: 50.8%;   3: 32.8%;   4: 66.0%;   5: 47.4%;   \n",
      "6: 48.8%;   7: 75.0%;   8: 39.2%;   9: 51.4%;   \n",
      "2021-05-08 13:14:26.838408\n",
      "epoch range:  6  to  10\n",
      "2021-05-08 13:15:16.824815  Epoch 1 : Average Loss [1.3589, 1.316, 1.3123]\n",
      "2021-05-08 13:16:09.673560  Epoch 2 : Average Loss [1.2876, 1.2922, 1.2736]\n",
      "2021-05-08 13:16:55.673684  Epoch 3 : Average Loss [1.26, 1.2373, 1.2499]\n",
      "2021-05-08 13:17:43.483243  Epoch 4 : Average Loss [1.2178, 1.2132, 1.2006]\n",
      "2021-05-08 13:18:35.380425  Epoch 5 : Average Loss [1.1945, 1.1963, 1.173]\n",
      "Finished Training\n",
      "Training accuracy: 61 %\n",
      "Testing accuracy: 61 %\n",
      "Testing accuracy (each class): \n",
      "0: 68.4%;   1: 78.4%;   2: 55.6%;   3: 40.0%;   4: 69.6%;   5: 55.2%;   \n",
      "6: 43.4%;   7: 79.2%;   8: 59.2%;   9: 62.8%;   \n",
      "2021-05-08 13:19:06.432131\n",
      "epoch range:  11  to  15\n",
      "2021-05-08 13:19:52.928928  Epoch 1 : Average Loss [1.1362, 1.1641, 1.1663]\n",
      "2021-05-08 13:20:41.394444  Epoch 2 : Average Loss [1.1469, 1.1017, 1.1248]\n",
      "2021-05-08 13:21:33.565257  Epoch 3 : Average Loss [1.0914, 1.0869, 1.1036]\n",
      "2021-05-08 13:22:24.739882  Epoch 4 : Average Loss [1.09, 1.0847, 1.0702]\n",
      "2021-05-08 13:23:11.575414  Epoch 5 : Average Loss [1.0573, 1.0626, 1.0408]\n",
      "Finished Training\n",
      "Training accuracy: 66 %\n",
      "Testing accuracy: 64 %\n",
      "Testing accuracy (each class): \n",
      "0: 71.6%;   1: 76.8%;   2: 63.2%;   3: 40.0%;   4: 75.0%;   5: 64.2%;   \n",
      "6: 53.6%;   7: 83.2%;   8: 57.2%;   9: 61.6%;   \n",
      "2021-05-08 13:23:41.365032\n",
      "epoch range:  16  to  20\n",
      "2021-05-08 13:24:33.471608  Epoch 1 : Average Loss [1.0265, 1.0236, 1.0208]\n",
      "2021-05-08 13:25:25.969870  Epoch 2 : Average Loss [1.022, 0.991, 0.9763]\n",
      "2021-05-08 13:26:13.267567  Epoch 3 : Average Loss [0.9865, 0.9829, 0.948]\n",
      "2021-05-08 13:27:02.441855  Epoch 4 : Average Loss [0.9575, 0.947, 0.9542]\n",
      "2021-05-08 13:27:55.737411  Epoch 5 : Average Loss [0.9274, 0.9102, 0.9152]\n",
      "Finished Training\n",
      "Training accuracy: 71 %\n",
      "Testing accuracy: 69 %\n",
      "Testing accuracy (each class): \n",
      "0: 73.6%;   1: 77.6%;   2: 68.6%;   3: 46.4%;   4: 82.2%;   5: 67.2%;   \n",
      "6: 58.6%;   7: 86.8%;   8: 63.6%;   9: 66.0%;   \n",
      "2021-05-08 13:28:25.575519\n",
      "epoch range:  21  to  25\n",
      "2021-05-08 13:29:11.995005  Epoch 1 : Average Loss [0.8916, 0.899, 0.8925]\n",
      "2021-05-08 13:30:00.646802  Epoch 2 : Average Loss [0.8612, 0.8862, 0.8524]\n",
      "2021-05-08 13:30:53.172857  Epoch 3 : Average Loss [0.8519, 0.848, 0.832]\n",
      "2021-05-08 13:31:43.459936  Epoch 4 : Average Loss [0.829, 0.817, 0.8119]\n",
      "2021-05-08 13:32:31.070951  Epoch 5 : Average Loss [0.7672, 0.836, 0.802]\n",
      "Finished Training\n",
      "Training accuracy: 75 %\n",
      "Testing accuracy: 72 %\n",
      "Testing accuracy (each class): \n",
      "0: 84.0%;   1: 73.8%;   2: 76.4%;   3: 50.6%;   4: 77.6%;   5: 78.4%;   \n",
      "6: 62.0%;   7: 87.4%;   8: 67.6%;   9: 71.4%;   \n",
      "2021-05-08 13:32:59.994095\n",
      "epoch range:  26  to  30\n",
      "2021-05-08 13:33:52.289366  Epoch 1 : Average Loss [0.7835, 0.7556, 0.7944]\n",
      "2021-05-08 13:34:42.671735  Epoch 2 : Average Loss [0.7652, 0.761, 0.755]\n",
      "2021-05-08 13:35:29.320030  Epoch 3 : Average Loss [0.7443, 0.7268, 0.7269]\n",
      "2021-05-08 13:36:18.981152  Epoch 4 : Average Loss [0.7452, 0.7176, 0.7244]\n",
      "2021-05-08 13:37:12.139644  Epoch 5 : Average Loss [0.7062, 0.7059, 0.7247]\n",
      "Finished Training\n",
      "Training accuracy: 78 %\n",
      "Testing accuracy: 76 %\n",
      "Testing accuracy (each class): \n",
      "0: 88.4%;   1: 80.4%;   2: 77.8%;   3: 64.0%;   4: 78.0%;   5: 69.8%;   \n",
      "6: 73.4%;   7: 86.0%;   8: 67.6%;   9: 75.6%;   \n"
     ]
    }
   ],
   "source": [
    "param = {'conv':[(), \n",
    "                  (3, 16, 5, 1, 1), \n",
    "                  (16, 32, 3, 1, 1),\n",
    "                  (32, 32, 3, 1, 0),\n",
    "                  (32, 16, 3, 1, 1)], # in_channels, out_channels, kernel_size, stride, padding\n",
    "          'pool':[(), \n",
    "                  (2, 2, 0),\n",
    "                  (2, 2, 0)], # kernel_size, stride, padding\n",
    "          'fc':[(), \n",
    "                (16*6*6, 120),\n",
    "                (120, 90), \n",
    "                (90, 10)] # in_channels, out_channels\n",
    "         }\n",
    "params.append(param)\n",
    "\n",
    "model_ft = Net(param) # Model initialization\n",
    "model_ft = model_ft.to(device) # Move model to cpu\n",
    "criterion = nn.CrossEntropyLoss() # Loss function initialization\n",
    "# TODO: Adjust the following hyper-parameters: learning rate, decay strategy, number of training epochs.\n",
    "optimizer_ft = optim.Adam(model_ft.parameters(), lr=1e-5) # Optimizer initialization\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=20, gamma=0.1) # Learning rate decay strategy\n",
    "\n",
    "epoch = 30\n",
    "for n in range((epoch // 5)):\n",
    "    print(datetime.datetime.now())\n",
    "    epo = 5*n+5\n",
    "    print(\"epoch range: \", epo-4, \" to \", epo)\n",
    "    train_test(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=5)\n",
    "\n",
    "PATH = './9layer_30round_adam1e-5_steplr.pth'\n",
    "paths.append(PATH)\n",
    "torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model_ft.state_dict(),\n",
    "        'optimizer_state_dict': optimizer_ft.state_dict()\n",
    "        }, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./9layer_30round_adam1e-4_exponlr.pth\n",
      "CONV1 torch.Size([4, 16, 30, 30]) || CONV2 torch.Size([4, 32, 30, 30]) || POOL1 torch.Size([4, 32, 15, 15])\n",
      "CONV3 torch.Size([4, 32, 13, 13]) || CONV4 torch.Size([4, 16, 13, 13]) || POOL2 torch.Size([4, 16, 6, 6])\n",
      "FC1 torch.Size([4, 120]) || FC2 torch.Size([4, 90]) || FC3 torch.Size([4, 10])\n",
      "Training accuracy: 96 %\n",
      "Testing accuracy: 86 %\n",
      "Testing accuracy (each class): \n",
      "0: 86.6%;   1: 87.0%;   2: 90.2%;   3: 83.0%;   4: 91.2%;   5: 85.0%;   \n",
      "6: 85.0%;   7: 89.0%;   8: 86.6%;   9: 84.8%;   \n",
      "\n",
      "./9layer_30round_adam1e-4_steplr.pth\n",
      "CONV1 torch.Size([4, 16, 30, 30]) || CONV2 torch.Size([4, 32, 30, 30]) || POOL1 torch.Size([4, 32, 15, 15])\n",
      "CONV3 torch.Size([4, 32, 13, 13]) || CONV4 torch.Size([4, 16, 13, 13]) || POOL2 torch.Size([4, 16, 6, 6])\n",
      "FC1 torch.Size([4, 120]) || FC2 torch.Size([4, 90]) || FC3 torch.Size([4, 10])\n",
      "Training accuracy: 96 %\n",
      "Testing accuracy: 86 %\n",
      "Testing accuracy (each class): \n",
      "0: 91.0%;   1: 87.4%;   2: 86.4%;   3: 85.0%;   4: 87.6%;   5: 85.8%;   \n",
      "6: 83.8%;   7: 88.6%;   8: 85.8%;   9: 84.6%;   \n",
      "\n",
      "./9layer_30round_adam1e-5_steplr.pth\n",
      "CONV1 torch.Size([4, 16, 30, 30]) || CONV2 torch.Size([4, 32, 30, 30]) || POOL1 torch.Size([4, 32, 15, 15])\n",
      "CONV3 torch.Size([4, 32, 13, 13]) || CONV4 torch.Size([4, 16, 13, 13]) || POOL2 torch.Size([4, 16, 6, 6])\n",
      "FC1 torch.Size([4, 120]) || FC2 torch.Size([4, 90]) || FC3 torch.Size([4, 10])\n",
      "Training accuracy: 78 %\n",
      "Testing accuracy: 76 %\n",
      "Testing accuracy (each class): \n",
      "0: 88.4%;   1: 80.4%;   2: 77.8%;   3: 64.0%;   4: 78.0%;   5: 69.8%;   \n",
      "6: 73.4%;   7: 86.0%;   8: 67.6%;   9: 75.6%;   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(paths)):\n",
    "    model = Net(params[i])\n",
    "    optimizer = optim.Adam(model_ft.parameters(), lr=1e-4)\n",
    "    PATH = paths[i]\n",
    "    print(PATH)\n",
    "    checkpoint = torch.load(PATH)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "\n",
    "    model.eval()\n",
    "    test(model)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-08 13:44:30.875731\n",
      "epoch range:  1  to  5\n",
      "CONV1 torch.Size([4, 16, 30, 30]) || CONV2 torch.Size([4, 32, 30, 30]) || POOL1 torch.Size([4, 32, 15, 15])\n",
      "CONV3 torch.Size([4, 32, 13, 13]) || CONV4 torch.Size([4, 16, 13, 13]) || POOL2 torch.Size([4, 16, 6, 6])\n",
      "FC1 torch.Size([4, 120]) || FC2 torch.Size([4, 90]) || FC3 torch.Size([4, 10])\n",
      "2021-05-08 13:45:21.591829  Epoch 1 : Average Loss [1.8685, 1.0733, 0.8645]\n",
      "2021-05-08 13:46:17.655272  Epoch 2 : Average Loss [0.6927, 0.6657, 0.6519]\n",
      "2021-05-08 13:47:09.573017  Epoch 3 : Average Loss [0.5697, 0.5904, 0.5428]\n",
      "2021-05-08 13:48:01.873023  Epoch 4 : Average Loss [0.499, 0.5101, 0.4774]\n",
      "2021-05-08 13:48:53.024517  Epoch 5 : Average Loss [0.4742, 0.4638, 0.4699]\n",
      "Finished Training\n",
      "Training accuracy: 87 %\n",
      "Testing accuracy: 84 %\n",
      "Testing accuracy (each class): \n",
      "0: 87.2%;   1: 86.0%;   2: 84.6%;   3: 71.2%;   4: 86.4%;   5: 89.8%;   \n",
      "6: 84.4%;   7: 92.4%;   8: 82.6%;   9: 84.4%;   \n",
      "2021-05-08 13:49:23.916877\n",
      "epoch range:  6  to  10\n",
      "2021-05-08 13:50:11.195846  Epoch 1 : Average Loss [0.4313, 0.4426, 0.4536]\n",
      "2021-05-08 13:50:58.895114  Epoch 2 : Average Loss [0.4062, 0.4222, 0.4244]\n",
      "2021-05-08 13:51:51.518463  Epoch 3 : Average Loss [0.3922, 0.3889, 0.4224]\n",
      "2021-05-08 15:00:55.563953  Epoch 4 : Average Loss [0.3758, 0.3816, 0.3812]\n",
      "2021-05-08 15:01:45.620164  Epoch 5 : Average Loss [0.3558, 0.3727, 0.388]\n",
      "Finished Training\n",
      "Training accuracy: 89 %\n",
      "Testing accuracy: 86 %\n",
      "Testing accuracy (each class): \n",
      "0: 90.6%;   1: 91.6%;   2: 87.6%;   3: 77.6%;   4: 90.6%;   5: 87.6%;   \n",
      "6: 79.0%;   7: 90.6%;   8: 87.2%;   9: 85.4%;   \n",
      "2021-05-08 15:02:13.273234\n",
      "epoch range:  11  to  15\n",
      "2021-05-08 15:03:08.420640  Epoch 1 : Average Loss [0.353, 0.3572, 0.3554]\n",
      "2021-05-08 15:04:01.353021  Epoch 2 : Average Loss [0.3363, 0.3466, 0.355]\n",
      "2021-05-08 15:04:51.626112  Epoch 3 : Average Loss [0.3179, 0.3511, 0.3412]\n",
      "2021-05-08 15:05:46.188008  Epoch 4 : Average Loss [0.3143, 0.3226, 0.3275]\n",
      "2021-05-08 15:06:41.802399  Epoch 5 : Average Loss [0.3014, 0.3159, 0.3364]\n",
      "Finished Training\n",
      "Training accuracy: 91 %\n",
      "Testing accuracy: 87 %\n",
      "Testing accuracy (each class): \n",
      "0: 87.6%;   1: 90.2%;   2: 87.0%;   3: 79.0%;   4: 90.6%;   5: 87.0%;   \n",
      "6: 85.0%;   7: 94.2%;   8: 85.6%;   9: 86.6%;   \n",
      "2021-05-08 15:07:08.136884\n",
      "epoch range:  16  to  20\n",
      "2021-05-08 15:07:58.382253  Epoch 1 : Average Loss [0.2949, 0.3049, 0.3152]\n",
      "2021-05-08 15:08:52.880789  Epoch 2 : Average Loss [0.2889, 0.3126, 0.3086]\n",
      "2021-05-08 15:09:48.686467  Epoch 3 : Average Loss [0.2727, 0.3099, 0.2794]\n",
      "2021-05-08 15:10:38.552303  Epoch 4 : Average Loss [0.2803, 0.2917, 0.2999]\n",
      "2021-05-08 15:11:31.090305  Epoch 5 : Average Loss [0.2797, 0.2764, 0.2941]\n",
      "Finished Training\n",
      "Training accuracy: 90 %\n",
      "Testing accuracy: 86 %\n",
      "Testing accuracy (each class): \n",
      "0: 91.2%;   1: 88.0%;   2: 85.4%;   3: 78.2%;   4: 91.4%;   5: 82.0%;   \n",
      "6: 88.6%;   7: 90.6%;   8: 85.2%;   9: 85.0%;   \n",
      "2021-05-08 15:12:01.940479\n",
      "epoch range:  21  to  25\n",
      "2021-05-08 15:12:59.199920  Epoch 1 : Average Loss [0.2762, 0.2825, 0.273]\n",
      "2021-05-08 15:13:50.025243  Epoch 2 : Average Loss [0.2496, 0.2596, 0.2778]\n",
      "2021-05-08 15:14:44.440046  Epoch 3 : Average Loss [0.245, 0.2627, 0.2678]\n",
      "2021-05-08 15:15:48.585129  Epoch 4 : Average Loss [0.2393, 0.2694, 0.2663]\n",
      "2021-05-08 15:16:47.832587  Epoch 5 : Average Loss [0.2386, 0.2441, 0.2668]\n",
      "Finished Training\n",
      "Training accuracy: 93 %\n",
      "Testing accuracy: 87 %\n",
      "Testing accuracy (each class): \n",
      "0: 86.8%;   1: 89.8%;   2: 87.8%;   3: 76.8%;   4: 92.2%;   5: 88.6%;   \n",
      "6: 89.6%;   7: 89.8%;   8: 85.4%;   9: 90.0%;   \n",
      "2021-05-08 15:17:18.865703\n",
      "epoch range:  26  to  30\n",
      "2021-05-08 15:18:13.909281  Epoch 1 : Average Loss [0.2357, 0.2629, 0.2645]\n",
      "2021-05-08 15:19:11.061932  Epoch 2 : Average Loss [0.2412, 0.2412, 0.2582]\n",
      "2021-05-08 15:20:12.639253  Epoch 3 : Average Loss [0.2333, 0.252, 0.2652]\n",
      "2021-05-08 15:21:10.262352  Epoch 4 : Average Loss [0.2227, 0.2378, 0.2309]\n",
      "2021-05-08 15:22:07.299530  Epoch 5 : Average Loss [0.2286, 0.2264, 0.261]\n",
      "Finished Training\n",
      "Training accuracy: 93 %\n",
      "Testing accuracy: 87 %\n",
      "Testing accuracy (each class): \n",
      "0: 88.2%;   1: 89.6%;   2: 86.6%;   3: 81.2%;   4: 89.4%;   5: 87.6%;   \n",
      "6: 83.8%;   7: 90.8%;   8: 88.0%;   9: 85.2%;   \n"
     ]
    }
   ],
   "source": [
    "param = {'conv':[(), \n",
    "                  (3, 16, 5, 1, 1), \n",
    "                  (16, 32, 3, 1, 1),\n",
    "                  (32, 32, 3, 1, 0),\n",
    "                  (32, 16, 3, 1, 1)], # in_channels, out_channels, kernel_size, stride, padding\n",
    "          'pool':[(), \n",
    "                  (2, 2, 0),\n",
    "                  (2, 2, 0)], # kernel_size, stride, padding\n",
    "          'fc':[(), \n",
    "                (16*6*6, 120),\n",
    "                (120, 90), \n",
    "                (90, 10)] # in_channels, out_channels\n",
    "         }\n",
    "params.append(param)\n",
    "\n",
    "model_ft = Net(param) # Model initialization\n",
    "model_ft = model_ft.to(device) # Move model to cpu\n",
    "criterion = nn.CrossEntropyLoss() # Loss function initialization\n",
    "# TODO: Adjust the following hyper-parameters: learning rate, decay strategy, number of training epochs.\n",
    "optimizer_ft = optim.Adam(model_ft.parameters(), lr=5e-4) # Optimizer initialization\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=20, gamma=0.1) # Learning rate decay strategy\n",
    "\n",
    "epoch = 30\n",
    "for n in range((epoch // 5)):\n",
    "    print(datetime.datetime.now())\n",
    "    epo = 5*n+5\n",
    "    print(\"epoch range: \", epo-4, \" to \", epo)\n",
    "    train_test(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=5)\n",
    "\n",
    "PATH = './9layer_30round_adam5e-4_steplr.pth'\n",
    "paths.append(PATH)\n",
    "torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model_ft.state_dict(),\n",
    "        'optimizer_state_dict': optimizer_ft.state_dict()\n",
    "        }, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-08 15:22:35.418812\n",
      "epoch range:  1  to  5\n",
      "CONV1 torch.Size([4, 16, 30, 30]) || CONV2 torch.Size([4, 32, 30, 30]) || POOL1 torch.Size([4, 32, 15, 15])\n",
      "CONV3 torch.Size([4, 32, 13, 13]) || CONV4 torch.Size([4, 16, 13, 13]) || POOL2 torch.Size([4, 16, 6, 6])\n",
      "FC1 torch.Size([4, 120]) || FC2 torch.Size([4, 90]) || FC3 torch.Size([4, 10])\n",
      "2021-05-08 15:23:20.967254  Epoch 1 : Average Loss [2.3018, 1.7417, 1.431]\n",
      "2021-05-08 15:24:09.922952  Epoch 2 : Average Loss [1.2775, 1.2028, 1.1288]\n",
      "2021-05-08 15:25:03.300994  Epoch 3 : Average Loss [1.0, 0.9492, 0.9183]\n",
      "2021-05-08 15:25:51.106545  Epoch 4 : Average Loss [0.8433, 0.7675, 0.7655]\n",
      "2021-05-08 15:26:37.336319  Epoch 5 : Average Loss [0.6912, 0.6726, 0.6586]\n",
      "Finished Training\n",
      "Training accuracy: 80 %\n",
      "Testing accuracy: 77 %\n",
      "Testing accuracy (each class): \n",
      "0: 85.2%;   1: 83.2%;   2: 82.6%;   3: 66.8%;   4: 82.6%;   5: 73.4%;   \n",
      "6: 66.6%;   7: 90.0%;   8: 73.8%;   9: 71.8%;   \n",
      "2021-05-08 15:27:08.380409\n",
      "epoch range:  6  to  10\n",
      "2021-05-08 15:28:04.209004  Epoch 1 : Average Loss [0.5917, 0.5755, 0.5977]\n",
      "2021-05-08 15:28:57.226261  Epoch 2 : Average Loss [0.5283, 0.5436, 0.5181]\n",
      "2021-05-08 15:29:49.469715  Epoch 3 : Average Loss [0.482, 0.5112, 0.4754]\n",
      "2021-05-08 15:30:38.842643  Epoch 4 : Average Loss [0.4471, 0.4627, 0.4457]\n",
      "2021-05-08 15:31:40.059320  Epoch 5 : Average Loss [0.4242, 0.4362, 0.4147]\n",
      "Finished Training\n",
      "Training accuracy: 88 %\n",
      "Testing accuracy: 83 %\n",
      "Testing accuracy (each class): \n",
      "0: 88.0%;   1: 87.0%;   2: 82.8%;   3: 74.6%;   4: 89.4%;   5: 81.2%;   \n",
      "6: 79.4%;   7: 92.4%;   8: 78.8%;   9: 82.4%;   \n",
      "2021-05-08 15:32:15.582421\n",
      "epoch range:  11  to  15\n",
      "2021-05-08 15:33:13.222769  Epoch 1 : Average Loss [0.3823, 0.415, 0.4048]\n",
      "2021-05-08 15:34:14.162870  Epoch 2 : Average Loss [0.37, 0.3745, 0.3742]\n",
      "2021-05-08 15:35:12.298535  Epoch 3 : Average Loss [0.3433, 0.3576, 0.3768]\n",
      "2021-05-08 15:36:12.237709  Epoch 4 : Average Loss [0.33, 0.3506, 0.3443]\n",
      "2021-05-08 15:37:09.490665  Epoch 5 : Average Loss [0.3126, 0.3276, 0.3267]\n",
      "Finished Training\n",
      "Training accuracy: 90 %\n",
      "Testing accuracy: 85 %\n",
      "Testing accuracy (each class): \n",
      "0: 84.8%;   1: 86.0%;   2: 87.2%;   3: 68.6%;   4: 87.4%;   5: 90.4%;   \n",
      "6: 85.4%;   7: 91.2%;   8: 85.2%;   9: 86.0%;   \n",
      "2021-05-08 15:37:44.996213\n",
      "epoch range:  16  to  20\n",
      "2021-05-08 15:38:44.123128  Epoch 1 : Average Loss [0.3086, 0.3075, 0.3037]\n",
      "2021-05-08 15:39:40.461866  Epoch 2 : Average Loss [0.2903, 0.2915, 0.2967]\n",
      "2021-05-08 15:40:39.709370  Epoch 3 : Average Loss [0.2786, 0.2795, 0.2838]\n",
      "2021-05-08 15:41:37.002777  Epoch 4 : Average Loss [0.2571, 0.2859, 0.2756]\n",
      "2021-05-08 15:42:38.136397  Epoch 5 : Average Loss [0.2583, 0.2777, 0.2638]\n",
      "Finished Training\n",
      "Training accuracy: 92 %\n",
      "Testing accuracy: 86 %\n",
      "Testing accuracy (each class): \n",
      "0: 92.2%;   1: 83.6%;   2: 86.2%;   3: 80.0%;   4: 91.8%;   5: 86.2%;   \n",
      "6: 84.0%;   7: 93.2%;   8: 83.0%;   9: 83.4%;   \n",
      "2021-05-08 15:43:11.519499\n",
      "epoch range:  21  to  25\n",
      "2021-05-08 15:44:13.355904  Epoch 1 : Average Loss [0.2501, 0.2507, 0.2448]\n",
      "2021-05-08 15:45:15.148780  Epoch 2 : Average Loss [0.2258, 0.2368, 0.2434]\n",
      "2021-05-08 15:46:16.815378  Epoch 3 : Average Loss [0.2199, 0.2264, 0.226]\n",
      "2021-05-08 15:47:17.715117  Epoch 4 : Average Loss [0.2096, 0.2369, 0.2237]\n",
      "2021-05-08 15:48:16.552082  Epoch 5 : Average Loss [0.201, 0.2107, 0.2245]\n",
      "Finished Training\n",
      "Training accuracy: 94 %\n",
      "Testing accuracy: 86 %\n",
      "Testing accuracy (each class): \n",
      "0: 89.6%;   1: 87.4%;   2: 85.6%;   3: 81.0%;   4: 90.2%;   5: 88.2%;   \n",
      "6: 82.8%;   7: 91.2%;   8: 86.6%;   9: 84.4%;   \n",
      "2021-05-08 15:48:51.212184\n",
      "epoch range:  26  to  30\n",
      "2021-05-08 15:49:46.986734  Epoch 1 : Average Loss [0.1916, 0.1924, 0.2078]\n",
      "2021-05-08 15:50:53.359064  Epoch 2 : Average Loss [0.1811, 0.1957, 0.1872]\n",
      "2021-05-08 15:51:58.488263  Epoch 3 : Average Loss [0.17, 0.1921, 0.183]\n",
      "2021-05-08 15:52:55.565456  Epoch 4 : Average Loss [0.1649, 0.1718, 0.1891]\n",
      "2021-05-08 15:53:57.359502  Epoch 5 : Average Loss [0.1581, 0.1776, 0.1692]\n",
      "Finished Training\n",
      "Training accuracy: 95 %\n",
      "Testing accuracy: 85 %\n",
      "Testing accuracy (each class): \n",
      "0: 88.8%;   1: 89.2%;   2: 80.6%;   3: 81.8%;   4: 87.8%;   5: 85.8%;   \n",
      "6: 80.2%;   7: 92.6%;   8: 85.0%;   9: 86.2%;   \n"
     ]
    }
   ],
   "source": [
    "param = {'conv':[(), \n",
    "                  (3, 16, 3, 1, 0), \n",
    "                  (16, 32, 3, 1, 1),\n",
    "                  (32, 32, 3, 1, 0),\n",
    "                  (32, 16, 3, 1, 1)], # in_channels, out_channels, kernel_size, stride, padding\n",
    "          'pool':[(), \n",
    "                  (2, 2, 0),\n",
    "                  (2, 2, 0)], # kernel_size, stride, padding\n",
    "          'fc':[(), \n",
    "                (16*6*6, 120),\n",
    "                (120, 90), \n",
    "                (90, 10)] # in_channels, out_channels\n",
    "         }\n",
    "params.append(param)\n",
    "\n",
    "model_ft = Net(param) # Model initialization\n",
    "model_ft = model_ft.to(device) # Move model to cpu\n",
    "criterion = nn.CrossEntropyLoss() # Loss function initialization\n",
    "# TODO: Adjust the following hyper-parameters: learning rate, decay strategy, number of training epochs.\n",
    "optimizer_ft = optim.Adam(model_ft.parameters(), lr=1e-4) # Optimizer initialization\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=20, gamma=0.1) # Learning rate decay strategy\n",
    "\n",
    "epoch = 30\n",
    "for n in range((epoch // 5)):\n",
    "    print(datetime.datetime.now())\n",
    "    epo = 5*n+5\n",
    "    print(\"epoch range: \", epo-4, \" to \", epo)\n",
    "    train_test(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=5)\n",
    "\n",
    "PATH = './9layer_30round_adam1e-4_steplr_310.pth'\n",
    "paths.append(PATH)\n",
    "torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model_ft.state_dict(),\n",
    "        'optimizer_state_dict': optimizer_ft.state_dict()\n",
    "        }, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./9layer_30round_adam5e-4_steplr.pth\n",
      "CONV1 torch.Size([4, 16, 30, 30]) || CONV2 torch.Size([4, 32, 30, 30]) || POOL1 torch.Size([4, 32, 15, 15])\n",
      "CONV3 torch.Size([4, 32, 13, 13]) || CONV4 torch.Size([4, 16, 13, 13]) || POOL2 torch.Size([4, 16, 6, 6])\n",
      "FC1 torch.Size([4, 120]) || FC2 torch.Size([4, 90]) || FC3 torch.Size([4, 10])\n",
      "Training accuracy: 93 %\n",
      "Testing accuracy: 87 %\n",
      "Testing accuracy (each class): \n",
      "0: 88.2%;   1: 89.6%;   2: 86.6%;   3: 81.2%;   4: 89.4%;   5: 87.6%;   \n",
      "6: 83.8%;   7: 90.8%;   8: 88.0%;   9: 85.2%;   \n",
      "\n",
      "./9layer_30round_adam1e-4_steplr_310.pth\n",
      "CONV1 torch.Size([4, 16, 30, 30]) || CONV2 torch.Size([4, 32, 30, 30]) || POOL1 torch.Size([4, 32, 15, 15])\n",
      "CONV3 torch.Size([4, 32, 13, 13]) || CONV4 torch.Size([4, 16, 13, 13]) || POOL2 torch.Size([4, 16, 6, 6])\n",
      "FC1 torch.Size([4, 120]) || FC2 torch.Size([4, 90]) || FC3 torch.Size([4, 10])\n",
      "Training accuracy: 95 %\n",
      "Testing accuracy: 85 %\n",
      "Testing accuracy (each class): \n",
      "0: 88.8%;   1: 89.2%;   2: 80.6%;   3: 81.8%;   4: 87.8%;   5: 85.8%;   \n",
      "6: 80.2%;   7: 92.6%;   8: 85.0%;   9: 86.2%;   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, len(paths)):\n",
    "    model = Net(params[i+1])\n",
    "    optimizer = optim.Adam(model_ft.parameters(), lr=1e-4)\n",
    "    PATH = paths[i]\n",
    "    print(PATH)\n",
    "    checkpoint = torch.load(PATH)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "\n",
    "    model.eval()\n",
    "    test(model)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import datasets, transforms, utils\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import datetime\n",
    "\n",
    "net_param = {'conv':[(),\n",
    "                          (3, 6, 3, 1, 0), \n",
    "                          (6, 10, 3, 1, 0),\n",
    "                          (10, 12, 3, 1, 0)], # in_channels, out_channels, kernel_size, stride, padding\n",
    "              'pool':[(), \n",
    "                      (2, 2, 0),\n",
    "                      (2, 2, 0)], # kernel_size, stride, padding\n",
    "              'fc':[(), \n",
    "                    (12*6*6, 120),\n",
    "                    (120, 84), \n",
    "                    (84, 10)], # in_channels, out_channels\n",
    "              'drop':[0, \n",
    "                      0.5, \n",
    "                      0.5]\n",
    "             }\n",
    "             \n",
    "epoch = 30\n",
    "\n",
    "optimizer_param_set = {'1e-4_expo': (1e-4, 'expo'),\n",
    "                       '5e-4_expo': (5e-4, 'expo'),\n",
    "                       '1e-5_step': (1e-5, 'step'), \n",
    "                       '1e-4_step': (1e-4, 'step'), \n",
    "                       '5e-4_step': (5e-4, 'step'), \n",
    "                       '1e-4_mult': (1e-4, 'mult'),\n",
    "                       '5e-4_mult': (5e-4, 'mult')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    \n",
    "    # overall training correct rate\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloaders['train'], 0):\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print('Training accuracy: %.2f %%' % (100 * correct / total))\n",
    "    \n",
    "    # overall testing correct rate\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloaders['test'], 0):\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print('Testing accuracy: %.2f %% (%d / %d)' % ((100 * correct / total), correct, total))\n",
    "    \n",
    "    # count testing predictions for each class\n",
    "    correct_pred = {classname: 0 for classname in class_names}\n",
    "    total_pred = {classname: 0 for classname in class_names}\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloaders['test'], 0):\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            for label, prediction in zip(labels, predictions):\n",
    "                if label == prediction:\n",
    "                    correct_pred[class_names[label]] += 1\n",
    "                total_pred[class_names[label]] += 1\n",
    "\n",
    "    # print accuracy for each class\n",
    "    print(\"Testing accuracy (each class): \")\n",
    "    for classname, correct_count in correct_pred.items():\n",
    "        accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "        print(\"{:1s}: {:.1f}%;  \".format(classname, accuracy), end=' ')\n",
    "        if classname == \"5\":\n",
    "            print()\n",
    "    print()\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "def train_test(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    \n",
    "    for epoch in range(num_epochs):  \n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(dataloaders['train'], 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()                \n",
    "                \n",
    "        print(datetime.datetime.now(), ' Epoch', (epoch + 1), ': Average Loss', round(running_loss / 3000, 8))\n",
    "        model.loss_.append(round(running_loss / 3000, 8))\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        '''if epoch % 5 == 4:\n",
    "            print('epoch', (epoch+1))\n",
    "            test(model)'''\n",
    "    \n",
    "    print('Finished Training')\n",
    "    \n",
    "    test(model)\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Data transformer\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((32,32)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((32,32)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Dataset initialization\n",
    "data_dir = 'data' \n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'test']} # Read train and test sets, respectively.\n",
    "\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=0) for x in ['train', 'test']}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'test']}\n",
    "\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # Set device to \"cpu\" if you have no gpu\n",
    "paths = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Input - 1x32x32\n",
    "    Output - 10\n",
    "    CONV1->CONV2->POOL1->CONV3->CONV4->POOL2->FC1->FC2->FC3\n",
    "    \"\"\"\n",
    "    def __init__(self, params):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        #Initialize layers\n",
    "        self.params = params\n",
    "        self.n_layers = len(params['conv'])+len(params['pool'])+len(params['fc'])+len(params['drop'])-4\n",
    "        self.printed = False\n",
    "        self.loss_ = []\n",
    "        \n",
    "        if self.n_layers == 11:\n",
    "            self.conv1 = nn.Conv2d(*self.params['conv'][1])\n",
    "            self.conv2 = nn.Conv2d(*self.params['conv'][2])\n",
    "            self.conv3 = nn.Conv2d(*self.params['conv'][3])\n",
    "            self.conv4 = nn.Conv2d(*self.params['conv'][4])\n",
    "            \n",
    "            self.pool1 = nn.MaxPool2d(*self.params['pool'][1])\n",
    "            self.pool2 = nn.MaxPool2d(*self.params['pool'][2])\n",
    "            \n",
    "            self.fc1 = nn.Linear(*self.params['fc'][1])\n",
    "            self.fc2 = nn.Linear(*self.params['fc'][2])\n",
    "            self.fc3 = nn.Linear(*self.params['fc'][3])\n",
    "            \n",
    "            self.dropout1 = nn.Dropout(self.params['drop'][1])\n",
    "            self.dropout2 = nn.Dropout(self.params['drop'][2])\n",
    "            \n",
    "        elif self.n_layers == 10:\n",
    "            self.conv1 = nn.Conv2d(*self.params['conv'][1])\n",
    "            self.conv2 = nn.Conv2d(*self.params['conv'][2])\n",
    "            self.conv3 = nn.Conv2d(*self.params['conv'][3])\n",
    "            \n",
    "            self.pool1 = nn.MaxPool2d(*self.params['pool'][1])\n",
    "            self.pool2 = nn.MaxPool2d(*self.params['pool'][2])\n",
    "            \n",
    "            self.fc1 = nn.Linear(*self.params['fc'][1])\n",
    "            self.fc2 = nn.Linear(*self.params['fc'][2])\n",
    "            self.fc3 = nn.Linear(*self.params['fc'][3])\n",
    "            \n",
    "            self.dropout1 = nn.Dropout(self.params['drop'][1])\n",
    "            self.dropout2 = nn.Dropout(self.params['drop'][2])\n",
    "        \n",
    "        elif self.n_layers == 9:\n",
    "            self.conv1 = nn.Conv2d(*self.params['conv'][1])\n",
    "            self.conv2 = nn.Conv2d(*self.params['conv'][2])\n",
    "            \n",
    "            self.pool1 = nn.MaxPool2d(*self.params['pool'][1])\n",
    "            self.pool2 = nn.MaxPool2d(*self.params['pool'][2])\n",
    "            \n",
    "            self.fc1 = nn.Linear(*self.params['fc'][1])\n",
    "            self.fc2 = nn.Linear(*self.params['fc'][2])\n",
    "            self.fc3 = nn.Linear(*self.params['fc'][3])\n",
    "            \n",
    "            self.dropout1 = nn.Dropout(self.params['drop'][1])\n",
    "            self.dropout2 = nn.Dropout(self.params['drop'][2])\n",
    "            \n",
    "\n",
    "    def forward(self, img):\n",
    "        # Implement forward pass\n",
    "        x = img\n",
    "        \n",
    "        if self.n_layers == 11:\n",
    "            x = F.relu(self.conv1(x))\n",
    "            if not self.printed: \n",
    "                print(\"CONV1\", x.size(), end=\" || \")\n",
    "            x = F.relu(self.conv2(x))\n",
    "            if not self.printed: \n",
    "                print(\"CONV2\", x.size(), end=\" || \")\n",
    "            x = self.pool1(x)\n",
    "            if not self.printed: \n",
    "                print(\"POOL1\", x.size())\n",
    "        \n",
    "            x = F.relu(self.conv3(x))\n",
    "            if not self.printed: \n",
    "                print(\"CONV3\", x.size(), end=\" || \")\n",
    "            x = F.relu(self.conv4(x))\n",
    "            if not self.printed: \n",
    "                print(\"CONV4\", x.size(), end=\" || \")\n",
    "            x = self.pool2(x)\n",
    "            if not self.printed: \n",
    "                print(\"POOL2\", x.size())\n",
    "        \n",
    "            x = x.view(x.size(0), -1)\n",
    "            x = F.relu(self.fc1(x))\n",
    "            if not self.printed: \n",
    "                print(\"FC1\", x.size(), end=\" || \")\n",
    "            x = self.dropout1(x)\n",
    "            x = F.relu(self.fc2(x))\n",
    "            if not self.printed: \n",
    "                print(\"FC2\", x.size(), end=\" || \")\n",
    "            x = self.fc3(x)\n",
    "            if not self.printed: \n",
    "                print(\"FC3\", x.size())\n",
    "                \n",
    "        elif self.n_layers == 10:\n",
    "            x = F.relu(self.conv1(x))\n",
    "            if not self.printed: \n",
    "                print(\"CONV1\", x.size(), end=\" || \")\n",
    "            x = self.pool1(x)\n",
    "            if not self.printed: \n",
    "                print(\"POOL1\", x.size())\n",
    "        \n",
    "            x = F.relu(self.conv2(x))\n",
    "            if not self.printed: \n",
    "                print(\"CONV2\", x.size(), end=\" || \")\n",
    "            x = F.relu(self.conv3(x))\n",
    "            if not self.printed: \n",
    "                print(\"CONV3\", x.size(), end=\" || \")\n",
    "            x = self.pool2(x)\n",
    "            if not self.printed: \n",
    "                print(\"POOL2\", x.size())\n",
    "        \n",
    "            x = x.view(x.size(0), -1)\n",
    "            x = F.relu(self.fc1(x))\n",
    "            if not self.printed: \n",
    "                print(\"FC1\", x.size(), end=\" || \")\n",
    "            x = self.dropout1(x)\n",
    "            x = F.relu(self.fc2(x))\n",
    "            if not self.printed: \n",
    "                print(\"FC2\", x.size(), end=\" || \")\n",
    "            x = self.fc3(x)\n",
    "            if not self.printed: \n",
    "                print(\"FC3\", x.size())\n",
    "        \n",
    "        elif self.n_layers == 9:\n",
    "            x = F.relu(self.conv1(x))\n",
    "            if not self.printed: \n",
    "                print(\"CONV1\", x.size(), end=\" || \")\n",
    "            x = self.pool1(x)\n",
    "            if not self.printed: \n",
    "                print(\"POOL1\", x.size())\n",
    "        \n",
    "            x = F.relu(self.conv2(x))\n",
    "            if not self.printed: \n",
    "                print(\"CONV2\", x.size(), end=\" || \")\n",
    "            x = self.pool2(x)\n",
    "            if not self.printed: \n",
    "                print(\"POOL2\", x.size())\n",
    "        \n",
    "            x = x.view(x.size(0), -1)\n",
    "            x = F.relu(self.fc1(x))\n",
    "            if not self.printed: \n",
    "                print(\"FC1\", x.size(), end=\" || \")\n",
    "            x = self.dropout1(x)\n",
    "            x = F.relu(self.fc2(x))\n",
    "            if not self.printed: \n",
    "                print(\"FC2\", x.size(), end=\" || \")\n",
    "            x = self.fc3(x)\n",
    "            if not self.printed: \n",
    "                print(\"FC3\", x.size())\n",
    "        \n",
    "        self.printed = True\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimizer parameter set:  5e-4_expo\n",
      "2021-05-10 00:01:50.327411\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/train/6/train_9a1eb5_6_942.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-63583beb3962>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mepo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtrain_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_scheduler_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mPATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./cnn_testing_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_.pth\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-7130494eac02>\u001b[0m in \u001b[0;36mtrain_test\u001b[0;34m(model, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0;31m# get the inputs; data is a list of [inputs, labels]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jiarui/snap/jupyter/common/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jiarui/snap/jupyter/common/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jiarui/snap/jupyter/common/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jiarui/snap/jupyter/common/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jiarui/snap/jupyter/common/lib/python3.7/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \"\"\"\n\u001b[1;32m    177\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jiarui/snap/jupyter/common/lib/python3.7/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maccimage_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jiarui/snap/jupyter/common/lib/python3.7/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;31m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/train/6/train_9a1eb5_6_942.png'"
     ]
    }
   ],
   "source": [
    "model_ft = Net(net_param)\n",
    "model_ft = model_ft.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.Adam(model_ft.parameters(), lr=5e-4)\n",
    "lanbda1 = lambda epoch: 0.5**(5*epoch)\n",
    "lr_scheduler_ft = lr_scheduler.LambdaLR(optimizer_ft, lr_lambda=lanbda1, last_epoch=-1)\n",
    "        \n",
    "print(\"optimizer parameter set: \", opt)\n",
    "print(datetime.datetime.now())\n",
    "epo = epoch\n",
    "train_test(model_ft, criterion, optimizer_ft, lr_scheduler_ft, num_epochs=epo)\n",
    "    \n",
    "PATH = \"./cnn_testing_\"+opt+\"_.pth\"\n",
    "paths.append(PATH)\n",
    "torch.save({'epoch': epoch,\n",
    "            'model_state_dict': model_ft.state_dict(),\n",
    "            'optimizer_state_dict': optimizer_ft.state_dict()\n",
    "           }, PATH)\n",
    "        \n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimizer parameter set:  5e-4_expo\n",
      "2021-05-09 23:32:50.921927\n",
      "CONV1 torch.Size([4, 6, 30, 30]) || CONV2 torch.Size([4, 10, 28, 28]) || POOL1 torch.Size([4, 10, 14, 14])\n",
      "CONV3 torch.Size([4, 12, 12, 12]) || CONV4 torch.Size([4, 10, 12, 12]) || POOL2 torch.Size([4, 10, 6, 6])\n",
      "FC1 torch.Size([4, 120]) || FC2 torch.Size([4, 84]) || FC3 torch.Size([4, 10])\n",
      "2021-05-09 23:33:49.147789  Epoch 1 : Average Loss 4.72096836\n",
      "2021-05-09 23:34:40.749374  Epoch 2 : Average Loss 2.76313736\n",
      "2021-05-09 23:35:27.832227  Epoch 3 : Average Loss 2.24721575\n",
      "2021-05-09 23:36:16.434910  Epoch 4 : Average Loss 2.05982471\n",
      "2021-05-09 23:37:01.017213  Epoch 5 : Average Loss 1.91957882\n",
      "2021-05-09 23:37:47.086036  Epoch 6 : Average Loss 1.82478556\n",
      "2021-05-09 23:38:38.476302  Epoch 7 : Average Loss 1.74360474\n",
      "2021-05-09 23:39:34.340413  Epoch 8 : Average Loss 1.68967239\n",
      "2021-05-09 23:40:23.301197  Epoch 9 : Average Loss 1.64623805\n",
      "2021-05-09 23:41:32.358832  Epoch 10 : Average Loss 1.61694721\n",
      "2021-05-09 23:42:41.147366  Epoch 11 : Average Loss 1.55639617\n",
      "2021-05-09 23:43:38.043336  Epoch 12 : Average Loss 1.53159863\n",
      "2021-05-09 23:44:29.825880  Epoch 13 : Average Loss 1.52551116\n",
      "2021-05-09 23:45:17.129082  Epoch 14 : Average Loss 1.50290212\n",
      "2021-05-09 23:46:04.688951  Epoch 15 : Average Loss 1.47898612\n",
      "2021-05-09 23:46:56.172676  Epoch 16 : Average Loss 1.45049716\n",
      "2021-05-09 23:47:45.294683  Epoch 17 : Average Loss 1.44611639\n",
      "2021-05-09 23:48:30.709561  Epoch 18 : Average Loss 1.43421678\n",
      "2021-05-09 23:49:20.784177  Epoch 19 : Average Loss 1.43619118\n",
      "2021-05-09 23:50:11.532649  Epoch 20 : Average Loss 1.4269426\n",
      "2021-05-09 23:50:57.067621  Epoch 21 : Average Loss 1.41761376\n",
      "2021-05-09 23:51:43.502169  Epoch 22 : Average Loss 1.39728326\n",
      "2021-05-09 23:52:33.159118  Epoch 23 : Average Loss 1.39304324\n",
      "2021-05-09 23:53:22.975921  Epoch 24 : Average Loss 1.41124757\n",
      "2021-05-09 23:54:09.911470  Epoch 25 : Average Loss 1.37091074\n",
      "2021-05-09 23:54:58.513355  Epoch 26 : Average Loss 1.3598784\n",
      "2021-05-09 23:56:03.651634  Epoch 27 : Average Loss 1.36285226\n",
      "2021-05-09 23:57:24.435363  Epoch 28 : Average Loss 1.35187125\n",
      "2021-05-09 23:58:32.978679  Epoch 29 : Average Loss 1.35158252\n",
      "2021-05-09 23:59:34.112209  Epoch 30 : Average Loss 1.34865102\n",
      "Finished Training\n",
      "Training accuracy: 83.64 %\n",
      "Testing accuracy: 81.18 % (4059 / 5000)\n",
      "Testing accuracy (each class): \n",
      "0: 83.8%;   1: 82.4%;   2: 79.6%;   3: 77.0%;   4: 89.2%;   5: 78.6%;   \n",
      "6: 74.0%;   7: 90.4%;   8: 78.4%;   9: 80.6%;   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_ft = Net(net_param)\n",
    "model_ft = model_ft.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.Adam(model_ft.parameters(), lr=5e-4)\n",
    "lanbda1 = lambda epoch: 0.5**(0.5*epoch)\n",
    "lr_scheduler_ft = lr_scheduler.LambdaLR(optimizer_ft, lr_lambda=lanbda1, last_epoch=-1)\n",
    "        \n",
    "print(\"optimizer parameter set: \", opt)\n",
    "print(datetime.datetime.now())\n",
    "epo = epoch\n",
    "train_test(model_ft, criterion, optimizer_ft, lr_scheduler_ft, num_epochs=epo)\n",
    "    \n",
    "PATH = \"./cnn_testing_\"+opt+\"_.pth\"\n",
    "paths.append(PATH)\n",
    "torch.save({'epoch': epoch,\n",
    "            'model_state_dict': model_ft.state_dict(),\n",
    "            'optimizer_state_dict': optimizer_ft.state_dict()\n",
    "           }, PATH)\n",
    "        \n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
